# üîß Flow-RAG-VLA Íµ¨ÌòÑ Î∞©Î≤ïÎ°†
## Implementation Methodology & Technical Guide

---

## üìã Table of Contents
1. [Íµ¨ÌòÑ Í∞úÏöî](#1-Íµ¨ÌòÑ-Í∞úÏöî)
2. [Í∞úÎ∞ú ÌôòÍ≤Ω ÏÑ§Ï†ï](#2-Í∞úÎ∞ú-ÌôòÍ≤Ω-ÏÑ§Ï†ï)
3. [Î™®ÎìàÎ≥Ñ Íµ¨ÌòÑ Í∞ÄÏù¥Îìú](#3-Î™®ÎìàÎ≥Ñ-Íµ¨ÌòÑ-Í∞ÄÏù¥Îìú)
4. [ÌÜµÌï© Î∞è ÏµúÏ†ÅÌôî](#4-ÌÜµÌï©-Î∞è-ÏµúÏ†ÅÌôî)
5. [Ïã§Ìóò ÌîÑÎ°úÌÜ†ÏΩú](#5-Ïã§Ìóò-ÌîÑÎ°úÌÜ†ÏΩú)
6. [ÌèâÍ∞Ä Î∞è Í≤ÄÏ¶ù](#6-ÌèâÍ∞Ä-Î∞è-Í≤ÄÏ¶ù)

---

## 1. Íµ¨ÌòÑ Í∞úÏöî

### 1.1 ÏãúÏä§ÌÖú ÏïÑÌÇ§ÌÖçÏ≤ò

```python
"""
Flow-RAG-VLA ÏãúÏä§ÌÖú Íµ¨Ï°∞
‚îú‚îÄ‚îÄ Flow Module (ÏÜçÎèÑ)
‚îÇ   ‚îú‚îÄ‚îÄ Vision Encoder (DINOv2)
‚îÇ   ‚îú‚îÄ‚îÄ Velocity Network
‚îÇ   ‚îî‚îÄ‚îÄ ODE Solver
‚îú‚îÄ‚îÄ RAG Module (ÏßÄÎä•)
‚îÇ   ‚îú‚îÄ‚îÄ Failure Detector
‚îÇ   ‚îú‚îÄ‚îÄ Vector Database (FAISS)
‚îÇ   ‚îî‚îÄ‚îÄ Memory Manager
‚îî‚îÄ‚îÄ Parallel Processor (ÌÜµÌï©)
    ‚îú‚îÄ‚îÄ Async Executor
    ‚îú‚îÄ‚îÄ Synchronizer
    ‚îî‚îÄ‚îÄ Decision Merger
"""
```

### 1.2 Í∞úÎ∞ú ÏõêÏπô

```python
development_principles = {
    "Î™®ÎìàÏÑ±": "Í∞Å Ïª¥Ìè¨ÎÑåÌä∏ ÎèÖÎ¶Ω Í∞úÎ∞ú/ÌÖåÏä§Ìä∏",
    "Ï†êÏßÑÏ†Å": "Îã®Í≥ÑÎ≥Ñ ÌÜµÌï© (ÏàúÏ∞® ‚Üí Î≥ëÎ†¨)",
    "Ï∏°Ï†ï Í∞ÄÎä•": "Î™®Îì† Îã®Í≥Ñ Î≤§ÏπòÎßàÌÅ¨",
    "Ïû¨ÌòÑ Í∞ÄÎä•": "ÏãúÎìú Í≥†Ï†ï, Î≤ÑÏ†Ñ Í¥ÄÎ¶¨"
}
```

---

## 2. Í∞úÎ∞ú ÌôòÍ≤Ω ÏÑ§Ï†ï

### 2.1 Í∏∞Î≥∏ ÌôòÍ≤Ω

```bash
# Step 1: Conda ÌôòÍ≤Ω ÏÉùÏÑ±
conda create -n flow_rag_vla python=3.9
conda activate flow_rag_vla

# Step 2: PyTorch ÏÑ§Ïπò (CUDA 11.8)
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# Step 3: ÌïµÏã¨ Ìå®ÌÇ§ÏßÄ
pip install transformers  # Vision/Language models
pip install faiss-gpu     # Vector search
pip install einops        # Tensor operations
pip install torchdiffeq   # ODE solvers
pip install hydra-core    # Configuration
pip install wandb         # Experiment tracking
```

### 2.2 Î°úÎ≥¥Ìã±Ïä§ ÌôòÍ≤Ω

```bash
# Simulation
pip install pybullet
pip install gym
pip install stable-baselines3

# Optional: Isaac Sim
# Follow: https://docs.omniverse.nvidia.com/isaacsim/latest/

# Robot control
pip install pyquaternion
pip install spatialmath-python
pip install modern_robotics
```

### 2.3 ÌîÑÎ°úÏ†ùÌä∏ Íµ¨Ï°∞

```bash
flow_rag_vla/
‚îú‚îÄ‚îÄ configs/              # Hydra configurations
‚îÇ   ‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îî‚îÄ‚îÄ experiment/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ flow/        # Flow Matching modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag/         # RAG modules
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fusion/      # Integration
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/
‚îú‚îÄ‚îÄ scripts/
‚îú‚îÄ‚îÄ tests/
‚îî‚îÄ‚îÄ experiments/
```

---

## 3. Î™®ÎìàÎ≥Ñ Íµ¨ÌòÑ Í∞ÄÏù¥Îìú

### 3.1 Flow Module Íµ¨ÌòÑ

#### Step 1: Velocity Network

```python
# src/models/flow/velocity_net.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

class VelocityNetwork(nn.Module):
    """Flow Matching Velocity Predictor"""
    
    def __init__(
        self,
        state_dim: int = 512,
        action_dim: int = 7,
        hidden_dim: int = 1024,
        num_layers: int = 6,
        dropout: float = 0.1
    ):
        super().__init__()
        
        # Time embedding
        self.time_embed = nn.Sequential(
            nn.Linear(1, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # State processor
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.SiLU()
        )
        
        # Action processor
        self.action_encoder = nn.Sequential(
            nn.Linear(action_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.SiLU()
        )
        
        # Transformer blocks
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=hidden_dim,
                nhead=8,
                dim_feedforward=hidden_dim * 4,
                dropout=dropout,
                batch_first=True
            ),
            num_layers=num_layers
        )
        
        # Output head
        self.output = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.SiLU(),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
    def forward(self, state, action, t):
        """
        Args:
            state: [B, state_dim] - Visual features
            action: [B, action_dim] - Current action
            t: [B, 1] - Time step
        Returns:
            velocity: [B, action_dim] - Predicted velocity
        """
        # Encode inputs
        t_emb = self.time_embed(t)
        s_emb = self.state_encoder(state)
        a_emb = self.action_encoder(action)
        
        # Combine
        features = s_emb + a_emb + t_emb
        features = features.unsqueeze(1)  # [B, 1, hidden_dim]
        
        # Transform
        out = self.transformer(features)
        out = out.squeeze(1)
        
        # Predict velocity
        velocity = self.output(out)
        
        return velocity
```

#### Step 2: Flow Matching Training

```python
# src/models/flow/flow_matching.py

class FlowMatching(nn.Module):
    """Complete Flow Matching Module"""
    
    def __init__(self, config):
        super().__init__()
        self.velocity_net = VelocityNetwork(**config.model)
        self.ode_steps = config.ode_steps
        
    def compute_loss(self, state, action_0, action_1):
        """Flow Matching loss"""
        batch_size = state.shape[0]
        
        # Sample random time
        t = torch.rand(batch_size, 1).to(state.device)
        
        # Linear interpolation
        action_t = (1 - t) * action_0 + t * action_1
        
        # True velocity (constant for linear flow)
        v_true = action_1 - action_0
        
        # Predicted velocity
        v_pred = self.velocity_net(state, action_t, t)
        
        # MSE loss
        loss = F.mse_loss(v_pred, v_true)
        
        return loss
    
    @torch.no_grad()
    def generate(self, state, action_init=None, steps=None):
        """Generate action via ODE integration"""
        if action_init is None:
            action_init = torch.zeros(state.shape[0], 7).to(state.device)
        
        if steps is None:
            steps = self.ode_steps
            
        dt = 1.0 / steps
        action = action_init
        
        for i in range(steps):
            t = torch.ones(state.shape[0], 1).to(state.device) * (i * dt)
            v = self.velocity_net(state, action, t)
            action = action + v * dt
            
        return action
```

### 3.2 RAG Module Íµ¨ÌòÑ

#### Step 1: Failure Memory

```python
# src/models/rag/failure_memory.py

import faiss
import numpy as np
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class FailureCase:
    """Single failure case"""
    state_embedding: np.ndarray
    failed_action: np.ndarray
    failure_type: str
    correction: np.ndarray
    confidence: float

class FailureMemory:
    """Selective failure memory with FAISS"""
    
    def __init__(
        self,
        embedding_dim: int = 512,
        max_size: int = 10000,
        similarity_threshold: float = 0.9
    ):
        # FAISS index for fast search
        self.index = faiss.IndexFlatL2(embedding_dim)
        self.index = faiss.index_cpu_to_all_gpus(self.index)  # GPU
        
        # Storage
        self.cases: List[FailureCase] = []
        self.max_size = max_size
        self.similarity_threshold = similarity_threshold
        
        # Failure clustering
        self.failure_clusters = {}
        self.cluster_id = 0
        
    def add_failure(self, state, action, failure_type, correction):
        """Add new failure case"""
        # Check if similar failure exists
        if self._is_duplicate(state):
            return False
            
        # Create failure case
        case = FailureCase(
            state_embedding=state,
            failed_action=action,
            failure_type=failure_type,
            correction=correction,
            confidence=1.0
        )
        
        # Add to index
        self.index.add(state.reshape(1, -1))
        self.cases.append(case)
        
        # Cluster management
        self._update_clusters(case)
        
        # Memory management
        if len(self.cases) > self.max_size:
            self._prune_memory()
            
        return True
    
    def search(self, state, k=5):
        """Search similar failures"""
        if len(self.cases) == 0:
            return []
            
        # Search in FAISS
        distances, indices = self.index.search(
            state.reshape(1, -1), 
            min(k, len(self.cases))
        )
        
        # Return relevant cases
        results = []
        for i, idx in enumerate(indices[0]):
            if distances[0][i] < self.similarity_threshold:
                results.append(self.cases[idx])
                
        return results
    
    def _is_duplicate(self, state):
        """Check if similar failure exists"""
        if len(self.cases) == 0:
            return False
            
        distances, _ = self.index.search(state.reshape(1, -1), 1)
        return distances[0][0] < 0.1  # Very similar
    
    def _update_clusters(self, case):
        """Update failure clusters"""
        # Find or create cluster
        cluster_found = False
        for cluster_id, cluster in self.failure_clusters.items():
            if case.failure_type == cluster['type']:
                cluster['cases'].append(case)
                cluster['count'] += 1
                cluster_found = True
                break
                
        if not cluster_found:
            self.failure_clusters[self.cluster_id] = {
                'type': case.failure_type,
                'cases': [case],
                'count': 1
            }
            self.cluster_id += 1
    
    def _prune_memory(self):
        """Remove least useful memories"""
        # Keep only representative cases per cluster
        new_cases = []
        new_embeddings = []
        
        for cluster in self.failure_clusters.values():
            # Keep most confident case per cluster
            best_case = max(cluster['cases'], key=lambda x: x.confidence)
            new_cases.append(best_case)
            new_embeddings.append(best_case.state_embedding)
        
        # Rebuild index
        self.cases = new_cases
        self.index.reset()
        if new_embeddings:
            self.index.add(np.vstack(new_embeddings))
```

#### Step 2: Failure Detector

```python
# src/models/rag/failure_detector.py

class FailureDetector(nn.Module):
    """Detect and classify failures"""
    
    def __init__(self, state_dim=512, hidden_dim=256):
        super().__init__()
        
        self.classifier = nn.Sequential(
            nn.Linear(state_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)  # Failure probability
        )
        
        self.failure_types = [
            'grasp_failure',
            'collision',
            'drop',
            'misalignment',
            'timeout'
        ]
        
    def forward(self, state_before, state_after):
        """Detect if action failed"""
        combined = torch.cat([state_before, state_after], dim=-1)
        failure_prob = torch.sigmoid(self.classifier(combined))
        return failure_prob
    
    def classify_failure(self, state_before, state_after, action):
        """Classify failure type"""
        # Rule-based classification (can be learned)
        
        # Check gripper state
        if self._check_grasp_failure(state_after):
            return 'grasp_failure'
        
        # Check collision
        if self._check_collision(state_after):
            return 'collision'
        
        # Check object drop
        if self._check_drop(state_before, state_after):
            return 'drop'
        
        return 'unknown'
    
    def _check_grasp_failure(self, state):
        # Implementation specific to robot
        pass
    
    def _check_collision(self, state):
        # Check force sensors
        pass
    
    def _check_drop(self, before, after):
        # Check object position change
        pass
```

### 3.3 Î≥ëÎ†¨ Ï≤òÎ¶¨ Íµ¨ÌòÑ

#### Dual Pathway Processor

```python
# src/models/fusion/parallel_processor.py

import asyncio
import concurrent.futures
import time
from typing import Optional, Tuple

class DualPathwayProcessor:
    """Parallel execution of Flow and RAG"""
    
    def __init__(
        self,
        flow_module,
        rag_module,
        max_latency_ms: float = 25.0
    ):
        self.flow = flow_module
        self.rag = rag_module
        self.max_latency = max_latency_ms / 1000.0  # Convert to seconds
        
        # Thread pool for parallel execution
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
        
        # Statistics
        self.stats = {
            'flow_time': [],
            'rag_time': [],
            'total_time': [],
            'rag_timeouts': 0,
            'corrections_applied': 0
        }
    
    def process(self, observation, state_features):
        """Main processing function"""
        start_time = time.time()
        
        # Submit both tasks
        flow_future = self.executor.submit(
            self._run_flow, 
            state_features
        )
        
        rag_future = self.executor.submit(
            self._run_rag,
            state_features
        )
        
        # Get Flow result (primary)
        action = flow_future.result(timeout=self.max_latency)
        flow_time = time.time() - start_time
        self.stats['flow_time'].append(flow_time)
        
        # Try to get RAG result (secondary)
        correction = None
        try:
            remaining_time = self.max_latency - flow_time
            if remaining_time > 0:
                risks = rag_future.result(timeout=remaining_time)
                if risks and risks[0].confidence > 0.8:
                    correction = risks[0].correction
                    self.stats['corrections_applied'] += 1
        except concurrent.futures.TimeoutError:
            self.stats['rag_timeouts'] += 1
        
        # Apply correction if available
        if correction is not None:
            action = self._apply_correction(action, correction)
        
        total_time = time.time() - start_time
        self.stats['total_time'].append(total_time)
        
        return action
    
    def _run_flow(self, state_features):
        """Run Flow Matching"""
        with torch.no_grad():
            action = self.flow.generate(state_features, steps=5)
        return action.cpu().numpy()
    
    def _run_rag(self, state_features):
        """Run RAG search"""
        # Convert to numpy for FAISS
        state_np = state_features.cpu().numpy()
        risks = self.rag.search(state_np, k=3)
        return risks
    
    def _apply_correction(self, action, correction):
        """Apply RAG correction to Flow action"""
        # Weighted combination
        alpha = 0.3  # Correction weight
        corrected = (1 - alpha) * action + alpha * correction
        return corrected
    
    def get_statistics(self):
        """Get performance statistics"""
        import numpy as np
        
        return {
            'avg_flow_time': np.mean(self.stats['flow_time']),
            'avg_total_time': np.mean(self.stats['total_time']),
            'rag_timeout_rate': self.stats['rag_timeouts'] / len(self.stats['total_time']),
            'correction_rate': self.stats['corrections_applied'] / len(self.stats['total_time']),
            'avg_frequency': 1.0 / np.mean(self.stats['total_time'])
        }
```

---

## 4. ÌÜµÌï© Î∞è ÏµúÏ†ÅÌôî

### 4.1 Ï†ÑÏ≤¥ ÏãúÏä§ÌÖú ÌÜµÌï©

```python
# src/models/flow_rag_vla.py

class FlowRAGVLA(nn.Module):
    """Complete Flow-RAG-VLA System"""
    
    def __init__(self, config):
        super().__init__()
        
        # Vision encoder
        self.vision_encoder = self._load_vision_encoder(config.vision)
        
        # Flow module
        self.flow = FlowMatching(config.flow)
        
        # RAG module
        self.failure_memory = FailureMemory(**config.rag)
        self.failure_detector = FailureDetector()
        
        # Parallel processor
        self.processor = DualPathwayProcessor(
            self.flow,
            self.failure_memory,
            max_latency_ms=config.max_latency
        )
        
        # Training mode flags
        self.use_parallel = config.use_parallel
        
    def forward(self, observation, instruction=None):
        """Forward pass"""
        # Extract visual features
        with torch.no_grad():
            state_features = self.vision_encoder(observation)
        
        if self.training:
            # Training: Flow only
            return self.flow(state_features)
        else:
            # Inference: Parallel processing
            if self.use_parallel:
                action = self.processor.process(observation, state_features)
            else:
                # Sequential fallback
                action = self.flow.generate(state_features)
                risks = self.failure_memory.search(state_features)
                if risks and risks[0].confidence > 0.8:
                    action = self._correct_action(action, risks[0])
            
            return action
    
    def update_memory(self, state_before, state_after, action, success):
        """Update failure memory after execution"""
        if not success:
            # Detect failure type
            failure_type = self.failure_detector.classify_failure(
                state_before, state_after, action
            )
            
            # Compute correction
            correction = self._compute_correction(action, failure_type)
            
            # Store in memory
            self.failure_memory.add_failure(
                state_before,
                action,
                failure_type,
                correction
            )
    
    def _compute_correction(self, failed_action, failure_type):
        """Compute corrected action"""
        correction = failed_action.copy()
        
        if failure_type == 'grasp_failure':
            correction[6] *= 1.3  # Increase grip force
        elif failure_type == 'collision':
            correction[:3] *= 0.8  # Reduce speed
        elif failure_type == 'drop':
            correction[2] += 0.05  # Lift higher
        
        return correction
```

### 4.2 ÏµúÏ†ÅÌôî Í∏∞Î≤ï

#### TensorRT Î≥ÄÌôò

```python
# scripts/optimize_tensorrt.py

import torch
import tensorrt as trt
from torch2trt import torch2trt

def optimize_flow_module(model, input_shape):
    """Convert Flow module to TensorRT"""
    
    # Example input
    x = torch.randn(1, *input_shape).cuda()
    
    # Convert
    model_trt = torch2trt(
        model,
        [x],
        fp16_mode=True,  # FP16 for speed
        max_batch_size=1,
        max_workspace_size=1 << 30  # 1GB
    )
    
    # Save
    torch.save(model_trt.state_dict(), 'flow_trt.pth')
    
    return model_trt
```

#### Quantization

```python
# scripts/quantize_model.py

import torch
from torch.quantization import quantize_dynamic

def quantize_rag_module(model):
    """Quantize RAG module for efficiency"""
    
    quantized = quantize_dynamic(
        model,
        {nn.Linear, nn.Conv2d},
        dtype=torch.qint8
    )
    
    return quantized
```

---

## 5. Ïã§Ìóò ÌîÑÎ°úÌÜ†ÏΩú

### 5.1 ÌõàÎ†® ÌîÑÎ°úÌÜ†ÏΩú

```python
# scripts/train.py

import hydra
from omegaconf import DictConfig

@hydra.main(config_path="../configs", config_name="train")
def train(cfg: DictConfig):
    """Main training script"""
    
    # Initialize model
    model = FlowRAGVLA(cfg.model)
    
    # Data loaders
    train_loader = get_dataloader(cfg.data.train)
    val_loader = get_dataloader(cfg.data.val)
    
    # Optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=cfg.training.lr,
        weight_decay=cfg.training.weight_decay
    )
    
    # Scheduler
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=cfg.training.epochs
    )
    
    # Training loop
    for epoch in range(cfg.training.epochs):
        # Train
        model.train()
        for batch in train_loader:
            # Flow Matching loss
            loss = model.flow.compute_loss(
                batch['state'],
                batch['action_noisy'],
                batch['action_expert']
            )
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        # Validate
        model.eval()
        val_metrics = validate(model, val_loader)
        
        # Log
        wandb.log({
            'epoch': epoch,
            'loss': loss.item(),
            **val_metrics
        })
        
        scheduler.step()

if __name__ == "__main__":
    train()
```

### 5.2 ÌèâÍ∞Ä ÌîÑÎ°úÌÜ†ÏΩú

```python
# src/evaluation/evaluate.py

def evaluate_system(model, env, num_episodes=100):
    """Complete system evaluation"""
    
    metrics = {
        'success_rate': [],
        'completion_time': [],
        'failure_repetitions': [],
        'action_frequency': []
    }
    
    failure_history = {}
    
    for episode in range(num_episodes):
        obs = env.reset()
        done = False
        steps = 0
        start_time = time.time()
        
        while not done and steps < 200:
            # Get action
            action_start = time.time()
            action = model(obs)
            action_time = time.time() - action_start
            
            # Execute
            obs_next, reward, done, info = env.step(action)
            
            # Check failure
            if info.get('failed'):
                failure_type = info['failure_type']
                
                # Check repetition
                if failure_type in failure_history:
                    failure_history[failure_type] += 1
                    metrics['failure_repetitions'].append(
                        failure_history[failure_type]
                    )
                else:
                    failure_history[failure_type] = 1
                
                # Update memory
                model.update_memory(
                    obs, obs_next, action, success=False
                )
            
            obs = obs_next
            steps += 1
            metrics['action_frequency'].append(1.0 / action_time)
        
        # Record episode metrics
        metrics['success_rate'].append(float(info.get('success', False)))
        metrics['completion_time'].append(time.time() - start_time)
    
    # Compute statistics
    results = {}
    for key, values in metrics.items():
        results[f'{key}_mean'] = np.mean(values)
        results[f'{key}_std'] = np.std(values)
    
    return results
```

---

## 6. ÌèâÍ∞Ä Î∞è Í≤ÄÏ¶ù

### 6.1 ÏÑ±Îä• Î≤§ÏπòÎßàÌÅ¨

```python
# benchmarks/speed_test.py

def benchmark_speed():
    """Test inference speed"""
    
    model = FlowRAGVLA(config)
    model.eval()
    
    # Warm up
    for _ in range(10):
        _ = model(dummy_input)
    
    # Benchmark
    times = []
    for _ in range(100):
        start = time.perf_counter()
        _ = model(dummy_input)
        times.append(time.perf_counter() - start)
    
    print(f"Average latency: {np.mean(times)*1000:.2f}ms")
    print(f"Frequency: {1.0/np.mean(times):.1f}Hz")
    print(f"95th percentile: {np.percentile(times, 95)*1000:.2f}ms")
```

### 6.2 Ablation Studies

```python
# experiments/ablation.py

def ablation_study():
    """Test different configurations"""
    
    configs = {
        'flow_only': {'use_rag': False, 'use_parallel': False},
        'rag_only': {'use_flow': False, 'use_parallel': False},
        'sequential': {'use_flow': True, 'use_rag': True, 'use_parallel': False},
        'parallel': {'use_flow': True, 'use_rag': True, 'use_parallel': True}
    }
    
    results = {}
    for name, config in configs.items():
        model = FlowRAGVLA(config)
        metrics = evaluate_system(model, env)
        results[name] = metrics
    
    # Compare
    df = pd.DataFrame(results).T
    print(df.to_markdown())
```

---

## üìä ÏòàÏÉÅ Í≤∞Í≥º

```python
expected_performance = {
    "Latency": {
        "Flow only": "20ms",
        "RAG only": "100ms",
        "Sequential": "120ms",
        "Parallel (Ours)": "25ms ‚úì"
    },
    
    "Success Rate": {
        "Flow only": "85%",
        "RAG only": "75%",
        "Sequential": "88%",
        "Parallel (Ours)": "92% ‚úì"
    },
    
    "Memory Usage": {
        "Flow": "500MB (model)",
        "RAG": "100MB (database)",
        "Total": "600MB ‚úì"
    }
}
```

---

## üöÄ Quick Start

```bash
# 1. Clone repository
git clone https://github.com/your-username/flow-rag-vla
cd flow-rag-vla

# 2. Install dependencies
bash scripts/setup.sh

# 3. Download pretrained models
python scripts/download_models.py

# 4. Run demo
python demo.py --task pick_and_place

# 5. Train
python scripts/train.py

# 6. Evaluate
python scripts/evaluate.py --checkpoint best_model.pth
```

---

## üìù Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏

### Í∞úÎ∞ú ÏßÑÌñâ ÏÉÅÌô©
- [ ] Flow Module Íµ¨ÌòÑ
- [ ] RAG Module Íµ¨ÌòÑ
- [ ] Parallel Processor Íµ¨ÌòÑ
- [ ] ÏãúÏä§ÌÖú ÌÜµÌï©
- [ ] ÏãúÎÆ¨Î†àÏù¥ÏÖò ÌÖåÏä§Ìä∏
- [ ] ÏÜçÎèÑ ÏµúÏ†ÅÌôî
- [ ] Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî
- [ ] Ïã§Ìóò ÏôÑÎ£å
- [ ] ÎÖºÎ¨∏ ÏûëÏÑ±

---

*Last Updated: 2025ÎÖÑ 1Ïõî*
*Version: 1.0*