# ğŸ§  Context & Memory ê´€ë ¨ ë…¼ë¬¸ë“¤
## Context Management and Memory Systems for VLA

---

## ğŸ“š ì´ í´ë”ì˜ ë…¼ë¬¸ë“¤

### ğŸ”¥ Critical Papers (Context-Aware RAG-VLA í•µì‹¬)

#### 1. **Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context** (2019)
- **íŒŒì¼**: `Transformer-XL_Long_Context_2019.md`
- **ì €ì**: Zihang Dai, et al. (CMU, Google)
- **ì¤‘ìš”ë„**: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
- **ë‚œì´ë„**: ğŸŸ¡ Intermediate
- **í•œì¤„ìš”ì•½**: ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ segment-level recurrence
- **ì™œ ì½ì–´ì•¼**: ìš°ë¦¬ L1/L2 ì»¨í…ìŠ¤íŠ¸ ê³„ì¸µí™”ì˜ ì´ë¡ ì  ë°°ê²½

#### 2. **Longformer: The Long-Document Transformer** (2020)
- **íŒŒì¼**: `Longformer_Long_Document_2020.md`
- **ì €ì**: Iz Beltagy, et al. (AllenAI)
- **ì¤‘ìš”ë„**: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
- **ë‚œì´ë„**: ğŸŸ¡ Intermediate  
- **í•œì¤„ìš”ì•½**: íš¨ìœ¨ì ì¸ ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ë¥¼ ìœ„í•œ sparse attention
- **ì™œ ì½ì–´ì•¼**: ì‹¤ì‹œê°„ ì œì•½ í•˜ì—ì„œ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë°©ë²•

#### 3. **Neural Episodic Control** (2017)
- **íŒŒì¼**: `Neural_Episodic_Control_2017.md`
- **ì €ì**: Alexander Pritzel, et al. (DeepMind)
- **ì¤‘ìš”ë„**: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
- **ë‚œì´ë„**: ğŸŸ¡ Intermediate
- **í•œì¤„ìš”ì•½**: ì™¸ë¶€ ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•œ episodic í•™ìŠµ
- **ì™œ ì½ì–´ì•¼**: L3 Knowledge ë ˆë²¨ì˜ ì™¸ë¶€ ë©”ëª¨ë¦¬ í™œìš© ë°©ë²•

### ğŸ“– Important Papers (ê¼­ ì½ì–´ë³¼ ê²ƒ)

#### 4. **Memory Networks** (2015)
- **íŒŒì¼**: `Memory_Networks_2015.md`
- **ì €ì**: Jason Weston, et al. (Facebook AI)
- **ì¤‘ìš”ë„**: ğŸ“–ğŸ“–ğŸ“–ğŸ“–
- **ë‚œì´ë„**: ğŸŸ¡ Intermediate
- **í•œì¤„ìš”ì•½**: ì™¸ë¶€ ë©”ëª¨ë¦¬ì™€ attention mechanism ê²°í•©
- **ì™œ ì½ì–´ì•¼**: ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ë¡ ì˜ ê¸°ì´ˆ ê°œë…

#### 5. **Differentiable Neural Computers** (2016)
- **íŒŒì¼**: `Differentiable_Neural_Computers_2016.md`
- **ì €ì**: Alex Graves, et al. (DeepMind)
- **ì¤‘ìš”ë„**: ğŸ“–ğŸ“–ğŸ“–ğŸ“–
- **ë‚œì´ë„**: ğŸ”´ Advanced
- **í•œì¤„ìš”ì•½**: ì½ê¸°/ì“°ê¸° ê°€ëŠ¥í•œ ì™¸ë¶€ ë©”ëª¨ë¦¬ë¥¼ ê°€ì§„ ì‹ ê²½ë§
- **ì™œ ì½ì–´ì•¼**: ë™ì  ë©”ëª¨ë¦¬ ê´€ë¦¬ì˜ ê³ ê¸‰ ê¸°ë²•

#### 6. **Retrieval-Augmented Memory** (2021)
- **íŒŒì¼**: `Retrieval_Augmented_Memory_2021.md`
- **ì €ì**: [Research Team]
- **ì¤‘ìš”ë„**: ğŸ“–ğŸ“–ğŸ“–
- **ë‚œì´ë„**: ğŸŸ¡ Intermediate
- **í•œì¤„ìš”ì–‘**: RAGì™€ episodic memoryì˜ ê²°í•©
- **ì™œ ì½ì–´ì•¼**: RAG + Memory í†µí•© ì ‘ê·¼ë²•

### ğŸ“š Reference Papers (ì°¸ê³ ìš©)

#### 7. **Hierarchical Memory Networks** (2018)
- **íŒŒì¼**: `Hierarchical_Memory_Networks_2018.md`
- **ì €ì**: [Research Team]  
- **ì¤‘ìš”ë„**: ğŸ“šğŸ“šğŸ“š
- **ë‚œì´ë„**: ğŸ”´ Advanced
- **í•œì¤„ìš”ì•½**: ê³„ì¸µì  êµ¬ì¡°ì˜ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
- **ì™œ ì½ì–´ì•¼**: ìš°ë¦¬ì˜ L1/L2/L3 ê³„ì¸µ êµ¬ì¡°ì™€ ìœ ì‚¬í•œ ì ‘ê·¼

#### 8. **Working Memory Networks** (2020)
- **íŒŒì¼**: `Working_Memory_Networks_2020.md`  
- **ì €ì**: [Research Team]
- **ì¤‘ìš”ë„**: ğŸ“šğŸ“šğŸ“š
- **ë‚œì´ë„**: ğŸŸ¡ Intermediate
- **í•œì¤„ìš”ì•½**: ì¸ì§€ê³¼í•™ì˜ working memoryë¥¼ ì‹ ê²½ë§ì— ì ìš©
- **ì™œ ì½ì–´ì•¼**: ë‹¨ê¸°/ì¥ê¸° ë©”ëª¨ë¦¬ êµ¬ë¶„ì˜ ì´ë¡ ì  ë°°ê²½

---

## ğŸ¯ Context-Aware RAG-VLA ì§ì ‘ ì—°ê´€ì„±

### L1 Immediate Context (< 1ì´ˆ)
```python
L1_related_concepts = {
    "Working_Memory": {
        "ë…¼ë¬¸": "Working Memory Networks (2020)",
        "ê°œë…": "ì¦‰ê°ì  ì •ë³´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë‹¨ê¸° ë©”ëª¨ë¦¬",
        "ì ìš©": "ë¡œë´‡ì˜ í˜„ì¬ ìƒíƒœì™€ ì§ì „ ì•¡ì…˜ë“¤ ì €ì¥",
        "êµ¬í˜„": "Fixed-size circular buffer + attention"
    },
    
    "Attention_Window": {
        "ë…¼ë¬¸": "Transformer-XL (2019)",
        "ê°œë…": "ì œí•œëœ attention window ë‚´ ì •ë³´ ì²˜ë¦¬",
        "ì ìš©": "ìµœê·¼ 1ì´ˆ ë‚´ ì„¼ì„œ ë°ì´í„°ì™€ ì•¡ì…˜ì—ë§Œ ì§‘ì¤‘",
        "êµ¬í˜„": "Sliding window attention mechanism"
    }
}
```

### L2 Task Context (< 5ì´ˆ)  
```python
L2_related_concepts = {
    "Segment_Recurrence": {
        "ë…¼ë¬¸": "Transformer-XL (2019)",  
        "ê°œë…": "ì„¸ê·¸ë¨¼íŠ¸ ë‹¨ìœ„ë¡œ ê³¼ê±° ì •ë³´ ì¬ì‚¬ìš©",
        "ì ìš©": "ì„œë¸ŒíƒœìŠ¤í¬ ë‹¨ìœ„ë¡œ ì§„í–‰ìƒí™© ì¶”ì ",
        "êµ¬í˜„": "Task segmentë³„ hidden state ìºì‹±"
    },
    
    "Hierarchical_Memory": {
        "ë…¼ë¬¸": "Hierarchical Memory Networks (2018)",
        "ê°œë…": "ê³„ì¸µì  êµ¬ì¡°ë¡œ ë©”ëª¨ë¦¬ ì¡°ì§í™”", 
        "ì ìš©": "íƒœìŠ¤í¬-ì„œë¸ŒíƒœìŠ¤í¬-ì•¡ì…˜ ê³„ì¸µ êµ¬ì¡°",
        "êµ¬í˜„": "Multi-level memory hierarchy"
    }
}
```

### L3 Knowledge Context (< 10ì´ˆ)
```python  
L3_related_concepts = {
    "Episodic_Memory": {
        "ë…¼ë¬¸": "Neural Episodic Control (2017)",
        "ê°œë…": "ê³¼ê±° ê²½í—˜ì„ episodicìœ¼ë¡œ ì €ì¥/ê²€ìƒ‰",
        "ì ìš©": "ìœ ì‚¬í•œ ìƒí™©ì˜ ê³¼ê±° ì‹¤í–‰ ê¸°ë¡ í™œìš©",
        "êµ¬í˜„": "Experience buffer + nearest neighbor search"
    },
    
    "External_Memory": {
        "ë…¼ë¬¸": "Differentiable Neural Computers (2016)",
        "ê°œë…": "ì‹ ê²½ë§ì—ì„œ ì œì–´ ê°€ëŠ¥í•œ ì™¸ë¶€ ë©”ëª¨ë¦¬",
        "ì ìš©": "ë¡œë´‡ ë§¤ë‰´ì–¼, ì‹¤íŒ¨ ì‚¬ë¡€ ë“± ì§€ì‹ ì €ì¥",
        "êµ¬í˜„": "Vector database + content-based addressing"
    }
}
```

---

## ğŸ“– ì½ê¸° ì „ëµ ë° í•µì‹¬ í¬ì¸íŠ¸

### Week 5: Context & Memory ì‹¬í™”
```python
week5_reading_plan = {
    "Day_1-2": "Transformer-XL - ê¸´ ì»¨í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜",
    "Day_3": "Longformer - íš¨ìœ¨ì  attention ë°©ë²•",  
    "Day_4-5": "Neural Episodic Control - ì™¸ë¶€ ë©”ëª¨ë¦¬ í™œìš©",
    "Day_6": "Memory Networks - ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ë¡ ",
    "Day_7": "ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ ì•„í‚¤í…ì²˜ ì„¤ê³„"
}
```

### ê° ë…¼ë¬¸ë³„ í•µì‹¬ ì§ˆë¬¸

#### Transformer-XL ì½ì„ ë•Œ
**Technical Questions**:
- Q: Segment-level recurrenceê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ê°€?
- Q: Relative positional encodingì˜ ì¥ì ì€?
- Q: Memory ìƒíƒœë¥¼ ì–´ë–»ê²Œ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ”ê°€?

**VLA Application**:
- Q: ë¡œë´‡ íƒœìŠ¤í¬ì—ì„œ segmentë¥¼ ì–´ë–»ê²Œ ì •ì˜í•  ê²ƒì¸ê°€?
- Q: ì•¡ì…˜ ì‹œí€€ìŠ¤ì— relative positionì´ ì¤‘ìš”í•œê°€?
- Q: ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ ì‚¬ì´ì¦ˆ ì œí•œì€?

#### Neural Episodic Control ì½ì„ ë•Œ
**Core Concepts**:
- Q: Episodic memoryì™€ semantic memoryì˜ ì°¨ì´ëŠ”?
- Q: Nearest neighbor searchì˜ íš¨ìœ¨ì„±ì€?
- Q: ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ ì „ëµì€?

**Implementation Ideas**:
- Q: ë¡œë´‡ ì‹¤í–‰ ê¸°ë¡ì„ ì–´ë–¤ í˜•íƒœë¡œ ì €ì¥í• ê¹Œ?
- Q: ìœ ì‚¬ì„± ì¸¡ì •ì„ ìœ„í•œ embedding spaceëŠ”?
- Q: ë©”ëª¨ë¦¬ í¬ê¸° ì œí•œ ì‹œ ì–´ë–¤ ê²ƒì„ ë¨¼ì € ì‚­ì œí• ê¹Œ?

---

## ğŸ’¡ Context-Aware ì•„í‚¤í…ì²˜ ì„¤ê³„

### ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ ì‹œìŠ¤í…œ
```python
class HierarchicalContextManager:
    def __init__(self):
        # L1: Immediate Context (Working Memory)
        self.L1_buffer = CircularBuffer(size=10)  # ìµœê·¼ 10 steps
        self.L1_attention = SlidingWindowAttention(window=5)
        
        # L2: Task Context (Episodic Memory)  
        self.L2_segments = SegmentMemory(max_segments=20)
        self.L2_recurrence = SegmentRecurrence()
        
        # L3: Knowledge Context (External Memory)
        self.L3_episodic = EpisodicMemory(capacity=10000)
        self.L3_semantic = VectorDatabase()
    
    def get_context(self, current_state, urgency_level):
        # Adaptive context selection based on situation
        if urgency_level > 0.8:  # Emergency
            return self.L1_buffer.get_recent(steps=3)
        elif urgency_level > 0.5:  # Normal task
            return self.combine_L1_L2(current_state)  
        else:  # Complex reasoning needed
            return self.combine_all_levels(current_state)
```

### ì ì‘ì  ê²€ìƒ‰ ì „ëµ
```python
class AdaptiveRetrievalPolicy:
    def should_retrieve(self, confidence, task_phase, urgency):
        """ìƒí™©ë³„ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨"""
        if urgency > 0.8:
            return None  # Skip retrieval in emergency
        
        if confidence < 0.7:
            return "L3_knowledge"  # Uncertain -> External knowledge
        elif task_phase == "transition":  
            return "L2_task"  # Task boundary -> Task memory
        else:
            return "L1_immediate"  # Normal -> Recent context only
    
    def select_retrieval_strategy(self, query_type, latency_budget):
        """ì§€ì—°ì‹œê°„ ì˜ˆì‚°ì— ë”°ë¥¸ ê²€ìƒ‰ ì „ëµ ì„ íƒ"""
        if latency_budget < 50:  # ms
            return "cached_lookup"
        elif latency_budget < 200:
            return "approximate_search"  
        else:
            return "full_search"
```

---

## ğŸ§ª ì‹¤í—˜ ì•„ì´ë””ì–´

### Context íš¨ìœ¨ì„± ê²€ì¦
```python
context_experiments = {
    "Context_Window_Size": {
        "ë³€ìˆ˜": "L1 buffer size (5, 10, 20, 50 steps)",
        "ì¸¡ì •": "Performance vs Memory usage",  
        "ê°€ì„¤": "10-20 stepsê°€ ìµœì ì¼ ê²ƒ"
    },
    
    "Hierarchical_vs_Flat": {
        "ë¹„êµ": "L1/L2/L3 ê³„ì¸µ vs ë‹¨ì¼ ë©”ëª¨ë¦¬",
        "ì¸¡ì •": "Task completion rate, Inference latency",
        "ê°€ì„¤": "ê³„ì¸µì  êµ¬ì¡°ê°€ íš¨ìœ¨ì„±ì—ì„œ ìš°ìœ„"
    },
    
    "Adaptive_vs_Fixed": {
        "ë¹„êµ": "ì ì‘ì  ê²€ìƒ‰ vs ê³ ì • ì „ëµ", 
        "ì¸¡ì •": "Success rate vs Retrieval frequency",
        "ê°€ì„¤": "ì ì‘ì  ê²€ìƒ‰ì´ ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ 50% ê°ì†Œ"
    }
}
```

### Memory ì‹œìŠ¤í…œ ìµœì í™”
```python
memory_optimization = {
    "Forgetting_Strategy": {
        "ë°©ë²•": ["LRU", "Importance-based", "Temporal decay"],
        "í‰ê°€": "Long-term performance retention",
        "ëª©í‘œ": "ì¤‘ìš”í•œ ì •ë³´ëŠ” ìœ ì§€, ë…¸ì´ì¦ˆëŠ” ì œê±°"
    },
    
    "Retrieval_Latency": {
        "ê¸°ë²•": ["Approximate search", "Caching", "Indexing"],
        "ëª©í‘œ": "< 10ms average retrieval time", 
        "íŠ¸ë ˆì´ë“œì˜¤í”„": "Accuracy vs Speed"
    }
}
```

---

## ğŸ“Š êµ¬í˜„ ê³ ë ¤ì‚¬í•­

### ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìµœì í™”
```python
realtime_optimizations = {
    "Memory_Budget": {
        "L1": "1MB (immediate buffer)",
        "L2": "10MB (task segments)",  
        "L3": "100MB (cached knowledge)",
        "ì œì•½": "ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ < 200MB"
    },
    
    "Latency_Budget": {
        "L1_access": "< 1ms",
        "L2_search": "< 10ms", 
        "L3_retrieval": "< 100ms",
        "Total": "< 50ms for normal operation"
    },
    
    "Parallelization": {
        "L1_processing": "Main thread",
        "L2_search": "Background thread",
        "L3_retrieval": "Async worker threads"
    }
}
```

---

## ğŸ“‹ ì½ê¸° ì§„ë„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Critical Understanding
- [ ] **Transformer-XL** - Segment recurrence ì™„ì „ ì´í•´ â­â­â­â­â­
- [ ] **Longformer** - Sparse attention ë©”ì»¤ë‹ˆì¦˜ ì´í•´ â­â­â­â­
- [ ] **Neural Episodic Control** - ì™¸ë¶€ ë©”ëª¨ë¦¬ í™œìš©ë²• ì´í•´ â­â­â­â­â­

### Architecture Design
- [ ] **L1/L2/L3 ê³„ì¸µ êµ¬ì¡°** ëª…í™•íˆ ì„¤ê³„
- [ ] **ì ì‘ì  ê²€ìƒ‰ ì •ì±…** êµ¬ì²´ì  ì•Œê³ ë¦¬ì¦˜ 
- [ ] **ì‹¤ì‹œê°„ ì²˜ë¦¬** ìµœì í™” ë°©ì•ˆ ìˆ˜ë¦½
- [ ] **ë©”ëª¨ë¦¬ ê´€ë¦¬ ì „ëµ** êµ¬í˜„ ê³„íš

### Implementation Planning  
- [ ] **ê¸°ìˆ  ìŠ¤íƒ ì„ ì •** (PyTorch, Vector DB ë“±)
- [ ] **ì„±ëŠ¥ ëª©í‘œ ì„¤ì •** (ì§€ì—°ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰)
- [ ] **í‰ê°€ ë°©ë²•ë¡ ** ì„¤ê³„
- [ ] **í”„ë¡œí† íƒ€ì… ê³„íš** ìˆ˜ë¦½

---

## ğŸ”— ê´€ë ¨ ê¸°ìˆ  ë° ë„êµ¬

### Vector Databases
- **ChromaDB**: ë¡œì»¬ ê°œë°œìš©, ê°„ë‹¨í•œ ì„¤ì •
- **Qdrant**: ê³ ì„±ëŠ¥, Rust ê¸°ë°˜  
- **Faiss**: Facebookì˜ ìœ ì‚¬ë„ ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **Annoy**: Spotifyì˜ ê·¼ì‚¬ ìµœê·¼ì ‘ ì´ì›ƒ ê²€ìƒ‰

### Memory-Efficient Transformers
- **FlashAttention**: GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì  attention
- **xFormers**: ë©”ëª¨ë¦¬ ìµœì í™”ëœ transformer êµ¬í˜„
- **Linformer**: Linear complexity attention

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

ì´ í´ë” ì™„ë£Œ í›„:

1. **Context Manager í”„ë¡œí† íƒ€ì…** êµ¬í˜„ ì‹œì‘
2. **Memory ì‹¤í—˜ ì„¤ê³„** - íš¨ìœ¨ì„± vs ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„
3. **ë‹¤ìŒ í´ë”**: í•„ìš”ì— ë”°ë¼ ë‹¤ë¥¸ í´ë” ì„ íƒ
4. **í†µí•© ì„¤ê³„**: VLA + RAG + Context ì „ì²´ ì•„í‚¤í…ì²˜

---

**Contextì™€ Memoryê°€ ìš°ë¦¬ ì—°êµ¬ì˜ í•µì‹¬ ì°¨ë³„í™” í¬ì¸íŠ¸ì…ë‹ˆë‹¤!**

Transformer-XLë¶€í„° ì‹œì‘í•´ì„œ ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ ê°œë…ì„ í™•ì‹¤íˆ ì¡ì•„ë³´ì„¸ìš”! ğŸ§ 

---

*Created: 2025-08-24*  
*Priority: Week 5 Deep Dive*  
*Focus: Context layering + Memory management*