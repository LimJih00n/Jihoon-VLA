# ğŸ§  Memory & Context ìƒì„¸ ì„¤ëª…

## ğŸ“Œ ê°œìš”
Memoryì™€ Context ì‹œìŠ¤í…œì€ AIê°€ ê³¼ê±° ì •ë³´ë¥¼ ì €ì¥í•˜ê³  í™œìš©í•˜ì—¬ í˜„ì¬ ìƒí™©ì„ ì´í•´í•˜ê³  ë¯¸ë˜ í–‰ë™ì„ ê³„íší•˜ëŠ” í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. VLAì—ì„œëŠ” ë³µì¡í•œ ì‘ì—… ìˆ˜í–‰, ì¥ê¸° ì˜ì¡´ì„± ì²˜ë¦¬, ê²½í—˜ ê¸°ë°˜ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

## ğŸ¯ í•µì‹¬ ê°œë…

### 1. ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì˜ ê³„ì¸µ êµ¬ì¡°

#### ì¸ê°„ ê¸°ì–µ vs AI ë©”ëª¨ë¦¬
| ì¸ê°„ ê¸°ì–µ | AI ë©”ëª¨ë¦¬ | íŠ¹ì§• | VLA í™œìš© |
|-----------|-----------|------|----------|
| **ê°ê° ê¸°ì–µ** | Attention Cache | 1-2ì´ˆ, ì›ì‹œ ì •ë³´ | ì¦‰ê°ì ì¸ ì„¼ì„œ ì…ë ¥ |
| **ì‘ì—… ê¸°ì–µ** | Working Memory | 7Â±2 í•­ëª©, í™œì„± ì²˜ë¦¬ | í˜„ì¬ ì‘ì—… ìƒíƒœ |
| **ì¥ê¸° ê¸°ì–µ** | External Memory | ë¬´ì œí•œ, ì˜êµ¬ ì €ì¥ | í•™ìŠµëœ ìŠ¤í‚¬, ê²½í—˜ |

#### ë©”ëª¨ë¦¬ ìš©ëŸ‰ê³¼ ì§€ì† ì‹œê°„
```
Sensory: ~1ì´ˆ, ëŒ€ìš©ëŸ‰ â†’ í•„í„°ë§ í•„ìš”
Working: ~30ì´ˆ, 7Â±2 chunks â†’ ì„ íƒì  ì €ì¥
Long-term: ì˜êµ¬, ë¬´ì œí•œ â†’ íš¨ìœ¨ì  ê²€ìƒ‰ í•„ìš”
```

### 2. Working Memory ì›ë¦¬

#### Miller's Magic Number 7Â±2
ì¸ê°„ì˜ ì‘ì—… ê¸°ì–µ ìš©ëŸ‰ í•œê³„ë¥¼ AIì— ì ìš©:
- **Chunking**: ì •ë³´ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¶ê¸°
- **Rehearsal**: ë°˜ë³µì„ í†µí•œ ìœ ì§€
- **Central Executive**: ì£¼ì˜ í• ë‹¹ ì œì–´

#### Neural Implementation
```python
# Slot-based memory
memory_slots = [slot_1, slot_2, ..., slot_7]
attention_weights = softmax(query @ keys)
output = attention_weights @ values
```

### 3. Long-term Memory ë©”ì»¤ë‹ˆì¦˜

#### Storage Mechanisms
1. **Hebbian Learning**: "í•¨ê»˜ ë°œí™”í•˜ëŠ” ë‰´ëŸ°ì€ í•¨ê»˜ ì—°ê²°ëœë‹¤"
2. **Consolidation**: ë‹¨ê¸° â†’ ì¥ê¸° ì „í™˜
3. **Retrieval**: ì—°ê´€ ê¸°ë°˜ ê²€ìƒ‰

#### Memory Indexing
```python
# Content-based addressing
similarity = cosine_similarity(query, memory_bank)
retrieved = weighted_sum(memory_bank, similarity)

# Location-based addressing
address = hash(key) % memory_size
retrieved = memory_bank[address]
```

### 4. Episodic vs Semantic Memory

#### Episodic Memory (ì—í”¼ì†Œë“œ ê¸°ì–µ)
- **What**: íŠ¹ì • ì‚¬ê±´ì˜ ê¸°ì–µ
- **When/Where**: ì‹œê³µê°„ ì •ë³´ í¬í•¨
- **Example**: "ì–´ì œ ë¹¨ê°„ ê³µì„ ì§‘ì—ˆë‹¤"

#### Semantic Memory (ì˜ë¯¸ ê¸°ì–µ)
- **What**: ì¼ë°˜ì  ì§€ì‹
- **How**: ì ˆì°¨ì  ì§€ì‹
- **Example**: "ê³µì€ ë‘¥ê¸€ë‹¤"

## ğŸ—ï¸ êµ¬í˜„ ë©”ì»¤ë‹ˆì¦˜ ìƒì„¸

### 1. Attention-based Working Memory

#### Key-Value Memory Networks
```python
class KeyValueMemory:
    def __init__(self):
        self.keys = []    # ë©”ëª¨ë¦¬ ì£¼ì†Œ
        self.values = []  # ë©”ëª¨ë¦¬ ë‚´ìš©
    
    def write(self, key, value):
        # Content-based addressing
        weights = softmax(key @ self.keys.T)
        # Weighted update
        self.values = (1-weights) * self.values + weights * value
    
    def read(self, query):
        weights = softmax(query @ self.keys.T)
        return weights @ self.values
```

#### Memory Networks Architecture
1. **Input Module**: ì…ë ¥ ì¸ì½”ë”©
2. **Memory Module**: ì €ì¥ ë° ì—…ë°ì´íŠ¸
3. **Output Module**: ì‘ë‹µ ìƒì„±
4. **Response Module**: ìµœì¢… ì¶œë ¥

### 2. Neural Turing Machine (NTM) ê°œë…

#### ì½ê¸° ì—°ì‚°
```
r_t = Î£_i w_t(i) * M_t(i)
```
- w_t: ì£¼ì˜ ê°€ì¤‘ì¹˜
- M_t: ë©”ëª¨ë¦¬ ë§¤íŠ¸ë¦­ìŠ¤

#### ì“°ê¸° ì—°ì‚°
```
M_t(i) = M_{t-1}(i) * (1 - w_t(i) * e_t) + w_t(i) * a_t
```
- e_t: ì§€ìš°ê¸° ë²¡í„°
- a_t: ì¶”ê°€ ë²¡í„°

### 3. Hierarchical Temporal Memory

#### ì‹œê°„ì  ê³„ì¸µ êµ¬ì¡°
```
Level 1: ë°€ë¦¬ì´ˆ (ì„¼ì„œ ë°ì´í„°)
    â†“ ì¶”ìƒí™”
Level 2: ì´ˆ ë‹¨ìœ„ (ë™ì‘ íŒ¨í„´)
    â†“ ì¶”ìƒí™”
Level 3: ë¶„ ë‹¨ìœ„ (ì‘ì—… ë‹¨ê³„)
    â†“ ì¶”ìƒí™”
Level 4: ì‹œê°„ ë‹¨ìœ„ (ì „ì²´ ì‘ì—…)
```

#### Sparse Distributed Representation
- 2% í™œì„± ë‰´ëŸ°
- ë†’ì€ ìš©ëŸ‰
- ë…¸ì´ì¦ˆ ê°•ê±´ì„±

### 4. Context Management

#### Context Window
```python
class ContextWindow:
    def __init__(self, max_length=2048):
        self.max_length = max_length
        self.buffer = deque(maxlen=max_length)
    
    def update(self, token):
        self.buffer.append(token)
        if len(self.buffer) > self.max_length:
            self.buffer.popleft()  # FIFO
```

#### Hierarchical Context
```
Immediate: ìµœê·¼ 5 í† í°
Local: ìµœê·¼ 50 í† í°
Global: ì „ì²´ ë¬¸ì„œ/ì—í”¼ì†Œë“œ
Meta: ì‘ì—… ìˆ˜ì¤€ ì»¨í…ìŠ¤íŠ¸
```

## ğŸ¤– VLAì—ì„œì˜ ë©”ëª¨ë¦¬ í™œìš©

### 1. Task Memory

#### ì‘ì—… ë‹¨ê³„ ê¸°ì–µ
```python
task_memory = {
    "goal": "assemble product",
    "completed_steps": ["pick_part_A", "pick_part_B"],
    "current_step": "align_parts",
    "remaining_steps": ["insert_screw", "tighten"],
    "context": {"tool": "screwdriver", "location": "workbench"}
}
```

#### Procedural Memory
```python
skill_library = {
    "grasp": GraspSkill(),
    "insert": InsertSkill(),
    "rotate": RotateSkill()
}
```

### 2. Object-Centric Memory

#### Object Permanence
ë¬¼ì²´ê°€ ì‹œì•¼ì—ì„œ ì‚¬ë¼ì ¸ë„ ì¡´ì¬í•œë‹¤ëŠ” ê°œë…:
```python
object_memory = {
    "red_cube": {
        "last_seen": timestamp,
        "location": [x, y, z],
        "properties": {"color": "red", "shape": "cube"},
        "confidence": 0.95
    }
}
```

#### Spatial Memory
```python
spatial_map = {
    "workbench": {"objects": ["tool_1", "part_A"], "free_space": 0.3},
    "storage": {"objects": ["part_B", "part_C"], "free_space": 0.7}
}
```

### 3. Experience Replay

#### Prioritized Experience Replay
```python
priority = |TD_error| + Îµ
probability = priority^Î± / Î£ priority^Î±
```

ì¤‘ìš”í•œ ê²½í—˜ì„ ë” ìì£¼ ì¬ìƒ

#### Hindsight Experience Replay
ì‹¤íŒ¨í•œ ê²½í—˜ë„ ë‹¤ë¥¸ ëª©í‘œ ë‹¬ì„±ìœ¼ë¡œ ì¬í•´ì„:
```python
if not achieved(goal):
    virtual_goal = final_state
    relabel_trajectory(trajectory, virtual_goal)
    store_as_success(trajectory)
```

## ğŸ”¬ ê³ ê¸‰ ë©”ëª¨ë¦¬ ê¸°ë²•

### 1. Memory Consolidation

#### System Consolidation
```python
def consolidate_memory(short_term, long_term):
    # Replay during "sleep"
    for memory in short_term:
        if is_important(memory):
            compressed = compress(memory)
            long_term.store(compressed)
    
    # Synaptic consolidation
    strengthen_connections(frequently_accessed)
    weaken_connections(rarely_accessed)
```

#### Memory Compression
```python
def compress_episode(episode):
    # Extract key frames
    keyframes = detect_keyframes(episode)
    # Abstract representation
    abstract = encode_abstract(keyframes)
    # Store compressed
    return {"keyframes": keyframes, "abstract": abstract}
```

### 2. Associative Memory

#### Hopfield Networks
```python
class HopfieldMemory:
    def __init__(self, patterns):
        # Store patterns in weights
        self.W = sum([p @ p.T for p in patterns])
        np.fill_diagonal(self.W, 0)
    
    def recall(self, partial):
        # Iterative retrieval
        state = partial
        while not converged:
            state = sign(self.W @ state)
        return state
```

#### Content-Addressable Memory
```python
def content_addressable_retrieve(query, memory):
    # Retrieve by content, not address
    best_match = None
    max_similarity = -inf
    
    for item in memory:
        similarity = compute_similarity(query, item.content)
        if similarity > max_similarity:
            max_similarity = similarity
            best_match = item
    
    return best_match
```

### 3. Forgetting Mechanisms

#### Adaptive Forgetting
```python
def adaptive_forget(memory_bank, capacity):
    if len(memory_bank) > capacity:
        # Compute importance scores
        importance = []
        for memory in memory_bank:
            score = memory.access_frequency * memory.recency * memory.relevance
            importance.append(score)
        
        # Remove least important
        threshold = percentile(importance, 20)
        memory_bank = [m for m, i in zip(memory_bank, importance) if i > threshold]
    
    return memory_bank
```

#### Catastrophic Forgetting Prevention
```python
class ElasticWeightConsolidation:
    def __init__(self, model):
        self.model = model
        self.fisher_matrix = {}  # Importance of parameters
    
    def penalty(self):
        loss = 0
        for name, param in self.model.named_parameters():
            if name in self.fisher_matrix:
                loss += (self.fisher_matrix[name] * 
                        (param - self.old_params[name])**2).sum()
        return loss
```

## ğŸ’¡ ì‹¤ì „ ì ìš© ê°€ì´ë“œ

### 1. ë©”ëª¨ë¦¬ í¬ê¸° ì„ íƒ

#### Working Memory
- **ì‘ì€ ì‘ì—…**: 3-5 ìŠ¬ë¡¯
- **ë³µì¡í•œ ì‘ì—…**: 7-10 ìŠ¬ë¡¯
- **ë‹¤ì¤‘ ì‘ì—…**: 10-15 ìŠ¬ë¡¯

#### Long-term Memory
- **ì œí•œëœ ë„ë©”ì¸**: 100-1000 í•­ëª©
- **ê°œë°©í˜• ë„ë©”ì¸**: 10000+ í•­ëª©
- **ì¦ë¶„ í•™ìŠµ**: ë™ì  í™•ì¥

### 2. ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ ì „ëµ

#### Write Strategies
1. **FIFO**: ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ êµì²´
2. **LRU**: ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ ê²ƒ êµì²´
3. **LFU**: ê°€ì¥ ì ì€ ë¹ˆë„ êµì²´
4. **Priority**: ì¤‘ìš”ë„ ê¸°ë°˜ êµì²´

#### Read Strategies
1. **Exact Match**: ì •í™•í•œ ë§¤ì¹­
2. **Similarity**: ìœ ì‚¬ë„ ê¸°ë°˜
3. **Temporal**: ì‹œê°„ ê¸°ë°˜
4. **Associative**: ì—°ê´€ ê¸°ë°˜

### 3. ì„±ëŠ¥ ìµœì í™”

#### Memory Indexing
```python
class EfficientMemory:
    def __init__(self):
        self.index = faiss.IndexFlatL2(dim)  # Fast similarity search
        self.metadata = {}
    
    def add(self, vector, meta):
        idx = self.index.ntotal
        self.index.add(vector)
        self.metadata[idx] = meta
    
    def search(self, query, k=5):
        distances, indices = self.index.search(query, k)
        return [(self.metadata[i], d) for i, d in zip(indices[0], distances[0])]
```

#### Caching
```python
@lru_cache(maxsize=128)
def expensive_retrieval(query):
    return search_in_memory(query)
```

## ğŸš€ ìµœì‹  ì—°êµ¬ ë™í–¥

### 1. Transformer Memory
- **Transformer-XL**: Segment-level recurrence
- **Compressive Transformer**: ì••ì¶•ëœ ë©”ëª¨ë¦¬
- **âˆ-former**: Infinite memory transformer

### 2. Neuromorphic Memory
- **Spiking Neural Networks**: ìƒë¬¼í•™ì  ë©”ëª¨ë¦¬
- **Memristor**: í•˜ë“œì›¨ì–´ ë©”ëª¨ë¦¬
- **Neuromorphic Chips**: ë‡Œ ëª¨ë°© ì¹©

### 3. Quantum Memory
- **Quantum Associative Memory**: ì–‘ì ì¤‘ì²©
- **Quantum RAM**: ì§€ìˆ˜ì  ìš©ëŸ‰
- **Quantum Error Correction**: ë…¸ì´ì¦ˆ ì²˜ë¦¬

## âš ï¸ ì£¼ì˜ì‚¬í•­ ë° í•œê³„

### ì£¼ìš” ë¬¸ì œì 
1. **Memory Overflow**: ìš©ëŸ‰ ì´ˆê³¼
2. **Memory Leak**: ë¶ˆí•„ìš”í•œ ì •ë³´ ì¶•ì 
3. **Retrieval Failure**: ê²€ìƒ‰ ì‹¤íŒ¨
4. **Context Confusion**: ë¬¸ë§¥ í˜¼ë™

### í•´ê²° ë°©ì•ˆ
1. **Garbage Collection**: ì£¼ê¸°ì  ì •ë¦¬
2. **Memory Compression**: ì••ì¶• ì €ì¥
3. **Redundant Storage**: ì¤‘ë³µ ì €ì¥
4. **Error Correction**: ì˜¤ë¥˜ ìˆ˜ì •

## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ

### í•µì‹¬ ë…¼ë¬¸
- "Neural Turing Machines" (Graves et al.)
- "Memory Networks" (Weston et al.)
- "Differentiable Neural Computer" (Graves et al.)

### êµ¬í˜„ ë¼ì´ë¸ŒëŸ¬ë¦¬
- Faiss (Facebook): ë²¡í„° ê²€ìƒ‰
- Annoy (Spotify): ê·¼ì‚¬ ìµœê·¼ì ‘ ì´ì›ƒ
- HNSW: ê³„ì¸µì  íƒìƒ‰

### ë²¤ì¹˜ë§ˆí¬
- bAbI tasks: ì¶”ë¡ ê³¼ ë©”ëª¨ë¦¬
- CLEVR: ì‹œê°ì  ì¶”ë¡ 
- Meta-World: ë¡œë´‡ ì‘ì—…

## ğŸ¯ í•µì‹¬ ìš”ì•½

ë©”ëª¨ë¦¬ì™€ ì»¨í…ìŠ¤íŠ¸ ì‹œìŠ¤í…œì€ VLAê°€ ê³¼ê±° ê²½í—˜ì„ í™œìš©í•˜ê³  ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” í•µì‹¬ì…ë‹ˆë‹¤. Working MemoryëŠ” ì¦‰ê°ì ì¸ ì‘ì—… ì²˜ë¦¬ë¥¼, Long-term MemoryëŠ” ì§€ì‹ ì¶•ì ì„, Episodic MemoryëŠ” ê²½í—˜ ê¸°ë°˜ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. íš¨ìœ¨ì ì¸ ì €ì¥, ê²€ìƒ‰, ì—…ë°ì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ê³¼ í•¨ê»˜ ê³„ì¸µì  êµ¬ì¡°ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì‹œê°„ ìŠ¤ì¼€ì¼ì˜ ì •ë³´ë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ì„±ê³µì ì¸ êµ¬í˜„ì˜ ì—´ì‡ ì…ë‹ˆë‹¤.