# ğŸ® Reinforcement Learning ìƒì„¸ ì„¤ëª…

## ğŸ“Œ ê°œìš”
Reinforcement Learning(ê°•í™”í•™ìŠµ)ì€ ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•´ ìµœì ì˜ í–‰ë™ ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. VLA ì‹œìŠ¤í…œì—ì„œëŠ” ë¡œë´‡ì´ ì‹¤ì œ í™˜ê²½ì—ì„œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë©° ìŠ¤ìŠ¤ë¡œ ê°œì„ í•˜ëŠ” í•µì‹¬ í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.

## ğŸ¯ í•µì‹¬ ê°œë…

### 1. ê°•í™”í•™ìŠµì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ

#### MDP (Markov Decision Process)
ê°•í™”í•™ìŠµ ë¬¸ì œë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•˜ëŠ” í”„ë ˆì„ì›Œí¬:

- **State (S)**: í™˜ê²½ì˜ í˜„ì¬ ìƒíƒœ
- **Action (A)**: ì—ì´ì „íŠ¸ê°€ ì·¨í•  ìˆ˜ ìˆëŠ” í–‰ë™
- **Transition (P)**: ìƒíƒœ ì „ì´ í™•ë¥  P(s'|s,a)
- **Reward (R)**: ì¦‰ê°ì  ë³´ìƒ R(s,a,s')
- **Discount (Î³)**: ë¯¸ë˜ ë³´ìƒì˜ í• ì¸ìœ¨

#### Markov Property
```
P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1} | s_t, a_t)
```
í˜„ì¬ ìƒíƒœë§Œìœ¼ë¡œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡ ê°€ëŠ¥ (ê³¼ê±° ì´ë ¥ ë¶ˆí•„ìš”)

### 2. ê°€ì¹˜ í•¨ìˆ˜ì™€ ì •ì±…

#### State Value Function V(s)
```
V^Ï€(s) = E_Ï€[Î£_{t=0}^âˆ Î³^t R_t | s_0 = s]
```
ì •ì±… Ï€ë¥¼ ë”°ë¥¼ ë•Œ ìƒíƒœ sì—ì„œ ê¸°ëŒ€ë˜ëŠ” ëˆ„ì  ë³´ìƒ

#### Action Value Function Q(s,a)
```
Q^Ï€(s,a) = E_Ï€[Î£_{t=0}^âˆ Î³^t R_t | s_0 = s, a_0 = a]
```
ìƒíƒœ sì—ì„œ í–‰ë™ aë¥¼ ì·¨í•œ í›„ ì •ì±… Ï€ë¥¼ ë”°ë¥¼ ë•Œì˜ ê¸°ëŒ€ ë³´ìƒ

#### Policy Ï€(a|s)
- **Deterministic**: Ï€(s) = a (ê²°ì •ì )
- **Stochastic**: Ï€(a|s) = P(a|s) (í™•ë¥ ì )

### 3. Bellman Equations

#### Bellman Expectation Equation
```
V^Ï€(s) = Î£_a Ï€(a|s) Î£_s' P(s'|s,a)[R(s,a,s') + Î³V^Ï€(s')]
```

#### Bellman Optimality Equation
```
V*(s) = max_a Î£_s' P(s'|s,a)[R(s,a,s') + Î³V*(s')]
Q*(s,a) = Î£_s' P(s'|s,a)[R(s,a,s') + Î³ max_a' Q*(s',a')]
```

## ğŸ—ï¸ ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ ìƒì„¸

### 1. Value-Based Methods

#### Q-Learning
**Off-policy TD control:**
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]
```

**íŠ¹ì§•:**
- Model-free: í™˜ê²½ ëª¨ë¸ ë¶ˆí•„ìš”
- Off-policy: í–‰ë™ ì •ì±…ê³¼ í•™ìŠµ ì •ì±… ë¶„ë¦¬
- Convergence: ì¡°ê±´ ë§Œì¡± ì‹œ ìˆ˜ë ´ ë³´ì¥

#### Deep Q-Network (DQN)
**ì£¼ìš” í˜ì‹ :**
1. **Experience Replay**: ìƒê´€ê´€ê³„ ì œê±°, ë°ì´í„° íš¨ìœ¨ì„±
2. **Target Network**: í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
3. **CNN Integration**: ê³ ì°¨ì› ì…ë ¥ ì²˜ë¦¬

**Loss Function:**
```
L = E[(r + Î³ max_a' Q_target(s',a') - Q(s,a))Â²]
```

#### DQN ê°œì„  ê¸°ë²•
- **Double DQN**: Overestimation bias í•´ê²°
- **Dueling DQN**: V(s)ì™€ A(s,a) ë¶„ë¦¬
- **Prioritized Replay**: ì¤‘ìš”í•œ ê²½í—˜ ìš°ì„  í•™ìŠµ
- **Rainbow**: ëª¨ë“  ê°œì„  ê¸°ë²• í†µí•©

### 2. Policy-Based Methods

#### Policy Gradient Theorem
```
âˆ‡_Î¸ J(Î¸) = E_Ï€[âˆ‡_Î¸ log Ï€_Î¸(a|s) Q^Ï€(s,a)]
```

#### REINFORCE Algorithm
**Monte Carlo Policy Gradient:**
```
Î¸ â† Î¸ + Î± âˆ‡_Î¸ log Ï€_Î¸(a_t|s_t) G_t
```
- G_t: ì‹œì  të¶€í„°ì˜ ëˆ„ì  ë³´ìƒ
- Varianceê°€ ë†’ìŒ â†’ Baseline ì‚¬ìš©

#### Baseline and Advantage
```
A(s,a) = Q(s,a) - V(s)
```
Advantage functionìœ¼ë¡œ variance ê°ì†Œ

### 3. Actor-Critic Methods

#### Architecture
- **Actor**: ì •ì±… Ï€_Î¸(a|s) í•™ìŠµ
- **Critic**: ê°€ì¹˜ í•¨ìˆ˜ V_w(s) í•™ìŠµ

#### A2C (Advantage Actor-Critic)
```
Actor Loss: L_actor = -E[log Ï€_Î¸(a|s) A(s,a)]
Critic Loss: L_critic = E[(R + Î³V(s') - V(s))Â²]
```

#### A3C (Asynchronous A2C)
- ë³‘ë ¬ í™˜ê²½ì—ì„œ ë¹„ë™ê¸° í•™ìŠµ
- ë‹¤ì–‘í•œ ê²½í—˜ ìˆ˜ì§‘
- í•™ìŠµ ì†ë„ í–¥ìƒ

### 4. Advanced Policy Optimization

#### Trust Region Methods
**ë¬¸ì œ**: Policy updateê°€ ë„ˆë¬´ í¬ë©´ ì„±ëŠ¥ ì•…í™”

**TRPO (Trust Region Policy Optimization):**
```
maximize E[Ï€_Î¸/Ï€_Î¸_old * A]
subject to KL(Ï€_Î¸_old || Ï€_Î¸) â‰¤ Î´
```

#### PPO (Proximal Policy Optimization)
**Clipped Surrogate Objective:**
```
L_clip = E[min(r_t(Î¸)A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)A_t)]
where r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)
```

**ì¥ì :**
- TRPOë³´ë‹¤ ê°„ë‹¨í•œ êµ¬í˜„
- ì•ˆì •ì ì¸ í•™ìŠµ
- ë†’ì€ ì„±ëŠ¥

## ğŸ¤– ë¡œë´‡ ì œì–´ë¥¼ ìœ„í•œ RL

### 1. Continuous Action Space

#### DDPG (Deep Deterministic Policy Gradient)
- Deterministic policy for continuous control
- Actor-Critic with replay buffer
- Target networks for stability

#### TD3 (Twin Delayed DDPG)
**ê°œì„ ì‚¬í•­:**
1. Twin Critics: Overestimation ë°©ì§€
2. Delayed Policy Updates: ì•ˆì •ì„±
3. Target Policy Smoothing: ë…¸ì´ì¦ˆ ì¶”ê°€

#### SAC (Soft Actor-Critic)
**Maximum Entropy RL:**
```
J = E[Î£_t (r_t + Î±H(Ï€(Â·|s_t)))]
```
- Entropy regularization
- íƒí—˜ê³¼ í™œìš© ìë™ ê· í˜•
- Sample efficient

### 2. ê³„ì¸µì  ê°•í™”í•™ìŠµ (HRL)

#### Options Framework
- **Option**: ì„œë¸Œ ì •ì±… (skill)
- **Initiation Set**: ì˜µì…˜ ì‹œì‘ ì¡°ê±´
- **Termination Condition**: ì¢…ë£Œ ì¡°ê±´
- **Policy**: ì˜µì…˜ ë‚´ë¶€ ì •ì±…

#### HAC (Hierarchical Actor-Critic)
- Multi-level hierarchy
- Goal-conditioned policies
- Hindsight Experience Replay

### 3. ì•ˆì „í•œ ê°•í™”í•™ìŠµ

#### Constrained MDP
```
maximize E[Î£_t Î³^t r_t]
subject to E[Î£_t Î³^t c_t] â‰¤ C
```
- c_t: Cost/constraint violation
- C: Safety threshold

#### Safe Exploration Strategies
1. **Action Masking**: ìœ„í—˜í•œ í–‰ë™ ì°¨ë‹¨
2. **Reward Shaping**: ì•ˆì „ í–‰ë™ ìœ ë„
3. **Shield**: ì•ˆì „ ë³´ì¥ ë ˆì´ì–´
4. **Risk-Sensitive RL**: CVaR ìµœì í™”

## ğŸ”¬ í•µì‹¬ ê¸°ìˆ  ìƒì„¸

### 1. Exploration vs Exploitation

#### Îµ-greedy
```python
if random() < Îµ:
    action = random_action()
else:
    action = argmax_a Q(s,a)
```

#### UCB (Upper Confidence Bound)
```
a_t = argmax_a [Q(s,a) + câˆš(ln(t)/N(s,a))]
```

#### Thompson Sampling
- Bayesian approach
- Sample from posterior
- Natural exploration

#### Intrinsic Motivation
- **Curiosity**: ì˜ˆì¸¡ ì˜¤ì°¨ ê¸°ë°˜
- **Empowerment**: ì œì–´ ê°€ëŠ¥ì„± ìµœëŒ€í™”
- **Count-based**: ë°©ë¬¸ íšŸìˆ˜ ê¸°ë°˜

### 2. Credit Assignment Problem

#### Temporal Credit Assignment
- **Eligibility Traces**: ê³¼ê±° ìƒíƒœ-í–‰ë™ ê¸°ì—¬ë„
- **TD(Î»)**: Monte Carloì™€ TD í˜¼í•©
```
V(s) â† V(s) + Î±[G_t^Î» - V(s)]
where G_t^Î» = (1-Î»)Î£_{n=1}^âˆ Î»^{n-1} G_t^{(n)}
```

#### Structural Credit Assignment
- Which parts of the policy contributed?
- Attention mechanisms
- Modular networks

### 3. Sample Efficiency

#### Model-Based RL
**ì¥ì :**
- ì ì€ ìƒ˜í”Œë¡œ í•™ìŠµ
- ê³„íš ê°€ëŠ¥

**ë°©ë²•:**
1. **Dyna**: ëª¨ë¸ í•™ìŠµ + ê³„íš
2. **MBPO**: Model-based policy optimization
3. **World Models**: ì ì¬ ê³µê°„ ëª¨ë¸

#### Off-Policy Learning
- ê³¼ê±° ê²½í—˜ ì¬ì‚¬ìš©
- Importance sampling correction
```
Ï = Ï€(a|s) / b(a|s)
```

#### Transfer Learning
- **Domain Adaptation**: ì‹œë®¬ë ˆì´ì…˜ â†’ ì‹¤ì œ
- **Multi-task Learning**: ê³µìœ  í‘œí˜„
- **Meta-Learning**: ë¹ ë¥¸ ì ì‘

## ğŸ’¡ ì‹¤ì „ ì ìš© ê°€ì´ë“œ

### 1. ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ê¸°ì¤€

#### ì´ì‚° í–‰ë™ ê³µê°„
- **ê°„ë‹¨í•œ í™˜ê²½**: Q-Learning, DQN
- **ë³µì¡í•œ í™˜ê²½**: Rainbow DQN
- **ë¶€ë¶„ ê´€ì°°**: Recurrent DQN

#### ì—°ì† í–‰ë™ ê³µê°„
- **ì•ˆì •ì„± ì¤‘ìš”**: TD3, SAC
- **ìƒ˜í”Œ íš¨ìœ¨ì„±**: SAC, MBPO
- **ì‹¤ì‹œê°„**: PPO

#### ë©€í‹°íƒœìŠ¤í¬
- **ê³µìœ  í‘œí˜„**: Multi-task PPO
- **ê³„ì¸µì  êµ¬ì¡°**: HAC, Options
- **ë©”íƒ€ í•™ìŠµ**: MAML, Reptile

### 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

#### Learning Rate
- **Policy**: 3e-4 (ì¼ë°˜ì )
- **Value**: 1e-3 (ë” ë†’ê²Œ)
- **Schedule**: Decay or cyclic

#### Discount Factor (Î³)
- **Short horizon**: 0.9-0.95
- **Long horizon**: 0.99-0.999
- **Infinite**: 0.999

#### Exploration
- **Îµ-greedy**: 1.0 â†’ 0.01 decay
- **Temperature**: ìë™ ì¡°ì • (SAC)
- **Noise**: OU process (DDPG)

### 3. ë””ë²„ê¹… ë° í‰ê°€

#### í•™ìŠµ ê³¡ì„  ë¶„ì„
1. **Reward**: ì¦ê°€ ì¶”ì„¸ í™•ì¸
2. **Value Loss**: ê°ì†Œ ë° ìˆ˜ë ´
3. **Policy Entropy**: ì ì ˆí•œ íƒí—˜
4. **Gradient Norm**: ë°œì‚° ì²´í¬

#### ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°
1. **í•™ìŠµ ì•ˆ ë¨**: 
   - Reward scale ì¡°ì •
   - Network capacity ì¦ê°€
   - Learning rate ì¡°ì •

2. **ë¶ˆì•ˆì •í•œ í•™ìŠµ**:
   - Gradient clipping
   - Target network ì‚¬ìš©
   - Batch normalization

3. **ê³¼ì í•©**:
   - Regularization
   - Dropout
   - ë” ë§ì€ í™˜ê²½ ë³€í™”

## ğŸš€ ìµœì‹  ì—°êµ¬ ë™í–¥

### 1. Offline RL
- **CQL**: Conservative Q-Learning
- **IQL**: Implicit Q-Learning
- **Decision Transformer**: Sequence modeling

### 2. World Models
- **Dreamer**: ì ì¬ ê³µê°„ ê³„íš
- **PlaNet**: í”½ì…€ì—ì„œ ê³„íš
- **MuZero**: ëª¨ë¸ ê¸°ë°˜ ê²€ìƒ‰

### 3. Multi-Agent RL
- **QMIX**: Centralized training
- **MADDPG**: Multi-agent DDPG
- **CommNet**: Communication

## âš ï¸ ì£¼ì˜ì‚¬í•­ ë° íŒ

### ì‹¤ì œ ë¡œë´‡ ì ìš© ì‹œ
1. **ì‹œë®¬ë ˆì´ì…˜ ë¨¼ì €**: Sim2Real transfer
2. **ì•ˆì „ ì œì•½**: Hard constraints
3. **ì ì§„ì  í•™ìŠµ**: Curriculum learning
4. **Human oversight**: ì´ˆê¸° ë‹¨ê³„ ê°ë…

### ì„±ëŠ¥ ìµœì í™”
1. **Vectorized environments**: ë³‘ë ¬ ì²˜ë¦¬
2. **JIT compilation**: ì†ë„ í–¥ìƒ
3. **GPU utilization**: ë°°ì¹˜ ì²˜ë¦¬
4. **Distributed training**: ëŒ€ê·œëª¨ í•™ìŠµ

## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ

### í•µì‹¬ ë…¼ë¬¸
- "Playing Atari with Deep RL" (DQN)
- "Proximal Policy Optimization" (PPO)
- "Soft Actor-Critic" (SAC)

### ë¼ì´ë¸ŒëŸ¬ë¦¬
- Stable Baselines3
- RLlib (Ray)
- Tianshou

### ì‹œë®¬ë ˆì´í„°
- OpenAI Gym
- PyBullet
- MuJoCo

## ğŸ¯ í•µì‹¬ ìš”ì•½

ê°•í™”í•™ìŠµì€ ë¡œë´‡ì´ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ìµœì ì˜ í–‰ë™ì„ í•™ìŠµí•˜ëŠ” ê°•ë ¥í•œ ë°©ë²•ì…ë‹ˆë‹¤. Value-based (DQN), Policy-based (PPO), Actor-Critic (SAC) ë“± ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì´ ìˆìœ¼ë©°, ê°ê°ì˜ ì¥ë‹¨ì ì„ ì´í•´í•˜ê³  ì ì ˆíˆ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì‹¤ì œ ë¡œë´‡ ì ìš© ì‹œì—ëŠ” ì•ˆì „ì„±, ìƒ˜í”Œ íš¨ìœ¨ì„±, ì•ˆì •ì„±ì„ ê³ ë ¤í•´ì•¼ í•˜ë©°, ì‹œë®¬ë ˆì´ì…˜ì—ì„œì˜ ì¶©ë¶„í•œ ê²€ì¦ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.