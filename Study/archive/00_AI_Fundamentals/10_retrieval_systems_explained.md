# ğŸ” Retrieval Systems ìƒì„¸ ì„¤ëª…

## ğŸ“Œ ê°œìš”
Retrieval Systemì€ ëŒ€ê·œëª¨ ì •ë³´ ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. VLAì—ì„œëŠ” ê³¼ê±° ê²½í—˜, í•™ìŠµëœ ìŠ¤í‚¬, ìœ ì‚¬í•œ ìƒí™©ì„ ë¹ ë¥´ê²Œ ì°¾ì•„ í˜„ì¬ ì‘ì—…ì— í™œìš©í•˜ëŠ” í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¡œ, RAG(Retrieval-Augmented Generation) íŒ¨í„´ì˜ ê¸°ë°˜ì´ ë©ë‹ˆë‹¤.

## ğŸ¯ í•µì‹¬ ê°œë…

### 1. ê²€ìƒ‰ì˜ íŒ¨ëŸ¬ë‹¤ì„ ë³€í™”

#### ì „í†µì  ê²€ìƒ‰ vs ì˜ë¯¸ì  ê²€ìƒ‰
| êµ¬ë¶„ | ì „í†µì  ê²€ìƒ‰ | ì˜ë¯¸ì  ê²€ìƒ‰ |
|------|------------|------------|
| **ë§¤ì¹­ ë°©ì‹** | í‚¤ì›Œë“œ ì •í™• ë§¤ì¹­ | ì˜ë¯¸ ìœ ì‚¬ë„ |
| **í‘œí˜„** | ë‹¨ì–´/í† í° | ë²¡í„° ì„ë² ë”© |
| **ì¥ì ** | ì •í™•, í•´ì„ ê°€ëŠ¥ | ìœ ì—°, ë¬¸ë§¥ ì´í•´ |
| **ë‹¨ì ** | ë™ì˜ì–´ ì¸ì‹ ë¶ˆê°€ | ê³„ì‚° ë¹„ìš© |

#### ê²€ìƒ‰ í’ˆì§ˆ ìš”ì†Œ
```
Relevance (ê´€ë ¨ì„±) = Semantic Similarity Ã— Freshness Ã— Authority
```

### 2. Dense Retrieval ì›ë¦¬

#### ë²¡í„° ì„ë² ë”©
```python
text â†’ encoder â†’ d-dimensional vector
"ë¹¨ê°„ ê³µ" â†’ [0.2, -0.5, 0.8, ...]
"ë¶‰ì€ êµ¬ì²´" â†’ [0.19, -0.48, 0.79, ...]
```

#### ìœ ì‚¬ë„ ë©”íŠ¸ë¦­
1. **Cosine Similarity**
   ```
   sim(a,b) = (aÂ·b) / (||a|| Ã— ||b||)
   ë²”ìœ„: [-1, 1], 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬
   ```

2. **Euclidean Distance**
   ```
   dist(a,b) = âˆšÎ£(aáµ¢ - báµ¢)Â²
   ë²”ìœ„: [0, âˆ), 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬
   ```

3. **Dot Product**
   ```
   score(a,b) = Î£(aáµ¢ Ã— báµ¢)
   ë²”ìœ„: (-âˆ, âˆ), í´ìˆ˜ë¡ ìœ ì‚¬
   ```

### 3. Sparse Retrieval ì›ë¦¬

#### BM25 (Best Matching 25)
```
score(q,d) = Î£ IDF(qáµ¢) Ã— (f(qáµ¢,d) Ã— (kâ‚+1)) / (f(qáµ¢,d) + kâ‚Ã—(1-b+bÃ—|d|/avgdl))
```

êµ¬ì„± ìš”ì†Œ:
- **TF (Term Frequency)**: ë¬¸ì„œ ë‚´ ë‹¨ì–´ ë¹ˆë„
- **IDF (Inverse Document Frequency)**: ë‹¨ì–´ì˜ í¬ê·€ì„±
- **ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™”**: ê¸´ ë¬¸ì„œ ë¶ˆì´ìµ ë°©ì§€

#### TF-IDF
```
TF-IDF(t,d,D) = TF(t,d) Ã— IDF(t,D)
where IDF(t,D) = log(|D| / |{dâˆˆD: tâˆˆd}|)
```

### 4. Vector Database êµ¬ì¡°

#### ì¸ë±ìŠ¤ ìœ í˜•

1. **Flat Index**
   - ì„ í˜• íƒìƒ‰
   - ì •í™•í•˜ì§€ë§Œ ëŠë¦¼
   - O(n) ë³µì¡ë„

2. **IVF (Inverted File)**
   - í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜
   - Voronoi cells
   - O(âˆšn) ë³µì¡ë„

3. **HNSW (Hierarchical Navigable Small World)**
   - ê·¸ë˜í”„ ê¸°ë°˜
   - ê³„ì¸µì  êµ¬ì¡°
   - O(log n) ë³µì¡ë„

4. **LSH (Locality Sensitive Hashing)**
   - í•´ì‹œ ê¸°ë°˜
   - í™•ë¥ ì  ê·¼ì‚¬
   - O(1) ë³µì¡ë„

## ğŸ—ï¸ êµ¬í˜„ ë©”ì»¤ë‹ˆì¦˜ ìƒì„¸

### 1. íš¨ìœ¨ì ì¸ ë²¡í„° ê²€ìƒ‰

#### FAISS (Facebook AI Similarity Search)
```python
import faiss
import numpy as np

# Index types
index_flat = faiss.IndexFlatL2(d)  # Exact search
index_ivf = faiss.IndexIVFFlat(quantizer, d, nlist)  # Approximate
index_hnsw = faiss.IndexHNSWFlat(d, M)  # Graph-based

# GPU acceleration
gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)
```

#### Annoy (Approximate Nearest Neighbors Oh Yeah)
```python
from annoy import AnnoyIndex

index = AnnoyIndex(f, 'angular')  # or 'euclidean'
for i, v in enumerate(vectors):
    index.add_item(i, v)
index.build(n_trees)  # More trees = better accuracy
```

### 2. Hybrid Retrieval ì „ëµ

#### Score Fusion
```python
def reciprocal_rank_fusion(rankings, k=60):
    """RRF: ì—¬ëŸ¬ ë­í‚¹ ê²°í•©"""
    scores = {}
    for ranking in rankings:
        for rank, doc_id in enumerate(ranking):
            if doc_id not in scores:
                scores[doc_id] = 0
            scores[doc_id] += 1 / (k + rank + 1)
    return sorted(scores.items(), key=lambda x: x[1], reverse=True)
```

#### Late Interaction
```python
class ColBERT:
    """Late interaction for efficient retrieval"""
    def score(query_embeds, doc_embeds):
        # MaxSim: max similarity for each query token
        scores = []
        for q_emb in query_embeds:
            max_sim = max(cosine_sim(q_emb, d_emb) for d_emb in doc_embeds)
            scores.append(max_sim)
        return sum(scores)
```

### 3. í•™ìŠµ ê°€ëŠ¥í•œ ê²€ìƒ‰

#### Dense Passage Retrieval (DPR)
```python
class DPR:
    def __init__(self):
        self.query_encoder = BertModel()
        self.passage_encoder = BertModel()
    
    def train(self, queries, positive_passages, negative_passages):
        q_emb = self.query_encoder(queries)
        pos_emb = self.passage_encoder(positive_passages)
        neg_emb = self.passage_encoder(negative_passages)
        
        # Contrastive loss
        pos_scores = (q_emb * pos_emb).sum(dim=-1)
        neg_scores = (q_emb * neg_emb).sum(dim=-1)
        loss = -log(exp(pos_scores) / (exp(pos_scores) + Î£exp(neg_scores)))
```

#### Cross-Encoder Reranking
```python
class CrossEncoder:
    """ì •ë°€ ì¬ìˆœìœ„í™”"""
    def rerank(self, query, candidates):
        scores = []
        for doc in candidates:
            input_text = f"{query} [SEP] {doc}"
            score = self.model(input_text)
            scores.append(score)
        return sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
```

## ğŸ¤– VLAì—ì„œì˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ

### 1. Experience Retrieval

#### ì—í”¼ì†Œë“œ ê²€ìƒ‰
```python
class EpisodeRetrieval:
    def __init__(self):
        self.episode_encoder = TransformerEncoder()
        self.index = faiss.IndexFlatIP(768)
    
    def encode_episode(self, states, actions, rewards):
        # Temporal encoding
        trajectory = torch.stack([s for s in states])
        encoded = self.episode_encoder(trajectory)
        
        # Add outcome information
        success_weight = rewards.sum() / len(rewards)
        weighted = encoded * success_weight
        
        return weighted.mean(dim=0)  # Aggregate
```

#### ì„œë¸Œê³¨ ê²€ìƒ‰
```python
def retrieve_subgoals(current_state, goal, subgoal_library):
    """ì¤‘ê°„ ëª©í‘œ ê²€ìƒ‰"""
    # Encode current and goal
    gap_vector = goal_encoder(goal) - state_encoder(current_state)
    
    # Find similar gaps in library
    similar_gaps = subgoal_library.search(gap_vector, k=5)
    
    # Extract subgoal sequence
    subgoals = []
    for gap in similar_gaps:
        subgoals.extend(gap['subgoal_sequence'])
    
    return deduplicate_and_order(subgoals)
```

### 2. Skill Library Management

#### ê³„ì¸µì  ìŠ¤í‚¬ ê²€ìƒ‰
```python
class HierarchicalSkillLibrary:
    def __init__(self):
        self.primitive_skills = VectorDatabase()  # ê¸°ë³¸ ë™ì‘
        self.composite_skills = VectorDatabase()  # ë³µí•© ë™ì‘
        self.meta_skills = VectorDatabase()      # ì¶”ìƒ ì „ëµ
    
    def retrieve_skill_chain(self, task_description):
        # Top-down retrieval
        meta = self.meta_skills.search(task_description, k=1)[0]
        composites = []
        for comp_id in meta['composite_ids']:
            comp = self.composite_skills.get(comp_id)
            composites.append(comp)
        
        primitives = []
        for comp in composites:
            for prim_id in comp['primitive_ids']:
                prim = self.primitive_skills.get(prim_id)
                primitives.append(prim)
        
        return {'meta': meta, 'composites': composites, 'primitives': primitives}
```

#### ìŠ¤í‚¬ ì ì‘
```python
def adapt_skill(retrieved_skill, current_context):
    """ê²€ìƒ‰ëœ ìŠ¤í‚¬ì„ í˜„ì¬ ìƒí™©ì— ë§ê²Œ ì¡°ì •"""
    # Parameter adaptation
    adapted_params = skill_adapter(
        skill_params=retrieved_skill['parameters'],
        context=current_context
    )
    
    # Precondition checking
    if not check_preconditions(retrieved_skill['preconditions'], current_context):
        # Find alternative or modify
        adapted_params = modify_for_preconditions(adapted_params, current_context)
    
    return adapted_params
```

### 3. Multi-Modal Retrieval

#### Cross-Modal Alignment
```python
class CrossModalRetriever:
    def __init__(self):
        self.vision_proj = nn.Linear(2048, 512)
        self.language_proj = nn.Linear(768, 512)
        self.action_proj = nn.Linear(7, 512)
    
    def align_modalities(self, vision, language, action):
        # Project to common space
        v_emb = self.vision_proj(vision)
        l_emb = self.language_proj(language)
        a_emb = self.action_proj(action)
        
        # Normalize for cosine similarity
        v_emb = F.normalize(v_emb, dim=-1)
        l_emb = F.normalize(l_emb, dim=-1)
        a_emb = F.normalize(a_emb, dim=-1)
        
        return v_emb, l_emb, a_emb
```

#### Attention-based Retrieval
```python
class AttentiveRetrieval:
    def retrieve_with_attention(self, query, database, attention_weights):
        """ì£¼ì˜ ê¸°ë°˜ ê²€ìƒ‰"""
        # Weight different aspects
        weighted_query = query * attention_weights
        
        # Multi-head retrieval
        results = []
        for head in range(self.num_heads):
            head_query = self.head_projection[head](weighted_query)
            head_results = database.search(head_query, k=3)
            results.extend(head_results)
        
        # Aggregate and deduplicate
        return self.aggregate_results(results)
```

## ğŸ”¬ ê³ ê¸‰ ê¸°ë²•

### 1. Learned Index

#### Neural Index
```python
class LearnedIndex:
    """í•™ìŠµëœ ì¸ë±ìŠ¤ êµ¬ì¡°"""
    def __init__(self):
        self.model = nn.Sequential(
            nn.Linear(768, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)  # Position prediction
        )
    
    def train(self, vectors, positions):
        # Learn to predict storage position
        for v, pos in zip(vectors, positions):
            pred_pos = self.model(v)
            loss = F.mse_loss(pred_pos, pos)
            loss.backward()
    
    def search(self, query):
        # Predict position range
        pred_pos = self.model(query)
        search_range = [pred_pos - Îµ, pred_pos + Îµ]
        return linear_search_in_range(search_range)
```

### 2. Continual Learning in Retrieval

#### Dynamic Index Update
```python
class DynamicRetriever:
    def __init__(self):
        self.index = None
        self.buffer = []
        self.rebuild_threshold = 1000
    
    def add_incremental(self, vectors, metadata):
        self.buffer.extend(zip(vectors, metadata))
        
        if len(self.buffer) > self.rebuild_threshold:
            self.rebuild_index()
    
    def rebuild_index(self):
        """íš¨ìœ¨ì  ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
        # Merge buffer with existing
        all_vectors = self.get_all_vectors()
        
        # Clustering for IVF
        kmeans = faiss.Kmeans(d, ncentroids)
        kmeans.train(all_vectors)
        
        # Build new index
        self.index = faiss.IndexIVFFlat(kmeans.index, d, ncentroids)
        self.index.add(all_vectors)
        
        self.buffer.clear()
```

### 3. Privacy-Preserving Retrieval

#### Differential Privacy
```python
def dp_retrieval(query, database, epsilon=1.0):
    """ì°¨ë“± í”„ë¼ì´ë²„ì‹œ ê²€ìƒ‰"""
    # Add noise to query
    noise = np.random.laplace(0, 1/epsilon, query.shape)
    noisy_query = query + noise
    
    # Retrieve with noisy query
    results = database.search(noisy_query, k=10)
    
    # Add noise to scores
    for r in results:
        r['score'] += np.random.laplace(0, 1/epsilon)
    
    return results
```

## ğŸ’¡ ì‹¤ì „ ìµœì í™” ê°€ì´ë“œ

### 1. ì¸ë±ìŠ¤ ì„ íƒ ê°€ì´ë“œ

| ë°ì´í„° í¬ê¸° | ì •í™•ë„ ìš”êµ¬ | ì¶”ì²œ ì¸ë±ìŠ¤ | ì´ìœ  |
|------------|------------|------------|------|
| < 1K | ë†’ìŒ | Flat | ì •í™•, ë¹ ë¦„ |
| 1K-10K | ì¤‘ê°„ | IVF | ê· í˜• |
| 10K-100K | ì¤‘ê°„ | HNSW | í™•ì¥ì„± |
| > 100K | ë‚®ìŒ | LSH | ë©”ëª¨ë¦¬ íš¨ìœ¨ |

### 2. ì„±ëŠ¥ íŠœë‹

#### Batch Processing
```python
def batch_search(queries, index, batch_size=100):
    results = []
    for i in range(0, len(queries), batch_size):
        batch = queries[i:i+batch_size]
        batch_results = index.search(batch, k=10)
        results.extend(batch_results)
    return results
```

#### Caching Strategy
```python
class LRUCache:
    def __init__(self, capacity=1000):
        self.cache = OrderedDict()
        self.capacity = capacity
    
    def get(self, key):
        if key in self.cache:
            self.cache.move_to_end(key)
            return self.cache[key]
        return None
    
    def put(self, key, value):
        self.cache[key] = value
        self.cache.move_to_end(key)
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)
```

### 3. í’ˆì§ˆ í‰ê°€

#### ê²€ìƒ‰ í’ˆì§ˆ ë©”íŠ¸ë¦­
```python
def evaluate_retrieval_quality(ground_truth, predictions):
    metrics = {}
    
    # Precision@K
    metrics['p@5'] = precision_at_k(ground_truth, predictions, k=5)
    
    # Mean Reciprocal Rank
    metrics['mrr'] = mean_reciprocal_rank(ground_truth, predictions)
    
    # Normalized Discounted Cumulative Gain
    metrics['ndcg'] = ndcg(ground_truth, predictions)
    
    # Semantic Similarity
    metrics['semantic_sim'] = average_semantic_similarity(ground_truth, predictions)
    
    return metrics
```

## ğŸš€ ìµœì‹  ì—°êµ¬ ë™í–¥

### 1. Neural Architecture
- **RETRO**: Retrieval-Enhanced Transformer
- **REALM**: Retrieval-Augmented Language Model
- **RAG**: Retrieval-Augmented Generation

### 2. Efficient Retrieval
- **SCANN**: Scalable Nearest Neighbors
- **DiskANN**: Billion-scale disk-based index
- **Pinecone**: Managed vector database

### 3. Multimodal Retrieval
- **CLIP**: Contrastive Language-Image Pre-training
- **ALIGN**: Large-scale noisy image-text pairs
- **Flamingo**: Few-shot multimodal learning

## âš ï¸ ì£¼ì˜ì‚¬í•­ ë° í•œê³„

### ì£¼ìš” ë¬¸ì œì 
1. **Semantic Gap**: ì„ë² ë”©ì´ ì˜ë¯¸ë¥¼ ì™„ë²½íˆ í¬ì°© ëª»í•¨
2. **Distribution Shift**: í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í¬ ì°¨ì´
3. **Storage Overhead**: ëŒ€ê·œëª¨ ì¸ë±ìŠ¤ ì €ì¥ ë¹„ìš©
4. **Update Latency**: ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì–´ë ¤ì›€

### í•´ê²° ë°©ì•ˆ
1. **Hybrid Methods**: Dense + Sparse ê²°í•©
2. **Continual Learning**: ì ì§„ì  ì—…ë°ì´íŠ¸
3. **Compression**: ë²¡í„° ì–‘ìí™”
4. **Distributed Systems**: ë¶„ì‚° ì €ì¥/ê²€ìƒ‰

## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ

### í•µì‹¬ ë…¼ë¬¸
- "Dense Passage Retrieval for Open-Domain Question Answering"
- "ColBERT: Efficient and Effective Passage Search"
- "Approximate Nearest Neighbor Search in High Dimensions"

### ë„êµ¬ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
- FAISS: ë²¡í„° ê²€ìƒ‰
- Elasticsearch: í…ìŠ¤íŠ¸ ê²€ìƒ‰
- Weaviate: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
- Milvus: í™•ì¥ ê°€ëŠ¥í•œ ë²¡í„° DB

### ë²¤ì¹˜ë§ˆí¬
- MS MARCO: Passage retrieval
- BEIR: Zero-shot retrieval
- TREC: Text REtrieval Conference

## ğŸ¯ í•µì‹¬ ìš”ì•½

Retrieval Systemì€ VLAê°€ ë°©ëŒ€í•œ ê²½í—˜ê³¼ ì§€ì‹ì„ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” í•µì‹¬ ì¸í”„ë¼ì…ë‹ˆë‹¤. Dense Retrievalì˜ ì˜ë¯¸ì  ì´í•´ë ¥ê³¼ Sparse Retrievalì˜ ì •í™•ì„±ì„ ê²°í•©í•œ Hybrid ì ‘ê·¼, íš¨ìœ¨ì ì¸ Vector Database êµ¬ì¶•, RAG íŒ¨í„´ì˜ í™œìš©ì´ ì„±ê³µì ì¸ ì‹œìŠ¤í…œ êµ¬í˜„ì˜ ì—´ì‡ ì…ë‹ˆë‹¤. ì‹¤ì‹œê°„ ì„±ëŠ¥, í™•ì¥ì„±, ê²€ìƒ‰ í’ˆì§ˆì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.