# ğŸ§  Neural Networks Basics

**ëª©í‘œ**: ê°œë°œì ê´€ì ì—ì„œ ì‹ ê²½ë§ì˜ ê¸°ë³¸ ê°œë…ê³¼ VLAì—ì„œì˜ í™œìš©ì„ ì´í•´

**ì‹œê°„**: 1-2ì‹œê°„

**ì „ì œì¡°ê±´**: Python, ê¸°ë³¸ì ì¸ ì„ í˜•ëŒ€ìˆ˜ (ë²¡í„°, í–‰ë ¬)

---

## ğŸ¯ ê°œë°œìë¥¼ ìœ„í•œ ì§ê´€ì  ì´í•´

### ì‹ ê²½ë§ = ë³µì¡í•œ í•¨ìˆ˜ ê·¼ì‚¬ê¸°
```python
# ì „í†µì ì¸ í”„ë¡œê·¸ë˜ë°
def traditional_function(x):
    if x > 5:
        return x * 2
    else:
        return x + 1

# ì‹ ê²½ë§ = ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ëŠ” í•¨ìˆ˜
def neural_network(x, weights, biases):
    # weightsì™€ biasesëŠ” ë°ì´í„°ë¡œë¶€í„° í•™ìŠµë¨
    return some_complex_computation(x, weights, biases)
```

### í•µì‹¬ ì•„ì´ë””ì–´
```python
neural_network_concept = {
    "ì…ë ¥": "ìˆ«ìë“¤ì˜ ë²¡í„° (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ì„¼ì„œ ë°ì´í„° ë“±)",
    "ì²˜ë¦¬": "ê°€ì¤‘í•© + ë¹„ì„ í˜• ë³€í™˜ì„ ì—¬ëŸ¬ ì¸µì—ì„œ ë°˜ë³µ",
    "ì¶œë ¥": "ì›í•˜ëŠ” í˜•íƒœì˜ ì˜ˆì¸¡ê°’ (ë¶„ë¥˜, íšŒê·€, ì•¡ì…˜ ë“±)",
    "í•™ìŠµ": "ì •ë‹µê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ë¥¼ ì¤„ì´ë„ë¡ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"
}
```

---

## ğŸ—ï¸ ê¸°ë³¸ êµ¬ì¡°: í¼ì…‰íŠ¸ë¡ ë¶€í„° ì‹œì‘

### 1. ë‹¨ìˆœ í¼ì…‰íŠ¸ë¡  (Linear Layer)
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimplePerceptron(nn.Module):
    def __init__(self, input_size, output_size):
        super().__init__()
        # y = W*x + b (ê°€ì¤‘í•©)
        self.linear = nn.Linear(input_size, output_size)
    
    def forward(self, x):
        return self.linear(x)

# ì˜ˆì‹œ: 3ì°¨ì› ì…ë ¥ â†’ 1ì°¨ì› ì¶œë ¥
perceptron = SimplePerceptron(input_size=3, output_size=1)

# ì…ë ¥ ë°ì´í„°
x = torch.tensor([1.0, 2.0, 3.0])  # 3ì°¨ì› ë²¡í„°
output = perceptron(x)  # 1ì°¨ì› ì¶œë ¥

print(f"ì…ë ¥: {x}")
print(f"ì¶œë ¥: {output}")
print(f"ê°€ì¤‘ì¹˜: {perceptron.linear.weight}")
print(f"í¸í–¥: {perceptron.linear.bias}")
```

### 2. ë¹„ì„ í˜•ì„± ì¶”ê°€ (Activation Functions)
```python
class NonLinearPerceptron(nn.Module):
    def __init__(self, input_size, output_size):
        super().__init__()
        self.linear = nn.Linear(input_size, output_size)
    
    def forward(self, x):
        # ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ ì ìš©
        return torch.relu(self.linear(x))  # ReLU: max(0, x)

# ì—¬ëŸ¬ í™œì„±í™” í•¨ìˆ˜ ë¹„êµ
x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])

activations = {
    "ReLU": F.relu(x),           # max(0, x)
    "Sigmoid": F.sigmoid(x),     # 1 / (1 + e^(-x))
    "Tanh": F.tanh(x),          # (e^x - e^(-x)) / (e^x + e^(-x))
    "GELU": F.gelu(x)           # x * Î¦(x) (Gaussian Error Linear Unit)
}

for name, result in activations.items():
    print(f"{name:8}: {result}")
```

### 3. ë‹¤ì¸µ ì‹ ê²½ë§ (Multi-Layer Perceptron)
```python
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        # ì—¬ëŸ¬ ì¸µì„ ìŒ“ì•„ì„œ ë³µì¡í•œ í•¨ìˆ˜ í‘œí˜„
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.layer2 = nn.Linear(hidden_size, hidden_size)
        self.layer3 = nn.Linear(hidden_size, output_size)
        
        # ì •ê·œí™” (ì„ íƒì )
        self.batch_norm1 = nn.BatchNorm1d(hidden_size)
        self.batch_norm2 = nn.BatchNorm1d(hidden_size)
        
        # ë“œë¡­ì•„ì›ƒ (ê³¼ì í•© ë°©ì§€)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x):
        # ì²« ë²ˆì§¸ ì¸µ: ì…ë ¥ â†’ ì€ë‹‰ì¸µ
        x = self.layer1(x)
        x = self.batch_norm1(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # ë‘ ë²ˆì§¸ ì¸µ: ì€ë‹‰ì¸µ â†’ ì€ë‹‰ì¸µ
        x = self.layer2(x)
        x = self.batch_norm2(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # ì¶œë ¥ì¸µ: ì€ë‹‰ì¸µ â†’ ì¶œë ¥
        x = self.layer3(x)
        return x

# ì˜ˆì‹œ: MNIST ì†ê¸€ì”¨ ì¸ì‹
mlp = MLP(input_size=784,    # 28x28 ì´ë¯¸ì§€
          hidden_size=256,   # ì€ë‹‰ì¸µ í¬ê¸°
          output_size=10)    # 10ê°œ í´ë˜ìŠ¤ (0~9 ìˆ«ì)

# ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
batch_size = 32
dummy_image = torch.randn(batch_size, 784)  # 32ê°œì˜ 784ì°¨ì› ì´ë¯¸ì§€
predictions = mlp(dummy_image)  # 32ê°œì˜ 10ì°¨ì› ì˜ˆì¸¡ê°’

print(f"ì…ë ¥ í¬ê¸°: {dummy_image.shape}")
print(f"ì¶œë ¥ í¬ê¸°: {predictions.shape}")
```

---

## ğŸ“š í•™ìŠµ ê³¼ì •: Forward & Backward Propagation

### 1. Forward Pass (ìˆœì „íŒŒ)
```python
def forward_pass_example():
    """
    Forward pass: ì…ë ¥ â†’ ì˜ˆì¸¡ê°’ ê³„ì‚°
    """
    # ëª¨ë¸ê³¼ ë°ì´í„° ì¤€ë¹„
    model = MLP(input_size=4, hidden_size=8, output_size=2)
    x = torch.randn(10, 4)  # ë°°ì¹˜ í¬ê¸° 10, íŠ¹ì„± 4ê°œ
    y_true = torch.randint(0, 2, (10,))  # ì‹¤ì œ ë ˆì´ë¸” (0 ë˜ëŠ” 1)
    
    # Forward pass
    y_pred = model(x)  # ëª¨ë¸ ì˜ˆì¸¡
    
    # ì†ì‹¤ ê³„ì‚°
    loss = F.cross_entropy(y_pred, y_true)
    
    print(f"ì…ë ¥: {x.shape}")
    print(f"ì˜ˆì¸¡: {y_pred.shape}")
    print(f"ì‹¤ì œ: {y_true.shape}")
    print(f"ì†ì‹¤: {loss.item():.4f}")
    
    return loss, model

forward_pass_example()
```

### 2. Backward Pass (ì—­ì „íŒŒ)
```python
def backward_pass_example():
    """
    Backward pass: ì†ì‹¤ â†’ ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚° â†’ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
    """
    # ëª¨ë¸, ë°ì´í„°, ì˜µí‹°ë§ˆì´ì € ì¤€ë¹„
    model = MLP(input_size=4, hidden_size=8, output_size=2)
    x = torch.randn(10, 4)
    y_true = torch.randint(0, 2, (10,))
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    print("=== í•™ìŠµ ì „ ===")
    first_layer_weight = model.layer1.weight.clone()
    
    # í•™ìŠµ ë£¨í”„
    for epoch in range(5):
        # Forward pass
        y_pred = model(x)
        loss = F.cross_entropy(y_pred, y_true)
        
        # Backward pass
        optimizer.zero_grad()  # ê·¸ë¼ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        loss.backward()        # ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚°
        optimizer.step()       # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        
        print(f"Epoch {epoch+1}: Loss = {loss.item():.4f}")
    
    print("=== í•™ìŠµ í›„ ===")
    weight_change = (model.layer1.weight - first_layer_weight).abs().mean()
    print(f"ê°€ì¤‘ì¹˜ ë³€í™”ëŸ‰: {weight_change.item():.6f}")

backward_pass_example()
```

### 3. ê·¸ë¼ë””ì–¸íŠ¸ ê³„ì‚° ê³¼ì • ì´í•´
```python
def gradient_computation_example():
    """
    ê·¸ë¼ë””ì–¸íŠ¸ê°€ ì–´ë–»ê²Œ ê³„ì‚°ë˜ëŠ”ì§€ ë‹¨ê³„ë³„ë¡œ í™•ì¸
    """
    # ê°„ë‹¨í•œ ëª¨ë¸
    x = torch.tensor([2.0, 3.0], requires_grad=True)
    w = torch.tensor([0.5, 0.8], requires_grad=True)
    b = torch.tensor(0.1, requires_grad=True)
    
    # Forward: y = sum(w * x) + b
    y = torch.sum(w * x) + b
    print(f"y = {y.item():.2f}")
    
    # Backward: dy/dx, dy/dw, dy/db ê³„ì‚°
    y.backward()
    
    print(f"xì˜ ê·¸ë¼ë””ì–¸íŠ¸: {x.grad}")  # dy/dx = w
    print(f"wì˜ ê·¸ë¼ë””ì–¸íŠ¸: {w.grad}")  # dy/dw = x
    print(f"bì˜ ê·¸ë¼ë””ì–¸íŠ¸: {b.grad}")  # dy/db = 1

gradient_computation_example()
```

---

## ğŸ¤– VLAì—ì„œì˜ ì‹ ê²½ë§ í™œìš©

### 1. Vision Encoder (ì´ë¯¸ì§€ â†’ íŠ¹ì„± ë²¡í„°)
```python
class VisionEncoder(nn.Module):
    """
    ë¡œë´‡ ì¹´ë©”ë¼ ì´ë¯¸ì§€ë¥¼ íŠ¹ì„± ë²¡í„°ë¡œ ë³€í™˜
    """
    def __init__(self, image_size=224, feature_dim=512):
        super().__init__()
        # CNN ê¸°ë°˜ íŠ¹ì„± ì¶”ì¶œ
        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1)  # Global average pooling
        )
        
        self.fc = nn.Linear(256, feature_dim)
    
    def forward(self, images):
        # images: (batch, 3, 224, 224)
        features = self.conv_layers(images)  # (batch, 256, 1, 1)
        features = features.flatten(1)       # (batch, 256)
        features = self.fc(features)         # (batch, 512)
        return features

# ì˜ˆì‹œ ì‚¬ìš©
vision_encoder = VisionEncoder()
dummy_image = torch.randn(4, 3, 224, 224)  # 4ê°œ ì´ë¯¸ì§€
visual_features = vision_encoder(dummy_image)
print(f"ì´ë¯¸ì§€ â†’ íŠ¹ì„± ë²¡í„°: {dummy_image.shape} â†’ {visual_features.shape}")
```

### 2. Language Encoder (ëª…ë ¹ì–´ â†’ íŠ¹ì„± ë²¡í„°)
```python
class LanguageEncoder(nn.Module):
    """
    ìì—°ì–´ ëª…ë ¹ì„ íŠ¹ì„± ë²¡í„°ë¡œ ë³€í™˜
    """
    def __init__(self, vocab_size=10000, embed_dim=256, hidden_dim=512):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 512)
    
    def forward(self, tokens):
        # tokens: (batch, sequence_length)
        embedded = self.embedding(tokens)        # (batch, seq_len, embed_dim)
        _, (hidden, _) = self.lstm(embedded)     # hidden: (1, batch, hidden_dim)
        features = self.fc(hidden.squeeze(0))    # (batch, 512)
        return features

# ì˜ˆì‹œ ì‚¬ìš©
language_encoder = LanguageEncoder()
# "pick up the red cup" â†’ [45, 123, 67, 891, 234]
dummy_tokens = torch.randint(0, 10000, (4, 8))  # 4ê°œ ë¬¸ì¥, ê° 8 í† í°
language_features = language_encoder(dummy_tokens)
print(f"ëª…ë ¹ì–´ â†’ íŠ¹ì„± ë²¡í„°: {dummy_tokens.shape} â†’ {language_features.shape}")
```

### 3. Action Decoder (íŠ¹ì„± ë²¡í„° â†’ ë¡œë´‡ ì•¡ì…˜)
```python
class ActionDecoder(nn.Module):
    """
    í†µí•©ëœ íŠ¹ì„±ì„ ë¡œë´‡ ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜
    """
    def __init__(self, feature_dim=1024, action_dim=7):  # 7-DOF ë¡œë´‡ íŒ”
        super().__init__()
        self.policy_head = nn.Sequential(
            nn.Linear(feature_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )
        
        # ì•¡ì…˜ ë²”ìœ„ ì œí•œ (ì˜ˆ: -1 ~ 1)
        self.action_scale = nn.Parameter(torch.ones(action_dim))
    
    def forward(self, features):
        raw_actions = self.policy_head(features)
        # Tanhë¡œ -1 ~ 1 ë²”ìœ„ë¡œ ì œí•œ
        normalized_actions = torch.tanh(raw_actions)
        # ìŠ¤ì¼€ì¼ ì ìš©
        actions = normalized_actions * self.action_scale
        return actions

# í†µí•© VLA ëª¨ë¸ ì˜ˆì‹œ
class SimpleVLA(nn.Module):
    def __init__(self):
        super().__init__()
        self.vision_encoder = VisionEncoder()
        self.language_encoder = LanguageEncoder()
        self.action_decoder = ActionDecoder(feature_dim=1024)  # 512 + 512
    
    def forward(self, images, instructions):
        # ê° ëª¨ë‹¬ë¦¬í‹° ì¸ì½”ë”©
        visual_features = self.vision_encoder(images)      # (batch, 512)
        language_features = self.language_encoder(instructions)  # (batch, 512)
        
        # íŠ¹ì„± ê²°í•©
        combined_features = torch.cat([visual_features, language_features], dim=1)  # (batch, 1024)
        
        # ì•¡ì…˜ ì˜ˆì¸¡
        actions = self.action_decoder(combined_features)   # (batch, 7)
        
        return actions

# ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
vla_model = SimpleVLA()
dummy_images = torch.randn(2, 3, 224, 224)
dummy_instructions = torch.randint(0, 10000, (2, 8))

predicted_actions = vla_model(dummy_images, dummy_instructions)
print(f"VLA ì¶œë ¥: {predicted_actions.shape}")  # (2, 7)
print(f"ì˜ˆì¸¡ëœ ì•¡ì…˜: {predicted_actions}")
```

---

## ğŸ”¬ í•µì‹¬ ê°œë… ì •ë¦¬

### 1. ì™œ ì‹ ê²½ë§ì´ íš¨ê³¼ì ì¸ê°€?
```python
key_advantages = {
    "ë²”ìš©ì„±": "ì–´ë–¤ í•¨ìˆ˜ë“  ê·¼ì‚¬ ê°€ëŠ¥ (Universal Approximation Theorem)",
    "í•™ìŠµëŠ¥ë ¥": "ë°ì´í„°ë¡œë¶€í„° ìë™ìœ¼ë¡œ íŒ¨í„´ ë°œê²¬",
    "í™•ì¥ì„±": "ë” ë§ì€ ë°ì´í„°ì™€ ê³„ì‚°ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ",
    "ë¯¸ë¶„ê°€ëŠ¥ì„±": "ê·¸ë¼ë””ì–¸íŠ¸ ê¸°ë°˜ ìµœì í™” ê°€ëŠ¥"
}
```

### 2. VLAì—ì„œ ì‹ ê²½ë§ì˜ ì—­í• 
```python
vla_neural_network_roles = {
    "íŠ¹ì„±ì¶”ì¶œ": "ì›ì‹œ ë°ì´í„°(ì´ë¯¸ì§€, í…ìŠ¤íŠ¸)ì—ì„œ ì˜ë¯¸ìˆëŠ” ì •ë³´ ì¶”ì¶œ",
    "ëª¨ë‹¬ë¦¬í‹°ìœµí•©": "ì„œë¡œ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì •ë³´(ë¹„ì „+ì–¸ì–´) ê²°í•©",
    "ì •ì±…í•™ìŠµ": "ìƒí™©ì— ë§ëŠ” ìµœì ì˜ í–‰ë™ ë°©ë²• í•™ìŠµ",
    "í‘œí˜„í•™ìŠµ": "ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ íš¨ê³¼ì  ì••ì¶•"
}
```

### 3. ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°
```python
important_hyperparameters = {
    "í•™ìŠµë¥ ": {
        "ê°’": "0.001 ~ 0.1",
        "ì—­í• ": "ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ë³´í­",
        "íŒ": "Adam ì˜µí‹°ë§ˆì´ì €ì—ì„œëŠ” 0.001ì´ ì¢‹ì€ ì‹œì‘ì "
    },
    
    "ë°°ì¹˜í¬ê¸°": {
        "ê°’": "16 ~ 128",
        "ì—­í• ": "í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ìƒ˜í”Œ ìˆ˜",
        "íŒ": "GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ì¡°ì •"
    },
    
    "ì€ë‹‰ì¸µí¬ê¸°": {
        "ê°’": "64 ~ 1024",
        "ì—­í• ": "ëª¨ë¸ í‘œí˜„ë ¥ ê²°ì •",
        "íŒ": "ë„ˆë¬´ í¬ë©´ ê³¼ì í•©, ë„ˆë¬´ ì‘ìœ¼ë©´ ì„±ëŠ¥ ë¶€ì¡±"
    },
    
    "ë“œë¡­ì•„ì›ƒ": {
        "ê°’": "0.1 ~ 0.5",
        "ì—­í• ": "ê³¼ì í•© ë°©ì§€",
        "íŒ": "í•™ìŠµ ì¤‘ì—ë§Œ ì ìš©, ì¶”ë¡  ì‹œì—ëŠ” ë¹„í™œì„±í™”"
    }
}
```

---

## ğŸ› ï¸ ì‹¤ìŠµ: ê°„ë‹¨í•œ VLA ëª¨ë¸ í›ˆë ¨

### ì™„ì „í•œ í›ˆë ¨ ì½”ë“œ
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt

# ë”ë¯¸ VLA ë°ì´í„°ì…‹
class DummyVLADataset(Dataset):
    def __init__(self, num_samples=1000):
        self.num_samples = num_samples
        
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        # ë”ë¯¸ ì´ë¯¸ì§€ (3, 64, 64)
        image = torch.randn(3, 64, 64)
        
        # ë”ë¯¸ ëª…ë ¹ì–´ í† í° (ê¸¸ì´ 16)
        instruction = torch.randint(0, 1000, (16,))
        
        # ë”ë¯¸ ì•¡ì…˜ (7-DOF)
        action = torch.randn(7)
        
        return image, instruction, action

# í›ˆë ¨ í•¨ìˆ˜
def train_vla_model():
    # ë°ì´í„° ì¤€ë¹„
    dataset = DummyVLADataset()
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    # ëª¨ë¸ (ê°„ë‹¨í™”ëœ ë²„ì „)
    class SimpleVLAModel(nn.Module):
        def __init__(self):
            super().__init__()
            # ê°„ë‹¨í•œ CNN for vision
            self.vision = nn.Sequential(
                nn.Conv2d(3, 32, 3),
                nn.ReLU(),
                nn.AdaptiveAvgPool2d(8),
                nn.Flatten(),
                nn.Linear(32*8*8, 256)
            )
            
            # ê°„ë‹¨í•œ embedding for language
            self.language = nn.Sequential(
                nn.Embedding(1000, 64),
                nn.LSTM(64, 128, batch_first=True),
            )
            
            # Action decoder
            self.action_head = nn.Sequential(
                nn.Linear(256 + 128, 128),
                nn.ReLU(),
                nn.Linear(128, 7)
            )
            
        def forward(self, images, instructions):
            # Vision features
            vis_feat = self.vision(images)
            
            # Language features
            lang_embed = self.language[0](instructions)
            _, (lang_feat, _) = self.language[1](lang_embed)
            lang_feat = lang_feat.squeeze(0)
            
            # Combine and predict
            combined = torch.cat([vis_feat, lang_feat], dim=1)
            actions = self.action_head(combined)
            return actions
    
    model = SimpleVLAModel()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()
    
    # í›ˆë ¨ ë£¨í”„
    losses = []
    for epoch in range(10):
        epoch_loss = 0
        for batch_idx, (images, instructions, target_actions) in enumerate(dataloader):
            # Forward pass
            predicted_actions = model(images, instructions)
            loss = criterion(predicted_actions, target_actions)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            
        avg_loss = epoch_loss / len(dataloader)
        losses.append(avg_loss)
        print(f"Epoch {epoch+1}: Loss = {avg_loss:.4f}")
    
    return model, losses

# ì‹¤í–‰
if __name__ == "__main__":
    trained_model, training_losses = train_vla_model()
    print("í›ˆë ¨ ì™„ë£Œ!")
    print(f"ìµœì¢… ì†ì‹¤: {training_losses[-1]:.4f}")
```

---

## ğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„

ì‹ ê²½ë§ ê¸°ì´ˆë¥¼ ì´í•´í–ˆë‹¤ë©´:

1. **Attention Mechanism** (`02_attention_mechanism.md`) - VLAì˜ í•µì‹¬
2. **Transformer Architecture** (`03_transformer_architecture.md`) - í˜„ëŒ€ VLAì˜ ê¸°ë°˜
3. **Multi-Modal Learning** (`04_multimodal_learning.md`) - Vision + Language ê²°í•©

### ì¶”ì²œ ì‹¤ìŠµ
```python
recommended_practice = {
    "ê¸°ì´ˆ": "PyTorch íŠœí† ë¦¬ì–¼ì˜ ì‹ ê²½ë§ ì„¹ì…˜ ì™„ì£¼",
    "ì¤‘ê¸‰": "CIFAR-10 ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ ì§ì ‘ êµ¬í˜„",
    "ê³ ê¸‰": "ê°„ë‹¨í•œ ë¡œë´‡ ì œì–´ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë°©í•™ìŠµ ì‹œë„"
}
```

---

## ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸

### ê¸°ì–µí•´ì•¼ í•  ê²ƒ
1. **ì‹ ê²½ë§ = í•™ìŠµ ê°€ëŠ¥í•œ í•¨ìˆ˜**: ë°ì´í„°ë¡œë¶€í„° ì…ì¶œë ¥ ê´€ê³„ í•™ìŠµ
2. **Forward + Backward**: ì˜ˆì¸¡ â†’ ì†ì‹¤ ê³„ì‚° â†’ ê·¸ë¼ë””ì–¸íŠ¸ â†’ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
3. **ë¹„ì„ í˜•ì„± í•„ìˆ˜**: ReLU ê°™ì€ í™œì„±í™” í•¨ìˆ˜ê°€ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ í•¨
4. **VLA = Vision + Language â†’ Action**: ê° ë‹¨ê³„ì—ì„œ ì‹ ê²½ë§ í™œìš©

### VLA ì—°êµ¬ì—ì„œì˜ ì˜ë¯¸
- **ê¸°ë³¸ ë¹Œë”© ë¸”ë¡**: ëª¨ë“  VLA ëª¨ë¸ì˜ ê¸°ì´ˆ
- **í™•ì¥ ê°€ëŠ¥**: ë” ë³µì¡í•œ ì•„í‚¤í…ì²˜(Transformer ë“±)ì˜ êµ¬ì„± ìš”ì†Œ
- **í•´ì„ ê°€ëŠ¥**: ê° ì¸µì—ì„œ ë¬´ì—‡ì„ í•™ìŠµí•˜ëŠ”ì§€ ë¶„ì„ ê°€ëŠ¥

**ë‹¤ìŒ: Attention ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ!** ğŸš€

---

*Created: 2025-08-24*  
*Time: 1-2 hours*  
*Next: 02_attention_mechanism.md*