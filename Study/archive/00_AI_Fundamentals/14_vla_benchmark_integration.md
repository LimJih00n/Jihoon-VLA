# ğŸ”Œ VLA ëª¨ë¸ì„ ë²¤ì¹˜ë§ˆí¬ì— í†µí•©í•˜ëŠ” ì™„ì „ ê°€ì´ë“œ

## ğŸ“Œ ê°œìš”
VLA ëª¨ë¸ì„ ë‹¤ì–‘í•œ ë¡œë´‡ ë²¤ì¹˜ë§ˆí¬ì— í†µí•©í•˜ëŠ” ì‹¤ì „ ê°€ì´ë“œì…ë‹ˆë‹¤. ê° ë²¤ì¹˜ë§ˆí¬ì˜ ì¸í„°í˜ì´ìŠ¤ì— ë§ì¶° VLA ëª¨ë¸ì„ ì ìš©í•˜ëŠ” ë°©ë²•ì„ ìƒì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤.

---

## ğŸ—ï¸ 1. VLA ëª¨ë¸ ê¸°ë³¸ êµ¬ì¡°

### í‘œì¤€ VLA ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤

```python
import torch
import torch.nn as nn
from typing import Dict, Tuple, Optional
import numpy as np

class BaseVLAModel(nn.Module):
    """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‚¬ìš©í•  ê¸°ë³¸ VLA ëª¨ë¸"""
    
    def __init__(self, 
                 vision_encoder,
                 language_encoder,
                 action_decoder,
                 action_dim=7,
                 proprio_dim=7):
        super().__init__()
        
        # Core components
        self.vision_encoder = vision_encoder
        self.language_encoder = language_encoder
        self.action_decoder = action_decoder
        
        # Dimensions
        self.action_dim = action_dim
        self.proprio_dim = proprio_dim
        
        # Fusion layer
        self.fusion = nn.Sequential(
            nn.Linear(768 * 2 + proprio_dim, 1024),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, 512)
        )
        
        # Action head
        self.action_head = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )
    
    def forward(self, 
                images: torch.Tensor,
                language: torch.Tensor,
                proprioception: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            images: [B, C, H, W] or [B, T, C, H, W] for video
            language: [B, L] tokenized instruction
            proprioception: [B, proprio_dim] robot state
        Returns:
            actions: [B, action_dim]
        """
        # Encode modalities
        visual_features = self.vision_encoder(images)
        language_features = self.language_encoder(language)
        
        # Combine features
        if proprioception is not None:
            combined = torch.cat([
                visual_features,
                language_features,
                proprioception
            ], dim=-1)
        else:
            combined = torch.cat([
                visual_features,
                language_features,
                torch.zeros(images.shape[0], self.proprio_dim).to(images.device)
            ], dim=-1)
        
        # Fuse and decode
        fused = self.fusion(combined)
        actions = self.action_head(fused)
        
        return actions
    
    def process_observation(self, obs: Dict) -> Tuple[torch.Tensor, ...]:
        """ë²¤ì¹˜ë§ˆí¬ë³„ ê´€ì°°ì„ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ë³€í™˜"""
        raise NotImplementedError
    
    def process_action(self, action: torch.Tensor, action_space) -> np.ndarray:
        """ëª¨ë¸ ì¶œë ¥ì„ ë²¤ì¹˜ë§ˆí¬ ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜"""
        raise NotImplementedError
```

### ë©€í‹°ëª¨ë‹¬ ì¸ì½”ë” êµ¬í˜„

```python
class VisionEncoder(nn.Module):
    """ë¹„ì „ ì¸ì½”ë” - ResNet/ViT ê¸°ë°˜"""
    def __init__(self, model_name='resnet50', pretrained=True):
        super().__init__()
        
        if 'resnet' in model_name:
            import torchvision.models as models
            backbone = getattr(models, model_name)(pretrained=pretrained)
            self.encoder = nn.Sequential(*list(backbone.children())[:-1])
            self.output_dim = backbone.fc.in_features
        elif 'vit' in model_name:
            from transformers import ViTModel
            self.encoder = ViTModel.from_pretrained(f'google/{model_name}')
            self.output_dim = self.encoder.config.hidden_size
        
        # Projection to common dimension
        self.projection = nn.Linear(self.output_dim, 768)
    
    def forward(self, images):
        if len(images.shape) == 5:  # [B, T, C, H, W]
            B, T = images.shape[:2]
            images = images.view(B * T, *images.shape[2:])
            features = self.encoder(images)
            features = features.view(B, T, -1).mean(dim=1)  # Temporal pooling
        else:  # [B, C, H, W]
            features = self.encoder(images)
        
        return self.projection(features.squeeze())

class LanguageEncoder(nn.Module):
    """ì–¸ì–´ ì¸ì½”ë” - BERT/RoBERTa ê¸°ë°˜"""
    def __init__(self, model_name='bert-base-uncased'):
        super().__init__()
        from transformers import AutoModel
        
        self.encoder = AutoModel.from_pretrained(model_name)
        self.output_dim = self.encoder.config.hidden_size
        self.projection = nn.Linear(self.output_dim, 768)
    
    def forward(self, input_ids, attention_mask=None):
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        # Use [CLS] token representation
        pooled_output = outputs.last_hidden_state[:, 0]
        return self.projection(pooled_output)
```

---

## ğŸ® 2. RLBench í†µí•©

### RLBenchìš© VLA ë˜í¼

```python
from rlbench.environment import Environment
from rlbench.action_modes import ArmActionMode, ActionMode
import torch
import numpy as np

class RLBenchVLA:
    """RLBench í™˜ê²½ìš© VLA ëª¨ë¸ ë˜í¼"""
    
    def __init__(self, model: BaseVLAModel, device='cuda'):
        self.model = model.to(device)
        self.device = device
        
        # RLBench specific settings
        self.image_size = (128, 128)
        self.action_mode = 'end_effector'  # or 'joint_velocity'
        
        # Tokenizer for language
        from transformers import AutoTokenizer
        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
        
        # Action normalization parameters
        self.action_scale = np.array([0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 1.0])
        self.action_offset = np.array([0, 0, 0, 0, 0, 0, 0])
    
    def process_observation(self, obs):
        """RLBench ê´€ì°°ì„ VLA ì…ë ¥ìœ¼ë¡œ ë³€í™˜"""
        
        # Extract images from multiple cameras
        images = []
        for cam in ['left_shoulder_rgb', 'right_shoulder_rgb', 'wrist_rgb']:
            if hasattr(obs, cam):
                img = getattr(obs, cam)
                img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0
                images.append(img)
        
        # Stack images or use primary camera
        if len(images) > 1:
            # Multi-view: concatenate along channel dimension
            image_tensor = torch.cat(images, dim=0)
        else:
            image_tensor = images[0]
        
        # Normalize
        image_tensor = self.normalize_image(image_tensor)
        
        # Get proprioception
        proprio = torch.from_numpy(np.concatenate([
            obs.joint_positions,
            obs.joint_velocities,
            [obs.gripper_open]
        ])).float()
        
        return image_tensor.unsqueeze(0).to(self.device), proprio.unsqueeze(0).to(self.device)
    
    def process_language(self, instruction: str):
        """ì–¸ì–´ ëª…ë ¹ ì²˜ë¦¬"""
        tokens = self.tokenizer(
            instruction,
            return_tensors='pt',
            padding='max_length',
            truncation=True,
            max_length=64
        )
        return tokens['input_ids'].to(self.device)
    
    def predict(self, obs, instruction: str):
        """ì•¡ì…˜ ì˜ˆì¸¡"""
        self.model.eval()
        
        with torch.no_grad():
            # Process inputs
            images, proprio = self.process_observation(obs)
            language = self.process_language(instruction)
            
            # Forward pass
            action = self.model(images, language, proprio)
            
            # Convert to numpy and denormalize
            action = action.cpu().numpy()[0]
            action = action * self.action_scale + self.action_offset
        
        return self.format_action(action)
    
    def format_action(self, action):
        """ì•¡ì…˜ í¬ë§·íŒ…"""
        if self.action_mode == 'end_effector':
            # [x, y, z, qx, qy, qz, qw, gripper]
            return np.concatenate([
                action[:3],  # position
                action[3:7] / np.linalg.norm(action[3:7]),  # quaternion (normalized)
                [action[6]]  # gripper
            ])
        elif self.action_mode == 'joint_velocity':
            # Direct joint velocities
            return action
        else:
            raise ValueError(f"Unknown action mode: {self.action_mode}")
    
    def normalize_image(self, image):
        """ì´ë¯¸ì§€ ì •ê·œí™”"""
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        return (image - mean) / std
```

### RLBench í•™ìŠµ ë£¨í”„

```python
class RLBenchTrainer:
    """RLBenchì—ì„œ VLA ëª¨ë¸ í•™ìŠµ"""
    
    def __init__(self, model, env, task_class):
        self.model = model
        self.env = env
        self.task = env.get_task(task_class)
        
        self.optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=10000
        )
        
        # Replay buffer
        self.buffer = ReplayBuffer(capacity=10000)
        
    def collect_demonstration(self, num_demos=100):
        """ì „ë¬¸ê°€ ì‹œì—° ìˆ˜ì§‘"""
        demos = []
        
        for _ in range(num_demos):
            demo = self.task.get_demos(1, live_demos=False)[0]
            
            for obs in demo:
                # Store observation-action pairs
                self.buffer.add({
                    'obs': obs,
                    'action': obs.joint_velocities,  # or end_effector_pose
                    'instruction': self.task.get_task_descriptions()[0]
                })
        
        return demos
    
    def train_step(self, batch_size=32):
        """í•™ìŠµ ìŠ¤í…"""
        batch = self.buffer.sample(batch_size)
        
        # Prepare batch
        images = []
        proprios = []
        languages = []
        actions = []
        
        for sample in batch:
            img, proprio = self.model.process_observation(sample['obs'])
            lang = self.model.process_language(sample['instruction'])
            
            images.append(img)
            proprios.append(proprio)
            languages.append(lang)
            actions.append(torch.from_numpy(sample['action']).float())
        
        images = torch.cat(images)
        proprios = torch.cat(proprios)
        languages = torch.cat(languages)
        actions = torch.stack(actions).to(self.model.device)
        
        # Forward pass
        pred_actions = self.model(images, languages, proprios)
        
        # Compute loss
        loss = F.mse_loss(pred_actions, actions)
        
        # Backward pass
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()
        self.scheduler.step()
        
        return loss.item()
    
    def evaluate(self, num_episodes=10):
        """í‰ê°€"""
        successes = []
        
        for _ in range(num_episodes):
            descriptions, obs = self.task.reset()
            instruction = descriptions[0]
            
            done = False
            while not done:
                action = self.model.predict(obs, instruction)
                obs, reward, done = self.task.step(action)
            
            successes.append(reward > 0)
        
        return np.mean(successes)
```

---

## ğŸ”§ 3. Meta-World í†µí•©

### Meta-Worldìš© VLA ì–´ëŒ‘í„°

```python
import metaworld
import numpy as np

class MetaWorldVLA:
    """Meta-World í™˜ê²½ìš© VLA ì–´ëŒ‘í„°"""
    
    def __init__(self, model: BaseVLAModel, camera_name='corner'):
        self.model = model
        self.camera_name = camera_name
        
        # Meta-World doesn't have built-in language, so we create task descriptions
        self.task_descriptions = {
            'reach-v2': 'Reach the target position',
            'push-v2': 'Push the block to the goal',
            'pick-place-v2': 'Pick up the block and place it at the target',
            'door-open-v2': 'Open the door',
            'drawer-open-v2': 'Open the drawer',
            'drawer-close-v2': 'Close the drawer',
            'button-press-v2': 'Press the button',
            'peg-insert-v2': 'Insert the peg into the hole',
            'window-open-v2': 'Open the window',
            'window-close-v2': 'Close the window'
        }
        
        # Action space: [x, y, z, gripper]
        self.action_dim = 4
        
    def get_image_obs(self, env):
        """í™˜ê²½ì—ì„œ ì´ë¯¸ì§€ ê´€ì°° íšë“"""
        # Render image from environment
        img = env.render(offscreen=True, camera_name=self.camera_name)
        
        # Convert to tensor
        img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0
        
        return img_tensor
    
    def get_proprio_obs(self, obs):
        """Proprioceptive ê´€ì°° ì¶”ì¶œ"""
        # Meta-World observation structure:
        # obs[:3] - end effector position
        # obs[3:6] - object position
        # obs[6:9] - object orientation (if applicable)
        # obs[9:] - additional task-specific features
        
        proprio = torch.from_numpy(obs[:9]).float()
        return proprio
    
    def predict(self, env, obs, task_name):
        """ì•¡ì…˜ ì˜ˆì¸¡"""
        self.model.eval()
        
        with torch.no_grad():
            # Get observations
            img = self.get_image_obs(env).unsqueeze(0).to(self.model.device)
            proprio = self.get_proprio_obs(obs).unsqueeze(0).to(self.model.device)
            
            # Get language instruction
            instruction = self.task_descriptions.get(task_name, 'Complete the task')
            lang_tokens = self.model.tokenizer(
                instruction,
                return_tensors='pt'
            )['input_ids'].to(self.model.device)
            
            # Predict action
            action = self.model(img, lang_tokens, proprio)
            action = action.cpu().numpy()[0]
        
        # Meta-World expects [x, y, z, gripper] where gripper is binary
        action[3] = 1 if action[3] > 0 else -1  # Binarize gripper
        
        return action
```

### Meta-World ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ

```python
class MetaWorldMultiTaskTrainer:
    """Meta-World ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ"""
    
    def __init__(self, model, benchmark='ML10'):
        self.model = model
        
        # Initialize benchmark
        if benchmark == 'ML10':
            self.ml = metaworld.ML10()
        elif benchmark == 'ML45':
            self.ml = metaworld.ML45()
        else:
            raise ValueError(f"Unknown benchmark: {benchmark}")
        
        # Create environments for each task
        self.train_envs = self._create_envs('train')
        self.test_envs = self._create_envs('test')
        
        # Training setup
        self.optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)
        self.buffer = {task: ReplayBuffer(5000) for task in self.train_envs}
    
    def _create_envs(self, split='train'):
        """í™˜ê²½ ìƒì„±"""
        envs = {}
        
        classes = self.ml.train_classes if split == 'train' else self.ml.test_classes
        tasks = self.ml.train_tasks if split == 'train' else self.ml.test_tasks
        
        for name, env_cls in classes.items():
            env = env_cls()
            task = [t for t in tasks if t.env_name == name][0]
            env.set_task(task)
            envs[name] = env
        
        return envs
    
    def collect_data(self, num_episodes_per_task=10):
        """ë°ì´í„° ìˆ˜ì§‘"""
        for task_name, env in self.train_envs.items():
            for _ in range(num_episodes_per_task):
                obs = env.reset()
                done = False
                
                while not done:
                    # Random action for initial data collection
                    action = env.action_space.sample()
                    next_obs, reward, done, info = env.step(action)
                    
                    # Store transition
                    self.buffer[task_name].add({
                        'obs': obs,
                        'action': action,
                        'reward': reward,
                        'next_obs': next_obs,
                        'done': done,
                        'task': task_name
                    })
                    
                    obs = next_obs
    
    def train_epoch(self, batch_size=64):
        """í•™ìŠµ ì—í­"""
        total_loss = 0
        num_updates = 0
        
        # Sample from each task buffer
        for task_name, buffer in self.buffer.items():
            if len(buffer) < batch_size:
                continue
            
            batch = buffer.sample(batch_size)
            loss = self.train_step(batch, task_name)
            total_loss += loss
            num_updates += 1
        
        return total_loss / max(num_updates, 1)
    
    def train_step(self, batch, task_name):
        """ë‹¨ì¼ í•™ìŠµ ìŠ¤í…"""
        # Prepare batch
        images = []
        proprios = []
        actions = []
        
        env = self.train_envs[task_name]
        
        for transition in batch:
            # Get image observation
            img = self.model.get_image_obs(env)
            proprio = self.model.get_proprio_obs(transition['obs'])
            
            images.append(img)
            proprios.append(proprio)
            actions.append(torch.from_numpy(transition['action']).float())
        
        images = torch.stack(images).to(self.model.device)
        proprios = torch.stack(proprios).to(self.model.device)
        actions = torch.stack(actions).to(self.model.device)
        
        # Language tokens for task
        instruction = self.model.task_descriptions[task_name]
        lang_tokens = self.model.tokenizer(
            [instruction] * len(batch),
            return_tensors='pt',
            padding=True
        )['input_ids'].to(self.model.device)
        
        # Forward pass
        pred_actions = self.model(images, lang_tokens, proprios)
        
        # Loss
        loss = F.mse_loss(pred_actions, actions)
        
        # Backward pass
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def evaluate(self, split='test', num_episodes=10):
        """í‰ê°€"""
        envs = self.test_envs if split == 'test' else self.train_envs
        results = {}
        
        for task_name, env in envs.items():
            successes = []
            
            for _ in range(num_episodes):
                obs = env.reset()
                done = False
                
                while not done:
                    action = self.model.predict(env, obs, task_name)
                    obs, reward, done, info = env.step(action)
                
                successes.append(info.get('success', reward > 0))
            
            results[task_name] = np.mean(successes)
        
        return results
```

---

## ğŸ—£ï¸ 4. CALVIN í†µí•©

### CALVINìš© VLA êµ¬í˜„

```python
from calvin_env.envs.play_table_env import PlayTableSimEnv
import hydra
from omegaconf import DictConfig

class CALVINVLA:
    """CALVIN í™˜ê²½ìš© VLA ëª¨ë¸"""
    
    def __init__(self, model: BaseVLAModel, cfg_path):
        self.model = model
        
        # Load CALVIN configuration
        with hydra.initialize(config_path=cfg_path):
            self.cfg = hydra.compose(config_name="config")
        
        # Initialize environment
        self.env = PlayTableSimEnv(**self.cfg.env)
        
        # Language embedder for CALVIN instructions
        from sentence_transformers import SentenceTransformer
        self.sentence_encoder = SentenceTransformer('all-MiniLM-L6-v2')
        
    def process_calvin_obs(self, obs):
        """CALVIN ê´€ì°° ì²˜ë¦¬"""
        # CALVIN provides multiple camera views
        rgb_static = obs['rgb_obs']['rgb_static']
        rgb_gripper = obs['rgb_obs']['rgb_gripper']
        
        # Robot proprioception
        robot_obs = obs['robot_obs']
        
        # Convert to tensors
        static_tensor = torch.from_numpy(rgb_static).permute(2, 0, 1).float() / 255.0
        gripper_tensor = torch.from_numpy(rgb_gripper).permute(2, 0, 1).float() / 255.0
        
        # Concatenate images (or process separately)
        images = torch.cat([static_tensor, gripper_tensor], dim=0)
        
        # Proprioception
        proprio = torch.from_numpy(robot_obs).float()
        
        return images, proprio
    
    def process_instruction_chain(self, instructions):
        """ì—°ì† ëª…ë ¹ ì²˜ë¦¬"""
        # CALVIN uses sentence embeddings
        embeddings = []
        
        for instruction in instructions:
            emb = self.sentence_encoder.encode(instruction)
            embeddings.append(torch.from_numpy(emb).float())
        
        return torch.stack(embeddings)
    
    def execute_instruction_chain(self, instructions, max_steps=1000):
        """ëª…ë ¹ ì²´ì¸ ì‹¤í–‰"""
        results = []
        obs = self.env.reset()
        
        for instruction in instructions:
            success = self.execute_single_instruction(
                instruction, 
                obs, 
                max_steps=max_steps // len(instructions)
            )
            results.append(success)
            
            if not success:
                break
        
        return results
    
    def execute_single_instruction(self, instruction, initial_obs, max_steps=300):
        """ë‹¨ì¼ ëª…ë ¹ ì‹¤í–‰"""
        obs = initial_obs
        
        for step in range(max_steps):
            # Get action from model
            action = self.predict_action(obs, instruction)
            
            # Step environment
            obs, reward, done, info = self.env.step(action)
            
            # Check if instruction completed
            if self.check_instruction_completion(instruction, obs, info):
                return True
        
        return False
    
    def predict_action(self, obs, instruction):
        """ì•¡ì…˜ ì˜ˆì¸¡"""
        self.model.eval()
        
        with torch.no_grad():
            # Process observation
            images, proprio = self.process_calvin_obs(obs)
            images = images.unsqueeze(0).to(self.model.device)
            proprio = proprio.unsqueeze(0).to(self.model.device)
            
            # Process language
            lang_emb = self.sentence_encoder.encode(instruction)
            lang_tensor = torch.from_numpy(lang_emb).float()
            lang_tensor = lang_tensor.unsqueeze(0).to(self.model.device)
            
            # Predict action
            action = self.model(images, lang_tensor, proprio)
            action = action.cpu().numpy()[0]
        
        return action
    
    def check_instruction_completion(self, instruction, obs, info):
        """ëª…ë ¹ ì™„ë£Œ í™•ì¸"""
        # This would need task-specific logic
        # CALVIN provides success checking in info
        return info.get('success', False)
```

### CALVIN í‰ê°€ í”„ë¡œí† ì½œ

```python
class CALVINEvaluator:
    """CALVIN ë²¤ì¹˜ë§ˆí¬ í‰ê°€"""
    
    def __init__(self, model, env, test_episodes):
        self.model = model
        self.env = env
        self.test_episodes = test_episodes
    
    def evaluate_success_rate(self):
        """SR_k ë©”íŠ¸ë¦­ ê³„ì‚° (kê°œ ì—°ì† ì„±ê³µ)"""
        results = {i: [] for i in range(1, 6)}  # SR_1 to SR_5
        
        for episode in self.test_episodes:
            instructions = episode['instructions']
            
            # Reset environment
            obs = self.env.reset()
            
            consecutive_success = 0
            for instruction in instructions[:5]:  # Max 5 instructions
                success = self.model.execute_single_instruction(
                    instruction, obs, max_steps=300
                )
                
                if success:
                    consecutive_success += 1
                    # Record success at each level
                    for k in range(1, consecutive_success + 1):
                        if k <= 5:
                            results[k].append(1)
                else:
                    # Record failure for remaining levels
                    for k in range(consecutive_success + 1, 6):
                        results[k].append(0)
                    break
        
        # Calculate average success rate
        sr_metrics = {}
        for k in range(1, 6):
            if results[k]:
                sr_metrics[f'SR_{k}'] = np.mean(results[k])
            else:
                sr_metrics[f'SR_{k}'] = 0.0
        
        # Average completed length
        avg_len = sum(k * sr_metrics[f'SR_{k}'] for k in range(1, 6))
        sr_metrics['Avg_Len'] = avg_len
        
        return sr_metrics
```

---

## ğŸ¤– 5. RoboSuite í†µí•©

### RoboSuiteìš© VLA ë˜í¼

```python
import robosuite as suite
from robosuite.wrappers import GymWrapper

class RoboSuiteVLA:
    """RoboSuite í™˜ê²½ìš© VLA ë˜í¼"""
    
    def __init__(self, model: BaseVLAModel, env_name='Lift', robot='Panda'):
        self.model = model
        
        # Create environment
        self.env = suite.make(
            env_name=env_name,
            robots=robot,
            has_renderer=False,
            has_offscreen_renderer=True,
            use_camera_obs=True,
            use_object_obs=True,
            horizon=200,
            reward_shaping=True,
            control_freq=20,
            camera_names=['agentview', 'robot0_eye_in_hand']
        )
        
        # Wrap for Gym interface
        self.env = GymWrapper(self.env)
        
        # Task descriptions for RoboSuite tasks
        self.task_descriptions = {
            'Lift': 'Lift the cube above the table',
            'Stack': 'Stack the red cube on the green cube',
            'NutAssembly': 'Place the nut on the peg',
            'PickPlace': 'Pick and place the object in the bin',
            'Door': 'Open the door',
            'Wipe': 'Wipe the table surface'
        }
    
    def process_robosuite_obs(self, obs):
        """RoboSuite ê´€ì°° ì²˜ë¦¬"""
        # Extract camera images
        agentview = obs['agentview_image']
        eye_in_hand = obs['robot0_eye_in_hand_image']
        
        # Convert to tensors
        agentview_tensor = torch.from_numpy(agentview).permute(2, 0, 1).float() / 255.0
        eye_tensor = torch.from_numpy(eye_in_hand).permute(2, 0, 1).float() / 255.0
        
        # Stack or concatenate views
        images = torch.cat([agentview_tensor, eye_tensor], dim=0)
        
        # Robot state
        robot_state = np.concatenate([
            obs['robot0_joint_pos'],
            obs['robot0_joint_vel'],
            obs['robot0_eef_pos'],
            obs['robot0_eef_quat'],
            [obs['robot0_gripper_qpos'][0]]  # Gripper state
        ])
        
        proprio = torch.from_numpy(robot_state).float()
        
        return images, proprio
    
    def predict(self, obs, task_name='Lift'):
        """ì•¡ì…˜ ì˜ˆì¸¡"""
        self.model.eval()
        
        with torch.no_grad():
            # Process observation
            images, proprio = self.process_robosuite_obs(obs)
            images = images.unsqueeze(0).to(self.model.device)
            proprio = proprio.unsqueeze(0).to(self.model.device)
            
            # Get task instruction
            instruction = self.task_descriptions.get(task_name, 'Complete the task')
            tokens = self.model.tokenizer(
                instruction,
                return_tensors='pt'
            )['input_ids'].to(self.model.device)
            
            # Predict action
            action = self.model(images, tokens, proprio)
            action = action.cpu().numpy()[0]
        
        return action
```

---

## ğŸ”¬ 6. í†µí•© í•™ìŠµ íŒŒì´í”„ë¼ì¸

### ë²”ìš© VLA í•™ìŠµê¸°

```python
class UniversalVLATrainer:
    """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë²”ìš© í•™ìŠµê¸°"""
    
    def __init__(self, model, benchmark_name, config):
        self.model = model
        self.benchmark_name = benchmark_name
        self.config = config
        
        # Initialize appropriate wrapper
        if benchmark_name == 'rlbench':
            self.wrapper = RLBenchVLA(model)
        elif benchmark_name == 'metaworld':
            self.wrapper = MetaWorldVLA(model)
        elif benchmark_name == 'calvin':
            self.wrapper = CALVINVLA(model, config.calvin_cfg_path)
        elif benchmark_name == 'robosuite':
            self.wrapper = RoboSuiteVLA(model)
        else:
            raise ValueError(f"Unknown benchmark: {benchmark_name}")
        
        # Training setup
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        self.scheduler = self.get_scheduler(config.scheduler_type)
        
        # Logging
        self.writer = SummaryWriter(config.log_dir)
        self.checkpoint_dir = config.checkpoint_dir
        
    def get_scheduler(self, scheduler_type):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì„ íƒ"""
        if scheduler_type == 'cosine':
            return torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, T_max=self.config.max_steps
            )
        elif scheduler_type == 'linear':
            return torch.optim.lr_scheduler.LinearLR(
                self.optimizer, 
                start_factor=1.0,
                end_factor=0.1,
                total_iters=self.config.max_steps
            )
        else:
            return None
    
    def train(self, num_epochs):
        """í†µí•© í•™ìŠµ ë£¨í”„"""
        for epoch in range(num_epochs):
            # Collect data
            train_data = self.collect_data()
            
            # Training
            epoch_loss = 0
            for batch in self.create_batches(train_data):
                loss = self.train_step(batch)
                epoch_loss += loss
            
            # Evaluation
            if epoch % self.config.eval_interval == 0:
                eval_metrics = self.evaluate()
                self.log_metrics(eval_metrics, epoch)
            
            # Save checkpoint
            if epoch % self.config.save_interval == 0:
                self.save_checkpoint(epoch)
            
            print(f"Epoch {epoch}: Loss = {epoch_loss:.4f}")
    
    def collect_data(self):
        """ë²¤ì¹˜ë§ˆí¬ë³„ ë°ì´í„° ìˆ˜ì§‘"""
        if self.benchmark_name == 'rlbench':
            return self.wrapper.collect_demonstration()
        elif self.benchmark_name == 'metaworld':
            return self.wrapper.collect_random_data()
        # ... etc
    
    def train_step(self, batch):
        """í†µí•© í•™ìŠµ ìŠ¤í…"""
        self.model.train()
        
        # Forward pass
        loss = self.wrapper.compute_loss(batch)
        
        # Backward pass
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()
        
        if self.scheduler:
            self.scheduler.step()
        
        return loss.item()
    
    def evaluate(self):
        """í†µí•© í‰ê°€"""
        self.model.eval()
        return self.wrapper.evaluate()
    
    def save_checkpoint(self, epoch):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'config': self.config
        }
        
        path = os.path.join(self.checkpoint_dir, f'checkpoint_{epoch}.pt')
        torch.save(checkpoint, path)
    
    def load_checkpoint(self, path):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if self.scheduler and checkpoint['scheduler_state_dict']:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        return checkpoint['epoch']
```

---

## ğŸ“Š 7. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„

### í†µí•© í‰ê°€ ì‹œìŠ¤í…œ

```python
class BenchmarkEvaluator:
    """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ í†µí•© í‰ê°€"""
    
    def __init__(self, model, benchmarks):
        self.model = model
        self.benchmarks = benchmarks
        self.results = {}
    
    def evaluate_all(self):
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ í‰ê°€"""
        for benchmark_name in self.benchmarks:
            print(f"Evaluating on {benchmark_name}...")
            
            if benchmark_name == 'rlbench':
                results = self.evaluate_rlbench()
            elif benchmark_name == 'metaworld':
                results = self.evaluate_metaworld()
            elif benchmark_name == 'calvin':
                results = self.evaluate_calvin()
            elif benchmark_name == 'robosuite':
                results = self.evaluate_robosuite()
            
            self.results[benchmark_name] = results
        
        return self.results
    
    def create_report(self):
        """í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            'summary': {},
            'detailed': self.results,
            'plots': {}
        }
        
        # Summary statistics
        for benchmark, results in self.results.items():
            report['summary'][benchmark] = {
                'success_rate': results.get('success_rate', 0),
                'average_reward': results.get('average_reward', 0),
                'completion_time': results.get('completion_time', 0)
            }
        
        # Generate plots
        report['plots'] = self.generate_plots()
        
        return report
    
    def generate_plots(self):
        """ì‹œê°í™” ìƒì„±"""
        import matplotlib.pyplot as plt
        
        plots = {}
        
        # Success rate comparison
        fig, ax = plt.subplots(figsize=(10, 6))
        benchmarks = list(self.results.keys())
        success_rates = [self.results[b].get('success_rate', 0) for b in benchmarks]
        
        ax.bar(benchmarks, success_rates)
        ax.set_ylabel('Success Rate')
        ax.set_title('VLA Performance Across Benchmarks')
        
        plt.tight_layout()
        plots['success_comparison'] = fig
        
        return plots
```

---

## ğŸ’¡ ì‹¤ì „ íŒ

### 1. ì‹œì‘í•˜ê¸°

```python
# Quick start example
model = BaseVLAModel(
    vision_encoder=VisionEncoder('resnet50'),
    language_encoder=LanguageEncoder('bert-base-uncased'),
    action_decoder=ActionDecoder()
)

# Choose benchmark
wrapper = RLBenchVLA(model)  # or MetaWorldVLA, etc.

# Train
trainer = UniversalVLATrainer(model, 'rlbench', config)
trainer.train(num_epochs=100)

# Evaluate
evaluator = BenchmarkEvaluator(model, ['rlbench'])
results = evaluator.evaluate_all()
```

### 2. ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

1. **ì•¡ì…˜ ê³µê°„ ë¶ˆì¼ì¹˜**: ì •ê·œí™”/ë¹„ì •ê·œí™” í™•ì¸
2. **ê´€ì°° í˜•ì‹ ì°¨ì´**: ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ê²€ì¦
3. **ì–¸ì–´ ì¸í„°í˜ì´ìŠ¤**: ë²¤ì¹˜ë§ˆí¬ë³„ ì ì ˆí•œ ëª…ë ¹ ìƒì„±
4. **ì„±ëŠ¥ ìµœì í™”**: ë°°ì¹˜ ì²˜ë¦¬, GPU í™œìš©

### 3. ìµœì í™” ì „ëµ

- **Multi-GPU Training**: DistributedDataParallel ì‚¬ìš©
- **Mixed Precision**: AMPë¡œ ì†ë„ í–¥ìƒ
- **Efficient Sampling**: ë³‘ë ¬ í™˜ê²½ ì‹¤í–‰
- **Curriculum Learning**: ì‰¬ìš´ ì‘ì—…ë¶€í„° ì‹œì‘

---

## ğŸ“š ì¶”ê°€ ìë£Œ

### ì½”ë“œ ì €ì¥ì†Œ
- [VLA-Benchmark-Integration](https://github.com/example/vla-benchmark)
- [Universal-Robot-Learning](https://github.com/example/url)

### íŠœí† ë¦¬ì–¼
- RLBench + VLA í†µí•© ë…¸íŠ¸ë¶
- Meta-World ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ ê°€ì´ë“œ
- CALVIN ì–¸ì–´ ì¡°ê±´ë¶€ ì œì–´ ì˜ˆì œ

---

## ğŸ¯ í•µì‹¬ ìš”ì•½

VLA ëª¨ë¸ì„ ë²¤ì¹˜ë§ˆí¬ì— í†µí•©í•˜ë ¤ë©´ ê° ë²¤ì¹˜ë§ˆí¬ì˜ ê´€ì°°/ì•¡ì…˜ ê³µê°„ì„ ì´í•´í•˜ê³  ì ì ˆí•œ ë˜í¼ë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤. í†µí•© ê³¼ì •ì€ 1) ê´€ì°° ì „ì²˜ë¦¬, 2) ì–¸ì–´ ëª…ë ¹ ì²˜ë¦¬, 3) ì•¡ì…˜ í›„ì²˜ë¦¬, 4) í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ë²”ìš© í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ë©´ ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.