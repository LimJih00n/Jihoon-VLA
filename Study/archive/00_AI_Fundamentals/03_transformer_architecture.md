# ğŸ—ï¸ Transformer Architecture

**ëª©í‘œ**: Transformerì˜ ì™„ì „í•œ êµ¬ì¡° ì´í•´ì™€ VLAì—ì„œì˜ í™œìš©

**ì‹œê°„**: 3-4ì‹œê°„

**ì „ì œì¡°ê±´**: Neural Networks, Attention Mechanism

---

## ğŸ¯ ê°œë°œìë¥¼ ìœ„í•œ Transformer ì§ê´€

### Transformer = Attention ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬ ì•„í‚¤í…ì²˜
```python
# ê¸°ì¡´ RNN/LSTM: ìˆœì°¨ ì²˜ë¦¬
def rnn_processing(sequence):
    hidden_state = initial_state
    for token in sequence:
        hidden_state = rnn_cell(token, hidden_state)
    return hidden_state

# Transformer: ë³‘ë ¬ ì²˜ë¦¬
def transformer_processing(sequence):
    # ëª¨ë“  í† í°ì„ ë™ì‹œì— ì²˜ë¦¬
    attended_sequence = multi_head_attention(sequence, sequence, sequence)
    processed_sequence = feed_forward(attended_sequence)
    return processed_sequence
```

### í•µì‹¬ í˜ì‹ 
```python
transformer_innovations = {
    "ë³‘ë ¬_ì²˜ë¦¬": "ëª¨ë“  ìœ„ì¹˜ë¥¼ ë™ì‹œì— ì²˜ë¦¬ â†’ í›¨ì”¬ ë¹ ë¥¸ í•™ìŠµ",
    "ìœ„ì¹˜_ì¸ì½”ë”©": "ìˆœì„œ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€",
    "Residual_Connection": "ê¹Šì€ ë„¤íŠ¸ì›Œí¬ ì•ˆì •ì  í•™ìŠµ",
    "Layer_Normalization": "ê° ì¸µì˜ ì¶œë ¥ ì •ê·œí™”"
}
```

---

## ğŸ—ï¸ Transformer êµ¬ì¡° ì™„ì „ ë¶„í•´

### 1. ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class TransformerBlock(nn.Module):
    """
    Transformerì˜ ê¸°ë³¸ ë¸”ë¡
    Multi-Head Attention + Feed Forward + Residual Connections
    """
    
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # Multi-Head Self-Attention
        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)
        
        # Feed Forward Network
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # 1. Multi-Head Self-Attention with Residual Connection
        attended, attention_weights = self.attention(x, x, x, attn_mask=mask)
        x1 = self.norm1(x + self.dropout(attended))
        
        # 2. Feed Forward with Residual Connection  
        fed_forward = self.feed_forward(x1)
        x2 = self.norm2(x1 + fed_forward)
        
        return x2, attention_weights

# ë‹¨ì¼ ë¸”ë¡ í…ŒìŠ¤íŠ¸
d_model, n_heads, d_ff = 256, 8, 1024
transformer_block = TransformerBlock(d_model, n_heads, d_ff)

# ì…ë ¥: (batch, seq_len, d_model)
x = torch.randn(2, 10, d_model)
output, attention = transformer_block(x)

print(f"Transformer Block:")
print(f"ì…ë ¥: {x.shape}")
print(f"ì¶œë ¥: {output.shape}")
print(f"Attention: {attention.shape}")
```

### 2. ìœ„ì¹˜ ì¸ì½”ë”© (Positional Encoding)
```python
class PositionalEncoding(nn.Module):
    """
    ìœ„ì¹˜ ì •ë³´ë¥¼ sine/cosine í•¨ìˆ˜ë¡œ ì¸ì½”ë”©
    TransformerëŠ” ìˆœì„œë¥¼ ëª¨ë¥´ë¯€ë¡œ ìœ„ì¹˜ ì •ë³´ í•„ìš”
    """
    
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        
        # ìœ„ì¹˜ ì¸ì½”ë”© í…Œì´ë¸” ìƒì„±
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        # div_term: 1/10000^(2i/d_model)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        # ì§ìˆ˜ ì¸ë±ìŠ¤: sin, í™€ìˆ˜ ì¸ë±ìŠ¤: cos
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        # í•™ìŠµë˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ë¡œ ë“±ë¡
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        # x: (seq_len, batch, d_model)
        x = x + self.pe[:x.size(0), :]
        return x

# ìœ„ì¹˜ ì¸ì½”ë”© ì‹œê°í™”
def visualize_positional_encoding():
    pe = PositionalEncoding(256, 100)
    
    # ì²˜ìŒ 100ê°œ ìœ„ì¹˜ì˜ ì¸ì½”ë”©
    dummy_input = torch.zeros(100, 1, 256)
    encoded = pe(dummy_input)
    
    # ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìœ„ì¹˜ ì¸ì½”ë”© ì‹œê°í™”
    pos_encoding = encoded[:, 0, :].detach().numpy()
    
    import matplotlib.pyplot as plt
    plt.figure(figsize=(12, 8))
    plt.imshow(pos_encoding.T, cmap='RdYlBu', aspect='auto')
    plt.colorbar()
    plt.xlabel('Position')
    plt.ylabel('Encoding Dimension')
    plt.title('Positional Encoding Visualization')
    plt.show()

# visualize_positional_encoding()  # ì‹¤í–‰í•˜ë©´ ê·¸ë˜í”„ ì¶œë ¥
```

### 3. ì™„ì „í•œ Transformer ì¸ì½”ë”
```python
class TransformerEncoder(nn.Module):
    """
    ì—¬ëŸ¬ Transformer ë¸”ë¡ì„ ìŒ“ì€ ì™„ì „í•œ ì¸ì½”ë”
    """
    
    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers, max_len=5000, dropout=0.1):
        super().__init__()
        
        self.d_model = d_model
        
        # í† í° ì„ë² ë”©
        self.embedding = nn.Embedding(vocab_size, d_model)
        
        # ìœ„ì¹˜ ì¸ì½”ë”©
        self.positional_encoding = PositionalEncoding(d_model, max_len)
        
        # ì—¬ëŸ¬ Transformer ë¸”ë¡
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, src, src_mask=None):
        # 1. í† í° ì„ë² ë”© + ìŠ¤ì¼€ì¼ë§
        embedded = self.embedding(src) * math.sqrt(self.d_model)  # (batch, seq, d_model)
        
        # 2. ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        # PyTorch MultiheadAttention expects (seq, batch, d_model)
        embedded = embedded.transpose(0, 1)  # (seq, batch, d_model)
        embedded = self.positional_encoding(embedded)
        embedded = embedded.transpose(0, 1)  # (batch, seq, d_model)
        embedded = self.dropout(embedded)
        
        # 3. Transformer ë¸”ë¡ë“¤ í†µê³¼
        x = embedded
        attention_weights = []
        
        for transformer_block in self.transformer_blocks:
            x, attention = transformer_block(x, src_mask)
            attention_weights.append(attention)
        
        return x, attention_weights

# ì™„ì „í•œ ì¸ì½”ë” í…ŒìŠ¤íŠ¸
encoder = TransformerEncoder(
    vocab_size=10000,
    d_model=256,
    n_heads=8,
    d_ff=1024,
    n_layers=6
)

# ì…ë ¥ í† í° ì‹œí€€ìŠ¤
input_tokens = torch.randint(0, 10000, (2, 12))  # batch_size=2, seq_len=12
encoded_output, all_attention = encoder(input_tokens)

print(f"Transformer Encoder:")
print(f"ì…ë ¥ í† í°: {input_tokens.shape}")
print(f"ì¸ì½”ë”©ëœ ì¶œë ¥: {encoded_output.shape}")
print(f"ë ˆì´ì–´ë³„ Attention: {len(all_attention)} layers")
```

### 4. ë§ˆìŠ¤í‚¹ (Masking) ë©”ì»¤ë‹ˆì¦˜
```python
def create_padding_mask(seq, pad_token=0):
    """
    íŒ¨ë”© í† í°ì„ ë¬´ì‹œí•˜ëŠ” ë§ˆìŠ¤í¬ ìƒì„±
    """
    # seq: (batch, seq_len)
    return seq != pad_token

def create_look_ahead_mask(seq_len):
    """
    ë¯¸ë˜ í† í°ì„ ë³´ì§€ ëª»í•˜ê²Œ í•˜ëŠ” ë§ˆìŠ¤í¬ (ë””ì½”ë”ìš©)
    """
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    return mask == 0  # TrueëŠ” ë³´ì—¬ì¤„ ë¶€ë¶„, FalseëŠ” ë§ˆìŠ¤í¬í•  ë¶€ë¶„

def demonstrate_masking():
    """
    ë§ˆìŠ¤í‚¹ì˜ íš¨ê³¼ ì‹œì—°
    """
    seq_len = 5
    
    print("=== Look-Ahead Mask ===")
    look_ahead_mask = create_look_ahead_mask(seq_len)
    print("í˜„ì¬ ìœ„ì¹˜ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” í† í° (True), ë§ˆìŠ¤í¬ëœ í† í° (False):")
    print(look_ahead_mask.int())
    
    print("\n=== Padding Mask ===")
    # ì˜ˆì‹œ: [1, 25, 463, 78, 0, 0] (0ì€ íŒ¨ë”©)
    sequence = torch.tensor([[1, 25, 463, 78, 0, 0],
                           [15, 234, 0, 0, 0, 0]])
    padding_mask = create_padding_mask(sequence)
    print("ì‹¤ì œ í† í° (True), íŒ¨ë”© (False):")
    print(padding_mask)

demonstrate_masking()
```

---

## ğŸ¤– VLAë¥¼ ìœ„í•œ íŠ¹ë³„í•œ Transformer ë³€í˜•

### 1. Vision Transformer (ViT) for VLA
```python
class VisionTransformer(nn.Module):
    """
    ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ê³  Transformerë¡œ ì²˜ë¦¬
    VLAì—ì„œ ì‹œê° ì •ë³´ ì²˜ë¦¬ìš©
    """
    
    def __init__(self, img_size=224, patch_size=16, in_channels=3, 
                 d_model=768, n_heads=12, n_layers=12):
        super().__init__()
        
        self.patch_size = patch_size
        self.n_patches = (img_size // patch_size) ** 2
        
        # ì´ë¯¸ì§€ íŒ¨ì¹˜ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        self.patch_embedding = nn.Conv2d(
            in_channels, d_model, 
            kernel_size=patch_size, stride=patch_size
        )
        
        # í´ë˜ìŠ¤ í† í° (ì „ì²´ ì´ë¯¸ì§€ ì •ë³´)
        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))
        
        # ìœ„ì¹˜ ì„ë² ë”© (íŒ¨ì¹˜ ìœ„ì¹˜ ì •ë³´)
        self.pos_embedding = nn.Parameter(
            torch.randn(1, self.n_patches + 1, d_model)
        )
        
        # Transformer ë¸”ë¡ë“¤
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_model*4)
            for _ in range(n_layers)
        ])
        
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x):
        batch_size = x.size(0)
        
        # 1. ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë¶„í•  ë° ì„ë² ë”©
        # x: (batch, 3, 224, 224) -> patches: (batch, d_model, 14, 14)
        patches = self.patch_embedding(x)  
        patches = patches.flatten(2).transpose(1, 2)  # (batch, n_patches, d_model)
        
        # 2. í´ë˜ìŠ¤ í† í° ì¶”ê°€
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat([cls_tokens, patches], dim=1)  # (batch, n_patches+1, d_model)
        
        # 3. ìœ„ì¹˜ ì„ë² ë”© ì¶”ê°€
        x = x + self.pos_embedding
        x = self.dropout(x)
        
        # 4. Transformer ë¸”ë¡ë“¤ í†µê³¼
        for transformer_block in self.transformer_blocks:
            x, _ = transformer_block(x)
        
        # 5. í´ë˜ìŠ¤ í† í° ë°˜í™˜ (ì „ì²´ ì´ë¯¸ì§€ í‘œí˜„)
        return x[:, 0]  # (batch, d_model)

# Vision Transformer í…ŒìŠ¤íŠ¸
vit = VisionTransformer(img_size=224, patch_size=16, d_model=768)
dummy_image = torch.randn(2, 3, 224, 224)
image_features = vit(dummy_image)

print(f"Vision Transformer:")
print(f"ì´ë¯¸ì§€ ì…ë ¥: {dummy_image.shape}")
print(f"ì´ë¯¸ì§€ íŠ¹ì„±: {image_features.shape}")
```

### 2. Cross-Modal Transformer for VLA
```python
class CrossModalTransformer(nn.Module):
    """
    ë¹„ì „ê³¼ ì–¸ì–´ë¥¼ ê²°í•©í•˜ëŠ” Cross-Modal Transformer
    VLAì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ
    """
    
    def __init__(self, vision_dim=768, language_dim=512, d_model=512, 
                 n_heads=8, n_layers=6):
        super().__init__()
        
        # ê° ëª¨ë‹¬ë¦¬í‹°ë¥¼ ê³µí†µ ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜
        self.vision_proj = nn.Linear(vision_dim, d_model)
        self.language_proj = nn.Linear(language_dim, d_model)
        
        # ëª¨ë‹¬ë¦¬í‹° êµ¬ë¶„ì„ ìœ„í•œ íƒ€ì… ì„ë² ë”©
        self.modal_type_embedding = nn.Embedding(2, d_model)  # 0: vision, 1: language
        
        # Cross-Modal Transformer ë¸”ë¡ë“¤
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_model*4)
            for _ in range(n_layers)
        ])
        
        # ì•¡ì…˜ ì˜ˆì¸¡ í—¤ë“œ
        self.action_head = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.ReLU(),
            nn.Linear(256, 7)  # 7-DOF ë¡œë´‡ ì•¡ì…˜
        )
    
    def forward(self, vision_features, language_features):
        """
        vision_features: (batch, n_patches, vision_dim)
        language_features: (batch, seq_len, language_dim)
        """
        batch_size = vision_features.size(0)
        
        # 1. ê° ëª¨ë‹¬ë¦¬í‹°ë¥¼ ê³µí†µ ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜
        vis_projected = self.vision_proj(vision_features)  # (batch, n_patches, d_model)
        lang_projected = self.language_proj(language_features)  # (batch, seq_len, d_model)
        
        # 2. ëª¨ë‹¬ë¦¬í‹° íƒ€ì… ì„ë² ë”© ì¶”ê°€
        n_patches = vis_projected.size(1)
        seq_len = lang_projected.size(1)
        
        vis_type = torch.zeros(batch_size, n_patches, dtype=torch.long, device=vis_projected.device)
        lang_type = torch.ones(batch_size, seq_len, dtype=torch.long, device=lang_projected.device)
        
        vis_projected += self.modal_type_embedding(vis_type)
        lang_projected += self.modal_type_embedding(lang_type)
        
        # 3. ë¹„ì „ê³¼ ì–¸ì–´ í† í° ê²°í•©
        combined_features = torch.cat([vis_projected, lang_projected], dim=1)
        # (batch, n_patches + seq_len, d_model)
        
        # 4. Cross-Modal Transformer ì²˜ë¦¬
        for transformer_block in self.transformer_blocks:
            combined_features, _ = transformer_block(combined_features)
        
        # 5. Global average pooling í›„ ì•¡ì…˜ ì˜ˆì¸¡
        global_features = combined_features.mean(dim=1)  # (batch, d_model)
        actions = self.action_head(global_features)
        
        return actions

# Cross-Modal Transformer í…ŒìŠ¤íŠ¸
cross_modal = CrossModalTransformer()

# ë”ë¯¸ ì…ë ¥
vision_feat = torch.randn(2, 196, 768)  # 14x14 = 196 íŒ¨ì¹˜
language_feat = torch.randn(2, 8, 512)   # 8ê°œ ë‹¨ì–´

predicted_actions = cross_modal(vision_feat, language_feat)

print(f"Cross-Modal Transformer:")
print(f"ë¹„ì „ ì…ë ¥: {vision_feat.shape}")  
print(f"ì–¸ì–´ ì…ë ¥: {language_feat.shape}")
print(f"ì˜ˆì¸¡ ì•¡ì…˜: {predicted_actions.shape}")
```

### 3. ì™„ì „í•œ VLA Transformer ëª¨ë¸
```python
class VLATransformer(nn.Module):
    """
    ì™„ì „í•œ Vision-Language-Action Transformer
    """
    
    def __init__(self, vocab_size=10000, img_size=224, patch_size=16,
                 d_model=512, n_heads=8, n_layers=6, action_dim=7):
        super().__init__()
        
        # Vision Transformer (ì´ë¯¸ì§€ ì¸ì½”ë”)
        self.vision_encoder = VisionTransformer(
            img_size=img_size,
            patch_size=patch_size,
            d_model=d_model,
            n_heads=n_heads,
            n_layers=n_layers//2  # ì ˆë°˜ì€ ë¹„ì „, ì ˆë°˜ì€ í¬ë¡œìŠ¤ëª¨ë‹¬
        )
        
        # Language Transformer (ì–¸ì–´ ì¸ì½”ë”)
        self.language_encoder = TransformerEncoder(
            vocab_size=vocab_size,
            d_model=d_model,
            n_heads=n_heads,
            d_ff=d_model*4,
            n_layers=n_layers//2
        )
        
        # Cross-Modal Fusion
        self.cross_modal_fusion = CrossModalTransformer(
            vision_dim=d_model,
            language_dim=d_model,
            d_model=d_model,
            n_heads=n_heads,
            n_layers=n_layers//2
        )
    
    def forward(self, images, instruction_tokens):
        """
        ì™„ì „í•œ VLA forward pass
        """
        # 1. ë¹„ì „ ì¸ì½”ë”©
        vision_cls = self.vision_encoder(images)  # (batch, d_model)
        # íŒ¨ì¹˜ë³„ íŠ¹ì„±ë„ í•„ìš”í•˜ë©´ ì „ì²´ ì‹œí€€ìŠ¤ ì‚¬ìš©
        
        # 2. ì–¸ì–´ ì¸ì½”ë”©  
        language_encoded, _ = self.language_encoder(instruction_tokens)
        # (batch, seq_len, d_model)
        
        # 3. í¬ë¡œìŠ¤ ëª¨ë‹¬ ìœµí•©ì„ ìœ„í•´ ì°¨ì› ë§ì¶¤
        vision_expanded = vision_cls.unsqueeze(1)  # (batch, 1, d_model)
        
        # 4. ìµœì¢… ì•¡ì…˜ ì˜ˆì¸¡
        actions = self.cross_modal_fusion(vision_expanded, language_encoded)
        
        return actions

# ì „ì²´ VLA Transformer í…ŒìŠ¤íŠ¸
vla_transformer = VLATransformer()

dummy_images = torch.randn(2, 3, 224, 224)
dummy_instructions = torch.randint(0, 10000, (2, 8))

final_actions = vla_transformer(dummy_images, dummy_instructions)

print(f"ì™„ì „í•œ VLA Transformer:")
print(f"ì´ë¯¸ì§€: {dummy_images.shape}")
print(f"ëª…ë ¹ì–´: {dummy_instructions.shape}")  
print(f"ìµœì¢… ì•¡ì…˜: {final_actions.shape}")
print(f"ì•¡ì…˜ ê°’: {final_actions}")
```

---

## ğŸ”¬ Transformer vs ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ ë¹„êµ

### 1. RNN/LSTM vs Transformer
```python
comparison_rnn_transformer = {
    "ì²˜ë¦¬_ë°©ì‹": {
        "RNN/LSTM": "ìˆœì°¨ì  ì²˜ë¦¬ (t=1 â†’ t=2 â†’ t=3)",
        "Transformer": "ë³‘ë ¬ ì²˜ë¦¬ (ëª¨ë“  t ë™ì‹œì—)"
    },
    
    "ì¥ê±°ë¦¬_ì˜ì¡´ì„±": {
        "RNN/LSTM": "ì •ë³´ê°€ ì ì°¨ í¬ì„ë¨ (Vanishing gradient)",
        "Transformer": "ì§ì ‘ ì—°ê²° (Attentionìœ¼ë¡œ ëª¨ë“  ìœ„ì¹˜ ì ‘ê·¼)"
    },
    
    "í•™ìŠµ_ì†ë„": {
        "RNN/LSTM": "ëŠë¦¼ (ìˆœì°¨ ì²˜ë¦¬)",
        "Transformer": "ë¹ ë¦„ (ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥)"
    },
    
    "ë©”ëª¨ë¦¬": {
        "RNN/LSTM": "ì ìŒ",
        "Transformer": "ë§ìŒ (ëª¨ë“  ìœ„ì¹˜ ê°„ attention ê³„ì‚°)"
    }
}
```

### 2. CNN vs Vision Transformer
```python
comparison_cnn_vit = {
    "ê·€ë‚©ì _í¸í–¥": {
        "CNN": "êµ­ì†Œì„± (locality), ì´ë™ ë¶ˆë³€ì„± (translation invariance)",
        "ViT": "ì—†ìŒ (ë°ì´í„°ë¡œë¶€í„° ëª¨ë“  ê²ƒì„ í•™ìŠµ)"
    },
    
    "ìˆ˜ìš©_ì˜ì—­": {
        "CNN": "ì ì§„ì  í™•ì¥ (ì‘ì€ í•„í„° â†’ í° ìˆ˜ìš© ì˜ì—­)",
        "ViT": "ê¸€ë¡œë²Œ (ì²« ë²ˆì§¸ ì¸µë¶€í„° ì „ì²´ ì´ë¯¸ì§€ ì ‘ê·¼)"
    },
    
    "ë°ì´í„°_ìš”êµ¬ëŸ‰": {
        "CNN": "ì ìŒ (ê·€ë‚©ì  í¸í–¥ ë•ë¶„)",
        "ViT": "ë§ìŒ (í¸í–¥ ì—†ì´ ëª¨ë“  ê²ƒì„ í•™ìŠµí•´ì•¼ í•¨)"
    }
}
```

---

## ğŸ› ï¸ ì‹¤ìŠµ: Transformer í›ˆë ¨ ë° ë¶„ì„

### 1. ê°„ë‹¨í•œ ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤ ì‘ì—…
```python
def create_copy_task_data(batch_size=32, seq_len=10, vocab_size=100):
    """
    ê°„ë‹¨í•œ ë³µì‚¬ ì‘ì—…: ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥
    Transformerê°€ ì˜ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸ìš©
    """
    # ëœë¤ ì‹œí€€ìŠ¤ ìƒì„±
    src = torch.randint(1, vocab_size, (batch_size, seq_len))
    tgt = src.clone()  # ë³µì‚¬ ì‘ì—…ì´ë¯€ë¡œ íƒ€ê²Ÿì€ ì†ŒìŠ¤ì™€ ë™ì¼
    
    return src, tgt

def train_transformer_copy_task():
    """
    Transformerë¡œ ë³µì‚¬ ì‘ì—… í•™ìŠµ
    """
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = TransformerEncoder(
        vocab_size=100,
        d_model=128,
        n_heads=4,
        d_ff=256,
        n_layers=2
    )
    
    # ì¶œë ¥ í—¤ë“œ ì¶”ê°€ (vocab_size ì°¨ì›ìœ¼ë¡œ ì˜ˆì¸¡)
    output_head = nn.Linear(128, 100)
    
    optimizer = torch.optim.Adam(
        list(model.parameters()) + list(output_head.parameters()), 
        lr=0.001
    )
    criterion = nn.CrossEntropyLoss()
    
    print("ë³µì‚¬ ì‘ì—… í•™ìŠµ ì‹œì‘...")
    
    for epoch in range(100):
        # ë°ì´í„° ìƒì„±
        src, tgt = create_copy_task_data(batch_size=32, seq_len=8)
        
        # Forward pass
        encoded, _ = model(src)  # (batch, seq_len, d_model)
        logits = output_head(encoded)  # (batch, seq_len, vocab_size)
        
        # ì†ì‹¤ ê³„ì‚°
        loss = criterion(
            logits.view(-1, 100),  # (batch*seq_len, vocab_size)
            tgt.view(-1)           # (batch*seq_len,)
        )
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if epoch % 20 == 0:
            print(f"Epoch {epoch}: Loss = {loss.item():.4f}")
    
    # í…ŒìŠ¤íŠ¸
    with torch.no_grad():
        test_src, test_tgt = create_copy_task_data(batch_size=1, seq_len=5)
        encoded, _ = model(test_src)
        logits = output_head(encoded)
        predictions = logits.argmax(dim=-1)
        
        print(f"\ní…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"ì…ë ¥:    {test_src[0].tolist()}")
        print(f"ì •ë‹µ:    {test_tgt[0].tolist()}")
        print(f"ì˜ˆì¸¡:    {predictions[0].tolist()}")
        print(f"ì •í™•ë„:  {(predictions[0] == test_tgt[0]).float().mean().item():.2f}")

# ì‹¤í–‰
train_transformer_copy_task()
```

### 2. Attention íŒ¨í„´ ë¶„ì„
```python
def analyze_attention_patterns(model, input_sequence):
    """
    Transformerì˜ attention íŒ¨í„´ ë¶„ì„
    """
    model.eval()
    with torch.no_grad():
        _, attention_weights = model(input_sequence)
    
    # ê° ë ˆì´ì–´ì˜ attention ë¶„ì„
    for layer_idx, attention in enumerate(attention_weights):
        print(f"\n=== Layer {layer_idx + 1} ===")
        
        # attention: (batch, n_heads, seq_len, seq_len)
        batch_idx = 0  # ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ ë¶„ì„
        
        for head_idx in range(attention.size(1)):
            head_attention = attention[batch_idx, head_idx]  # (seq_len, seq_len)
            
            # ê° ìœ„ì¹˜ê°€ ì–´ë””ì— ê°€ì¥ ë§ì´ ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ëŠ”ì§€
            max_attention_pos = head_attention.argmax(dim=-1)
            
            print(f"Head {head_idx}: ê° ìœ„ì¹˜ì˜ ìµœëŒ€ attention ëŒ€ìƒ")
            for pos, target in enumerate(max_attention_pos):
                print(f"  Position {pos} â†’ Position {target.item()}")

# ë¶„ì„ ì‹¤í–‰ ì˜ˆì‹œ
def demo_attention_analysis():
    model = TransformerEncoder(vocab_size=100, d_model=64, n_heads=2, d_ff=128, n_layers=2)
    test_input = torch.randint(1, 100, (1, 6))  # ë°°ì¹˜ í¬ê¸° 1, ì‹œí€€ìŠ¤ ê¸¸ì´ 6
    
    analyze_attention_patterns(model, test_input)

demo_attention_analysis()
```

---

## ğŸ“ˆ VLAì—ì„œ Transformer í™œìš© ì „ëµ

### 1. ê³„ì¸µì  ì²˜ë¦¬ (Hierarchical Processing)
```python
vla_transformer_strategy = {
    "Low_Level": {
        "ì—­í• ": "ì¦‰ê°ì ì¸ ì„¼ì„œ-ì•¡ì…˜ ë§¤í•‘",
        "ì•„í‚¤í…ì²˜": "ì‘ì€ Transformer (2-4 layers)",
        "ì²˜ë¦¬": "í˜„ì¬ ê´€ì°° â†’ ì¦‰ê°ì  ì•¡ì…˜"
    },
    
    "Mid_Level": {
        "ì—­í• ": "íƒœìŠ¤í¬ ìˆ˜ì¤€ ì¶”ë¡ ",
        "ì•„í‚¤í…ì²˜": "ì¤‘ê°„ Transformer (6-8 layers)",  
        "ì²˜ë¦¬": "ëª…ë ¹ì–´ + ê´€ì°° â†’ ì„œë¸Œ ê³¨"
    },
    
    "High_Level": {
        "ì—­í• ": "ì¥ê¸° ê³„íš ë° ì¶”ë¡ ",
        "ì•„í‚¤í…ì²˜": "í° Transformer (12+ layers)",
        "ì²˜ë¦¬": "ë³µì¡í•œ ëª…ë ¹ì–´ â†’ ì „ì²´ ê³„íš"
    }
}
```

### 2. íš¨ìœ¨ì„± ìµœì í™”
```python
efficiency_optimizations = {
    "Attention_Sparsity": {
        "ë¬¸ì œ": "O(nÂ²) ë³µì¡ë„ë¡œ ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ëŠë¦¼",
        "í•´ê²°": "Sparse attention, Local attention",
        "VLA_ì ìš©": "ìµœê·¼ ê´€ì°°ì—ë§Œ ì§‘ì¤‘"
    },
    
    "Model_Distillation": {
        "ë¬¸ì œ": "í° ëª¨ë¸ì€ ì‹¤ì‹œê°„ ë¡œë´‡ ì œì–´ì— ë¶€ì í•©",
        "í•´ê²°": "í° ëª¨ë¸ â†’ ì‘ì€ ëª¨ë¸ ì§€ì‹ ì¦ë¥˜",
        "VLA_ì ìš©": "GPT-4 teacher â†’ ë¡œë´‡ìš© student"
    },
    
    "Dynamic_Inference": {
        "ë¬¸ì œ": "ëª¨ë“  ìƒí™©ì— ë™ì¼í•œ ê³„ì‚°ëŸ‰ ë‚­ë¹„",
        "í•´ê²°": "ìƒí™©ì— ë”°ë¥¸ ë™ì  ê¹Šì´ ì¡°ì ˆ",
        "VLA_ì ìš©": "ê¸´ê¸‰ìƒí™© â†’ ë¹ ë¥¸ ì¶”ë¡ , ë³µì¡í•œ ê³„íš â†’ ê¹Šì€ ì¶”ë¡ "
    }
}
```

---

## ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸

### ê¸°ì–µí•´ì•¼ í•  ê²ƒ
1. **ë³‘ë ¬ ì²˜ë¦¬ì˜ í˜**: RNN ëŒ€ë¹„ í›¨ì”¬ ë¹ ë¥¸ í•™ìŠµê³¼ ì¶”ë¡ 
2. **Attentionì˜ í™œìš©**: ì…ë ¥ì˜ ëª¨ë“  ë¶€ë¶„ì— ì§ì ‘ ì ‘ê·¼ ê°€ëŠ¥
3. **Residual Connection**: ê¹Šì€ ë„¤íŠ¸ì›Œí¬ë¥¼ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ
4. **ìœ„ì¹˜ ì¸ì½”ë”©**: ìˆœì„œ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì œê³µ

### VLA ì—°êµ¬ì—ì„œì˜ ì˜ë¯¸
- **ê¸°ë³¸ ì•„í‚¤í…ì²˜**: í˜„ëŒ€ VLA ëª¨ë¸ì˜ í‘œì¤€ êµ¬ì¡°
- **ëª¨ë‹¬ë¦¬í‹° í†µí•©**: Cross-attentionìœ¼ë¡œ ë¹„ì „ê³¼ ì–¸ì–´ íš¨ê³¼ì  ê²°í•©
- **í™•ì¥ì„±**: ë” ë§ì€ ë°ì´í„°ì™€ ê³„ì‚°ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥
- **ì „ì´ í•™ìŠµ**: ì‚¬ì „ í•™ìŠµëœ Transformerë¥¼ VLAì— í™œìš©

### ë‹¤ìŒ ë‹¨ê³„
1. **Multi-Modal Learning** (`04_multimodal_learning.md`) - Transformer ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ
2. **Vision Encoders** (`05_vision_encoders.md`) - ViT ì‹¬í™” í•™ìŠµ  
3. **Language Models** (`06_language_models.md`) - GPT ìŠ¤íƒ€ì¼ Transformer

**ë‹¤ìŒ: Multi-Modal Learningìœ¼ë¡œ!** ğŸš€

---

*Created: 2025-08-24*  
*Time: 3-4 hours*  
*Next: 04_multimodal_learning.md*