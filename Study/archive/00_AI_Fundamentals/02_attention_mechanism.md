# ğŸ” Attention Mechanism

**ëª©í‘œ**: Attentionì˜ í•µì‹¬ ê°œë…ê³¼ VLAì—ì„œì˜ í™œìš©ì„ ì½”ë“œë¡œ ì™„ì „ ì´í•´

**ì‹œê°„**: 2-3ì‹œê°„

**ì „ì œì¡°ê±´**: Neural Networks Basics

---

## ğŸ¯ ê°œë°œìë¥¼ ìœ„í•œ Attention ì§ê´€

### Attention = ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜
```python
# ì „í†µì ì¸ ë°©ë²•: ëª¨ë“  ì •ë³´ë¥¼ ë˜‘ê°™ì´ ì²˜ë¦¬
def traditional_processing(sequence):
    # ëª¨ë“  ìš”ì†Œì— ë™ì¼í•œ ê°€ì¤‘ì¹˜
    return sum(sequence) / len(sequence)

# Attention ë°©ë²•: ì¤‘ìš”ë„ì— ë”°ë¼ ê°€ì¤‘ í‰ê· 
def attention_processing(sequence, query):
    # 1. ê° ìš”ì†Œì˜ ì¤‘ìš”ë„ ê³„ì‚°
    attention_scores = compute_importance(sequence, query)
    # 2. ì¤‘ìš”ë„ë¡œ ê°€ì¤‘ í‰ê· 
    weighted_sum = sum(score * value for score, value in zip(attention_scores, sequence))
    return weighted_sum
```

### í•µì‹¬ ì•„ì´ë””ì–´
```python
attention_concept = {
    "Query (Q)": "ì§€ê¸ˆ ì°¾ê³ ì í•˜ëŠ” ì •ë³´ (ì§ˆë¬¸)",
    "Key (K)": "ê° ìœ„ì¹˜ì˜ ì •ë³´ ì‹ë³„ì (ìƒ‰ì¸)",  
    "Value (V)": "ê° ìœ„ì¹˜ì˜ ì‹¤ì œ ì •ë³´ (ë‚´ìš©)",
    "ê³¼ì •": "Qì™€ Kë¥¼ ë¹„êµí•´ì„œ ì¤‘ìš”ë„ ê³„ì‚° â†’ Vë¥¼ ì¤‘ìš”ë„ë¡œ ê°€ì¤‘í•©"
}
```

---

## ğŸ” Step-by-Step: Attention êµ¬í˜„

### 1. Scaled Dot-Product Attention (ê°€ì¥ ê¸°ë³¸)
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

def scaled_dot_product_attention(query, key, value, mask=None):
    """
    Attention(Q,K,V) = softmax(QK^T / âˆšd_k)V
    """
    # 1. Qì™€ Kì˜ ë‚´ì ìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
    # query: (batch, seq_len, d_model)
    # key: (batch, seq_len, d_model)
    scores = torch.matmul(query, key.transpose(-2, -1))  # (batch, seq_len, seq_len)
    
    # 2. ìŠ¤ì¼€ì¼ë§ (ê·¸ë¼ë””ì–¸íŠ¸ ì•ˆì •í™”)
    d_k = query.size(-1)
    scores = scores / math.sqrt(d_k)
    
    # 3. ë§ˆìŠ¤í‚¹ (ì„ íƒì , íŠ¹ì • ìœ„ì¹˜ ë¬´ì‹œ)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # 4. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥  ë¶„í¬ ìƒì„±
    attention_weights = F.softmax(scores, dim=-1)  # (batch, seq_len, seq_len)
    
    # 5. ê°€ì¤‘í•©ìœ¼ë¡œ ìµœì¢… ê²°ê³¼
    output = torch.matmul(attention_weights, value)  # (batch, seq_len, d_model)
    
    return output, attention_weights

# ì˜ˆì‹œ ì‹¤í–‰
batch_size, seq_len, d_model = 2, 4, 8

query = torch.randn(batch_size, seq_len, d_model)
key = torch.randn(batch_size, seq_len, d_model)  
value = torch.randn(batch_size, seq_len, d_model)

output, weights = scaled_dot_product_attention(query, key, value)

print(f"ì…ë ¥ í¬ê¸°: Q{query.shape}, K{key.shape}, V{value.shape}")
print(f"ì¶œë ¥ í¬ê¸°: {output.shape}")
print(f"Attention ê°€ì¤‘ì¹˜ í¬ê¸°: {weights.shape}")
print(f"ê°€ì¤‘ì¹˜ í•©ê³„ (ê° í–‰): {weights.sum(dim=-1)}")  # ëª¨ë‘ 1ì´ì–´ì•¼ í•¨
```

### 2. Multi-Head Attention (ë³‘ë ¬ ì²˜ë¦¬)
```python
class MultiHeadAttention(nn.Module):
    """
    ì—¬ëŸ¬ ê°œì˜ attention headë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
    ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì •ë³´ ìˆ˜ì§‘
    """
    
    def __init__(self, d_model, n_heads):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # ê° í—¤ë“œìš© ì„ í˜• ë³€í™˜
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # 1. Q, K, V ìƒì„± ë° í—¤ë“œë³„ ë¶„ë¦¬
        Q = self.w_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # 2. ê° í—¤ë“œì—ì„œ ì–´í…ì…˜ ê³„ì‚°
        attention_output, attention_weights = self.attention(Q, K, V, mask)
        
        # 3. í—¤ë“œë“¤ì„ ë‹¤ì‹œ í•©ì¹˜ê¸°
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        # 4. ì¶œë ¥ íˆ¬ì˜
        output = self.w_o(attention_output)
        
        return output, attention_weights
    
    def attention(self, query, key, value, mask=None):
        """ê° í—¤ë“œì—ì„œ ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜"""
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
            
        attention_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attention_weights, value)
        
        return output, attention_weights

# ì˜ˆì‹œ ì‚¬ìš©
d_model, n_heads = 256, 8
mha = MultiHeadAttention(d_model, n_heads)

# ì…ë ¥ ë°ì´í„°
seq_len = 10
x = torch.randn(2, seq_len, d_model)

# Self-attention (Q, K, Vê°€ ëª¨ë‘ ê°™ì€ ì…ë ¥)
output, weights = mha(x, x, x)

print(f"Multi-Head Attention ì¶œë ¥: {output.shape}")
print(f"Attention ê°€ì¤‘ì¹˜: {weights.shape}")  # (batch, n_heads, seq_len, seq_len)
```

### 3. Self-Attention vs Cross-Attention
```python
def demonstrate_attention_types():
    """
    Self-Attentionê³¼ Cross-Attentionì˜ ì°¨ì´ì  ì‹œì—°
    """
    d_model = 64
    mha = MultiHeadAttention(d_model, 4)
    
    # ë¬¸ì¥: "The robot picks up the red cup"
    sentence = torch.randn(1, 6, d_model)  # 6ê°œ ë‹¨ì–´
    
    # ì´ë¯¸ì§€ íŠ¹ì„±
    image_features = torch.randn(1, 49, d_model)  # 7x7 ì´ë¯¸ì§€ íŒ¨ì¹˜
    
    print("=== Self-Attention ===")
    # Self-attention: ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„
    self_attended, self_weights = mha(sentence, sentence, sentence)
    print(f"ì…ë ¥: {sentence.shape} (ë¬¸ì¥)")
    print(f"ì¶œë ¥: {self_attended.shape} (ìê¸° ì°¸ì¡° í›„)")
    print(f"Attention ê°€ì¤‘ì¹˜: {self_weights.shape}")
    
    print("\n=== Cross-Attention ===")  
    # Cross-attention: ë¬¸ì¥ì´ ì´ë¯¸ì§€ë¥¼ ì°¸ì¡°
    cross_attended, cross_weights = mha(
        query=sentence,      # ë¬¸ì¥ì—ì„œ ì§ˆë¬¸
        key=image_features,  # ì´ë¯¸ì§€ì—ì„œ ê²€ìƒ‰
        value=image_features # ì´ë¯¸ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ
    )
    print(f"Query: {sentence.shape} (ë¬¸ì¥)")
    print(f"Key/Value: {image_features.shape} (ì´ë¯¸ì§€)")
    print(f"ì¶œë ¥: {cross_attended.shape} (ì´ë¯¸ì§€ ì •ë³´ê°€ ë°˜ì˜ëœ ë¬¸ì¥)")
    print(f"Cross-attention ê°€ì¤‘ì¹˜: {cross_weights.shape}")

demonstrate_attention_types()
```

---

## ğŸ¤– VLAì—ì„œì˜ Attention í™œìš©

### 1. Visual Attention (ì´ë¯¸ì§€ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ ì°¾ê¸°)
```python
class VisualAttention(nn.Module):
    """
    ë¡œë´‡ì´ ì´ë¯¸ì§€ì—ì„œ ì¤‘ìš”í•œ ì˜ì—­ì— ì§‘ì¤‘
    """
    
    def __init__(self, feature_dim=256):
        super().__init__()
        self.feature_dim = feature_dim
        
        # ì´ë¯¸ì§€ íŒ¨ì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” CNN
        self.patch_encoder = nn.Conv2d(3, feature_dim, kernel_size=16, stride=16)
        
        # Attention mechanism
        self.attention = MultiHeadAttention(feature_dim, 8)
        
        # ê³µê°„ ìœ„ì¹˜ ì¸ì½”ë”©
        self.pos_encoding = nn.Parameter(torch.randn(1, 49, feature_dim))  # 7x7 íŒ¨ì¹˜
    
    def forward(self, image, task_query):
        """
        image: (batch, 3, 224, 224)
        task_query: (batch, feature_dim) - ìˆ˜í–‰í•  íƒœìŠ¤í¬ ì •ë³´
        """
        batch_size = image.size(0)
        
        # 1. ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë¶„í• í•˜ê³  ì¸ì½”ë”©
        patches = self.patch_encoder(image)  # (batch, feature_dim, 7, 7)
        patches = patches.flatten(2).transpose(1, 2)  # (batch, 49, feature_dim)
        
        # 2. ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        patches = patches + self.pos_encoding
        
        # 3. íƒœìŠ¤í¬ ê¸°ë°˜ ì–´í…ì…˜
        # Query: íƒœìŠ¤í¬, Key/Value: ì´ë¯¸ì§€ íŒ¨ì¹˜
        task_query = task_query.unsqueeze(1)  # (batch, 1, feature_dim)
        
        attended_features, attention_weights = self.attention(
            query=task_query,
            key=patches,
            value=patches
        )
        
        return attended_features.squeeze(1), attention_weights

# ì˜ˆì‹œ: "ë¹¨ê°„ ì»µì„ ì°¾ì•„ë¼" íƒœìŠ¤í¬
visual_attention = VisualAttention()

dummy_image = torch.randn(2, 3, 224, 224)
task_embedding = torch.randn(2, 256)  # "find red cup" ì„ë² ë”©

attended_visual, attention_map = visual_attention(dummy_image, task_embedding)

print(f"ì‹œê°ì  ì£¼ì˜ì§‘ì¤‘ ê²°ê³¼: {attended_visual.shape}")
print(f"ì£¼ì˜ì§‘ì¤‘ ì§€ë„: {attention_map.shape}")

# ì–´í…ì…˜ ë§µ ì‹œê°í™” (ê°œë…ì )
def visualize_attention_map(attention_weights):
    """
    ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ 7x7 ì´ë¯¸ì§€ë¡œ ì‹œê°í™”
    """
    # attention_weights: (batch, n_heads, 1, 49)
    avg_attention = attention_weights.mean(dim=1).squeeze(1)  # (batch, 49)
    attention_2d = avg_attention.view(-1, 7, 7)  # (batch, 7, 7)
    
    print(f"ì£¼ì˜ì§‘ì¤‘ ê°•ë„ (7x7 ê·¸ë¦¬ë“œ):")
    for i in range(attention_2d.size(0)):
        print(f"ìƒ˜í”Œ {i+1}:")
        print(attention_2d[i].detach().numpy())
        print()

visualize_attention_map(attention_map)
```

### 2. Language Attention (ëª…ë ¹ì–´ì—ì„œ ì¤‘ìš”í•œ ë‹¨ì–´ ì°¾ê¸°)
```python
class LanguageAttention(nn.Module):
    """
    ìì—°ì–´ ëª…ë ¹ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ ì°¾ê¸°
    """
    
    def __init__(self, vocab_size=10000, embed_dim=256):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.attention = MultiHeadAttention(embed_dim, 8)
        
        # ìœ„ì¹˜ ì¸ì½”ë”© (Transformer ìŠ¤íƒ€ì¼)
        self.pos_encoding = self.create_positional_encoding(100, embed_dim)
    
    def create_positional_encoding(self, max_len, d_model):
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        return nn.Parameter(pe.unsqueeze(0), requires_grad=False)
    
    def forward(self, tokens, current_state=None):
        """
        tokens: (batch, seq_len) - í† í°í™”ëœ ëª…ë ¹ì–´
        current_state: (batch, embed_dim) - í˜„ì¬ ë¡œë´‡ ìƒíƒœ (ì„ íƒì )
        """
        batch_size, seq_len = tokens.shape
        
        # 1. í† í° ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©
        embedded = self.embedding(tokens)  # (batch, seq_len, embed_dim)
        embedded = embedded + self.pos_encoding[:, :seq_len, :]
        
        if current_state is not None:
            # 2. í˜„ì¬ ìƒíƒœ ê¸°ë°˜ ì–´í…ì…˜ (ì–´ë–¤ ëª…ë ¹ì–´ ë¶€ë¶„ì´ í˜„ì¬ ìƒí™©ì— ê´€ë ¨ìˆëŠ”ì§€)
            state_query = current_state.unsqueeze(1)  # (batch, 1, embed_dim)
            
            attended_lang, attention_weights = self.attention(
                query=state_query,
                key=embedded,
                value=embedded
            )
            
            return attended_lang.squeeze(1), attention_weights
        else:
            # 3. Self-attention (ëª…ë ¹ì–´ ë‚´ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„)
            self_attended, attention_weights = self.attention(embedded, embedded, embedded)
            
            # í‰ê·  í’€ë§ìœ¼ë¡œ ë¬¸ì¥ ì „ì²´ í‘œí˜„
            sentence_repr = self_attended.mean(dim=1)
            
            return sentence_repr, attention_weights

# ì˜ˆì‹œ ì‚¬ìš©
lang_attention = LanguageAttention()

# "Pick up the red cup on the table"
instruction_tokens = torch.tensor([[45, 123, 67, 891, 234, 456, 67, 789]])
robot_state = torch.randn(1, 256)

# í˜„ì¬ ìƒíƒœ ê¸°ë°˜ ëª…ë ¹ì–´ ì´í•´
attended_instruction, lang_attention_weights = lang_attention(
    instruction_tokens, robot_state
)

print(f"ì£¼ì˜ì§‘ì¤‘ëœ ëª…ë ¹ì–´: {attended_instruction.shape}")
print(f"ì–¸ì–´ ì–´í…ì…˜ ê°€ì¤‘ì¹˜: {lang_attention_weights.shape}")

# ì–´ë–¤ ë‹¨ì–´ì— ì£¼ëª©í–ˆëŠ”ì§€ í™•ì¸
def analyze_language_attention(tokens, attention_weights):
    """
    ì–´ë–¤ ë‹¨ì–´ì— ë†’ì€ ì–´í…ì…˜ì„ ì¤¬ëŠ”ì§€ ë¶„ì„
    """
    # ë‹¨ì–´ë³„ ì–´í…ì…˜ ì ìˆ˜ (í‰ê· )
    word_scores = attention_weights.mean(dim=1).squeeze(1)  # (seq_len,)
    
    # ê°€ìƒì˜ ë‹¨ì–´ë“¤
    words = ["pick", "up", "the", "red", "cup", "on", "the", "table"]
    
    print("ë‹¨ì–´ë³„ ì–´í…ì…˜ ì ìˆ˜:")
    for word, score in zip(words, word_scores[0]):
        print(f"{word:8}: {score.item():.3f}")

analyze_language_attention(instruction_tokens, lang_attention_weights)
```

### 3. Cross-Modal Attention (Vision â†” Language)
```python
class CrossModalAttention(nn.Module):
    """
    ì‹œê° ì •ë³´ì™€ ì–¸ì–´ ì •ë³´ ê°„ì˜ ìƒí˜¸ì‘ìš©
    """
    
    def __init__(self, feature_dim=256):
        super().__init__()
        self.feature_dim = feature_dim
        
        # ê° ëª¨ë‹¬ë¦¬í‹° ì²˜ë¦¬
        self.vision_proj = nn.Linear(feature_dim, feature_dim)
        self.language_proj = nn.Linear(feature_dim, feature_dim)
        
        # Cross-attention layers
        self.vision_to_lang = MultiHeadAttention(feature_dim, 8)
        self.lang_to_vision = MultiHeadAttention(feature_dim, 8)
        
        # ìµœì¢… ìœµí•©
        self.fusion = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.ReLU(),
            nn.Linear(feature_dim, feature_dim)
        )
    
    def forward(self, visual_features, language_features):
        """
        visual_features: (batch, num_patches, feature_dim)
        language_features: (batch, seq_len, feature_dim)
        """
        # í”„ë¡œì ì…˜
        vis_proj = self.vision_proj(visual_features)
        lang_proj = self.language_proj(language_features)
        
        # 1. ì–¸ì–´ê°€ ì‹œê° ì •ë³´ ì°¸ì¡° (ì–¸ì–´ ê¸°ë°˜ìœ¼ë¡œ ì‹œê° ì •ë³´ ì„ íƒ)
        lang_attended_vis, _ = self.vision_to_lang(
            query=lang_proj,     # ì–¸ì–´ì—ì„œ ì§ˆë¬¸
            key=vis_proj,        # ì‹œê°ì—ì„œ ê²€ìƒ‰  
            value=vis_proj       # ì‹œê°ì—ì„œ ì •ë³´ ì¶”ì¶œ
        )
        
        # 2. ì‹œê°ì´ ì–¸ì–´ ì •ë³´ ì°¸ì¡° (ì‹œê° ê¸°ë°˜ìœ¼ë¡œ ì–¸ì–´ ì •ë³´ ì„ íƒ)
        vis_attended_lang, _ = self.lang_to_vision(
            query=vis_proj,      # ì‹œê°ì—ì„œ ì§ˆë¬¸
            key=lang_proj,       # ì–¸ì–´ì—ì„œ ê²€ìƒ‰
            value=lang_proj      # ì–¸ì–´ì—ì„œ ì •ë³´ ì¶”ì¶œ  
        )
        
        # 3. ì •ë³´ í†µí•© (í‰ê·  í’€ë§ í›„ ìœµí•©)
        fused_vis_lang = lang_attended_vis.mean(dim=1)  # (batch, feature_dim)
        fused_lang_vis = vis_attended_lang.mean(dim=1)  # (batch, feature_dim)
        
        # 4. ìµœì¢… ìœµí•©ëœ í‘œí˜„
        combined = torch.cat([fused_vis_lang, fused_lang_vis], dim=1)
        final_features = self.fusion(combined)
        
        return final_features

# ì™„ì „í•œ VLA ëª¨ë¸ with Attention
class AttentionBasedVLA(nn.Module):
    """
    ì–´í…ì…˜ì„ í™œìš©í•œ ì™„ì „í•œ VLA ëª¨ë¸
    """
    
    def __init__(self, feature_dim=256, action_dim=7):
        super().__init__()
        
        # ê° ëª¨ë‹¬ë¦¬í‹° ì¸ì½”ë”
        self.visual_attention = VisualAttention(feature_dim)
        self.language_attention = LanguageAttention(embed_dim=feature_dim)
        
        # í¬ë¡œìŠ¤ ëª¨ë‹¬ ìœµí•©
        self.cross_modal = CrossModalAttention(feature_dim)
        
        # ì•¡ì…˜ ë””ì½”ë”
        self.action_decoder = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, images, instructions):
        batch_size = images.size(0)
        
        # 1. ì–¸ì–´ ì²˜ë¦¬ (ëª…ë ¹ì–´ ì´í•´)
        lang_features, _ = self.language_attention(instructions)
        lang_features = lang_features.unsqueeze(1)  # (batch, 1, feature_dim)
        
        # 2. ì‹œê° ì²˜ë¦¬ (íƒœìŠ¤í¬ ê¸°ë°˜ ì‹œê° ì£¼ì˜ì§‘ì¤‘)
        vis_features, attention_map = self.visual_attention(images, lang_features.squeeze(1))
        vis_features = vis_features.unsqueeze(1)  # (batch, 1, feature_dim)
        
        # 3. í¬ë¡œìŠ¤ ëª¨ë‹¬ ìœµí•©
        fused_features = self.cross_modal(vis_features, lang_features)
        
        # 4. ì•¡ì…˜ ì˜ˆì¸¡
        actions = self.action_decoder(fused_features)
        
        return actions, attention_map

# ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
attention_vla = AttentionBasedVLA()

dummy_images = torch.randn(2, 3, 224, 224)
dummy_instructions = torch.randint(0, 10000, (2, 8))

predicted_actions, attention_maps = attention_vla(dummy_images, dummy_instructions)

print(f"Attention ê¸°ë°˜ VLA ì¶œë ¥:")
print(f"- ì˜ˆì¸¡ ì•¡ì…˜: {predicted_actions.shape}")
print(f"- ì‹œê° ì–´í…ì…˜ ë§µ: {attention_maps.shape}")
print(f"- ì•¡ì…˜ ê°’: {predicted_actions}")
```

---

## ğŸ”¬ í•µì‹¬ ê°œë… ì •ë¦¬

### 1. Attentionì˜ í•µì‹¬ ì•„ì´ë””ì–´
```python
attention_key_concepts = {
    "ì„ íƒì _ì§‘ì¤‘": "ëª¨ë“  ì •ë³´ë¥¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘",
    "ë™ì _ê°€ì¤‘ì¹˜": "ìƒí™©ì— ë”°ë¼ ì–´ëŠ ë¶€ë¶„ì´ ì¤‘ìš”í•œì§€ ë™ì ìœ¼ë¡œ ê²°ì •",
    "ìœ ì—°í•œ_ì •ë³´ê²°í•©": "ì„œë¡œ ë‹¤ë¥¸ ì†ŒìŠ¤ì˜ ì •ë³´ë¥¼ ìœ ì—°í•˜ê²Œ ê²°í•©",
    "í•´ì„ê°€ëŠ¥ì„±": "ì–´ëŠ ë¶€ë¶„ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì˜€ëŠ”ì§€ ì‹œê°í™” ê°€ëŠ¥"
}
```

### 2. VLAì—ì„œ Attentionì˜ ì—­í• 
```python
vla_attention_roles = {
    "ì‹œê°_ì£¼ì˜ì§‘ì¤‘": "ì´ë¯¸ì§€ì—ì„œ íƒœìŠ¤í¬ ê´€ë ¨ ì˜ì—­ ì°¾ê¸°",
    "ì–¸ì–´_ì´í•´": "ëª…ë ¹ì–´ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì‹ë³„",
    "ì‹œê³µê°„_ì¶”ë¡ ": "ì—¬ëŸ¬ ì‹œì ì˜ ì •ë³´ë¥¼ ì—°ê²°í•´ì„œ ì´í•´",
    "ëª¨ë‹¬ë¦¬í‹°_ìœµí•©": "ë¹„ì „ê³¼ ì–¸ì–´ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•©"
}
```

### 3. Attention vs ê¸°ì¡´ ë°©ë²•
```python
attention_vs_traditional = {
    "ê¸°ì¡´_CNN": {
        "ë°©ì‹": "ê³ ì •ëœ í•„í„°ë¡œ ëª¨ë“  ìœ„ì¹˜ ë™ì¼ ì²˜ë¦¬",
        "í•œê³„": "ìƒí™©ì— ë”°ë¥¸ ì ì‘ì  ì²˜ë¦¬ ë¶ˆê°€"
    },
    
    "ê¸°ì¡´_RNN": {
        "ë°©ì‹": "ìˆœì°¨ì ìœ¼ë¡œ ì •ë³´ ì²˜ë¦¬",
        "í•œê³„": "ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ì •ë³´ ì†ì‹¤"
    },
    
    "Attention": {
        "ë°©ì‹": "ìƒí™©ì— ë§ì¶° ì¤‘ìš”ë„ ë™ì  ê³„ì‚°",
        "ì¥ì ": "ê¸´ ì‹œí€€ìŠ¤, ë³µì¡í•œ ê´€ê³„ íš¨ê³¼ì  ì²˜ë¦¬"
    }
}
```

---

## ğŸ› ï¸ ì‹¤ìŠµ: Attention ì‹œê°í™”

### Attention ê°€ì¤‘ì¹˜ ì‹œê°í™”
```python
import matplotlib.pyplot as plt
import numpy as np

def visualize_attention_weights(attention_weights, words=None, save_path=None):
    """
    Attention ê°€ì¤‘ì¹˜ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”
    """
    # attention_weights: (seq_len, seq_len) ë˜ëŠ” (n_heads, seq_len, seq_len)
    
    if attention_weights.dim() == 3:
        # Multi-headì¸ ê²½ìš° í‰ê·  ê³„ì‚°
        attention_weights = attention_weights.mean(dim=0)
    
    attention_np = attention_weights.detach().numpy()
    
    plt.figure(figsize=(10, 8))
    plt.imshow(attention_np, cmap='Blues', interpolation='nearest')
    plt.colorbar()
    
    if words:
        plt.xticks(range(len(words)), words, rotation=45)
        plt.yticks(range(len(words)), words)
        plt.xlabel('Key (ì°¸ì¡°ë˜ëŠ” ë‹¨ì–´)')
        plt.ylabel('Query (ì§ˆë¬¸í•˜ëŠ” ë‹¨ì–´)')
    
    plt.title('Attention Weights Visualization')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

# ì˜ˆì‹œ ë°ì´í„°ë¡œ ì‹œê°í™”
def demo_attention_visualization():
    # ê°„ë‹¨í•œ self-attention ì˜ˆì‹œ
    sentence = ["The", "robot", "picks", "up", "red", "cup"]
    seq_len = len(sentence)
    
    # ë”ë¯¸ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ìƒì„± (ì‹¤ì œë¡œëŠ” ëª¨ë¸ì—ì„œ ë‚˜ì˜´)
    attention_weights = torch.softmax(torch.randn(seq_len, seq_len), dim=-1)
    
    print("ë¬¸ì¥:", " ".join(sentence))
    print("Attention ê°€ì¤‘ì¹˜ ì‹œê°í™”:")
    
    visualize_attention_weights(attention_weights, sentence)

# ì‹¤í–‰
demo_attention_visualization()
```

---

## ğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„

Attentionì„ ì´í•´í–ˆë‹¤ë©´:

1. **Transformer Architecture** (`03_transformer_architecture.md`) - Attentionì˜ ì™„ì „í•œ í™œìš©
2. **Multi-Modal Learning** (`04_multimodal_learning.md`) - Cross-modal attention ì‹¬í™”
3. **Vision Transformers** (`05_vision_encoders.md`) - ì´ë¯¸ì§€ì—ì„œ Attention í™œìš©

### ì¶”ì²œ ì‹¤ìŠµ
```python
recommended_practice = {
    "ê¸°ì´ˆ": "ê°„ë‹¨í•œ attention ë©”ì»¤ë‹ˆì¦˜ ì§ì ‘ êµ¬í˜„",
    "ì¤‘ê¸‰": "Multi-head attentionìœ¼ë¡œ ê°„ë‹¨í•œ ë²ˆì—­ ëª¨ë¸",
    "ê³ ê¸‰": "Vision-Language attentionìœ¼ë¡œ ì´ë¯¸ì§€ ìº¡ì…”ë‹"
}
```

---

## ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸

### ê¸°ì–µí•´ì•¼ í•  ê²ƒ
1. **Query, Key, Value**: Attentionì˜ 3ìš”ì†Œ, ê°ê°ì˜ ì—­í•  ëª…í™•íˆ ì´í•´
2. **Softmaxì˜ ì¤‘ìš”ì„±**: í™•ë¥  ë¶„í¬ë¡œ ë§Œë“¤ì–´ì„œ í•´ì„ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ ìƒì„±
3. **Multi-Headì˜ ì¥ì **: ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì •ë³´ ìˆ˜ì§‘
4. **Self vs Cross Attention**: ì–¸ì œ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í• ì§€ íŒë‹¨ ì¤‘ìš”

### VLA ì—°êµ¬ì—ì„œì˜ ì˜ë¯¸
- **í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜**: í˜„ëŒ€ VLA ëª¨ë¸ì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ
- **ëª¨ë‹¬ë¦¬í‹° ê²°í•©**: Visionê³¼ Languageë¥¼ ì—°ê²°í•˜ëŠ” í•µì‹¬ ë„êµ¬
- **í•´ì„ê°€ëŠ¥ì„±**: ëª¨ë¸ì´ ì™œ ê·¸ëŸ° ê²°ì •ì„ ë‚´ë ¸ëŠ”ì§€ ì´í•´ ê°€ëŠ¥
- **ì„±ëŠ¥ í–¥ìƒ**: ê´€ë ¨ ì—†ëŠ” ì •ë³´ ë¬´ì‹œ, ì¤‘ìš”í•œ ì •ë³´ì— ì§‘ì¤‘

**ë‹¤ìŒ: Transformer ì•„í‚¤í…ì²˜ë¡œ!** ğŸš€

---

*Created: 2025-08-24*  
*Time: 2-3 hours*  
*Next: 03_transformer_architecture.md*