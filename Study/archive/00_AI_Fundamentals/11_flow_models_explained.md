# ğŸŒŠ Flow Models ìƒì„¸ ì„¤ëª…

## ğŸ“Œ ê°œìš”
Flow Modelì€ ë‹¨ìˆœí•œ í™•ë¥  ë¶„í¬(ì˜ˆ: ê°€ìš°ì‹œì•ˆ)ë¥¼ ë³µì¡í•œ ë°ì´í„° ë¶„í¬ë¡œ ë³€í™˜í•˜ëŠ” ê°€ì—­ì (invertible) ë³€í™˜ì„ í•™ìŠµí•˜ëŠ” ìƒì„± ëª¨ë¸ì…ë‹ˆë‹¤. VLAì—ì„œëŠ” ì—°ì†ì ì´ê³  ë¶€ë“œëŸ¬ìš´ ë¡œë´‡ ë™ì‘ì„ ìƒì„±í•˜ëŠ” ë° í™œìš©ë˜ë©°, íŠ¹íˆ Ï€â‚€(Pi-Zero) ê°™ì€ ìµœì‹  ë¡œë´‡ ì •ì±…ì—ì„œ í•µì‹¬ ì—­í• ì„ í•©ë‹ˆë‹¤.

## ğŸ¯ í•µì‹¬ ê°œë…

### 1. Flow Modelì˜ ê¸°ë³¸ ì›ë¦¬

#### ë³€í™˜ì˜ íë¦„
```
Simple Distribution â†’ Invertible Transform â†’ Complex Distribution
z ~ N(0,I) â†’ f(z; Î¸) â†’ x ~ p_data
```

#### í•µì‹¬ íŠ¹ì„±
| íŠ¹ì„± | ì„¤ëª… | ì¥ì  |
|------|------|------|
| **Invertibility** | fì™€ fâ»Â¹ ëª¨ë‘ ê³„ì‚° ê°€ëŠ¥ | ì–‘ë°©í–¥ ë³€í™˜ |
| **Exact Likelihood** | log p(x) ì •í™•íˆ ê³„ì‚° | ì •í™•í•œ ë°€ë„ ì¶”ì • |
| **Continuous Transform** | ì—°ì†ì ì¸ ë³€í™˜ | ë¶€ë“œëŸ¬ìš´ ìƒì„± |
| **Tractable Jacobian** | det(âˆ‚f/âˆ‚z) ê³„ì‚° ê°€ëŠ¥ | í•™ìŠµ ê°€ëŠ¥ |

### 2. Change of Variables Formula

#### í™•ë¥  ë°€ë„ ë³€í™˜
```
p_x(x) = p_z(fâ»Â¹(x)) |det(âˆ‚fâ»Â¹/âˆ‚x)|
ë˜ëŠ”
log p_x(x) = log p_z(z) - log |det(âˆ‚f/âˆ‚z)|
```

ì—¬ê¸°ì„œ:
- p_z: ê¸°ë³¸ ë¶„í¬ (Base distribution)
- p_x: ëª©í‘œ ë¶„í¬ (Target distribution)
- Jacobian determinant: ë¶€í”¼ ë³€í™”ìœ¨

### 3. Flow ìœ í˜•

#### Normalizing Flow
ì—°ì†ëœ ê°€ì—­ ë³€í™˜ì˜ í•©ì„±:
```
zâ‚€ â†’ fâ‚ â†’ zâ‚ â†’ fâ‚‚ â†’ ... â†’ fâ‚– â†’ x
log p(x) = log p(zâ‚€) - Î£áµ¢ log |det(âˆ‚fáµ¢/âˆ‚záµ¢â‚‹â‚)|
```

#### Continuous Normalizing Flow (CNF)
ì—°ì† ì‹œê°„ ë™ì—­í•™:
```
dz/dt = f(z,t)
Neural ODEë¡œ êµ¬í˜„
```

#### Flow Matching
ìµœì  ìš´ì†¡ ì´ë¡  ê¸°ë°˜:
```
Interpolation: x_t = (1-t)xâ‚€ + txâ‚
Velocity: v(x_t,t) = xâ‚ - xâ‚€
```

## ğŸ—ï¸ ì£¼ìš” ì•„í‚¤í…ì²˜ ìƒì„¸

### 1. Coupling Layer

#### Affine Coupling
```python
# Inputì„ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë¶„í• 
x = [x_a, x_b]

# x_aëŠ” ê·¸ëŒ€ë¡œ, x_bë§Œ ë³€í™˜
y_a = x_a
y_b = x_b âŠ™ exp(s(x_a)) + t(x_a)

# s: scale network, t: translation network
```

**ì¥ì :**
- Jacobianì´ ì‚¼ê° í–‰ë ¬
- Determinant ê³„ì‚° ì‰¬ì›€: Î£ s(x_a)
- ì—­ë³€í™˜ ê°„ë‹¨

#### Additive Coupling
```python
y_a = x_a
y_b = x_b + t(x_a)
# Jacobian determinant = 1 (volume-preserving)
```

### 2. Autoregressive Flow

#### MAF (Masked Autoregressive Flow)
```python
# ê° ì°¨ì›ì´ ì´ì „ ì°¨ì›ë“¤ì—ë§Œ ì˜ì¡´
x_i' = x_i * exp(s_i(x_<i)) + t_i(x_<i)
```

**íŠ¹ì§•:**
- ë¹ ë¥¸ ë°€ë„ í‰ê°€
- ëŠë¦° ìƒ˜í”Œë§ (ìˆœì°¨ì )
- MADE ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©

#### IAF (Inverse Autoregressive Flow)
```python
# ì—­ë°©í–¥ autoregressive
z_i = (x_i - t_i(z_<i)) / exp(s_i(z_<i))
```

**íŠ¹ì§•:**
- ëŠë¦° ë°€ë„ í‰ê°€
- ë¹ ë¥¸ ìƒ˜í”Œë§ (ë³‘ë ¬)
- ìƒì„±ì— ì í•©

### 3. Residual Flow

#### Residual Connection
```python
y = x + g(x)
# gëŠ” Lipschitz ì œì•½ ë§Œì¡±
```

#### Invertibility ì¡°ê±´
```
||g||_Lip < 1
Banach fixed-point theoremìœ¼ë¡œ ì—­í•¨ìˆ˜ ì¡´ì¬ ë³´ì¥
```

### 4. Neural ODE/SDE

#### Neural ODE
```python
dx/dt = f_Î¸(x,t)
x(T) = x(0) + âˆ«â‚€áµ€ f_Î¸(x,t) dt
```

#### Neural SDE
```python
dx = f(x,t)dt + g(x,t)dW
í™•ë¥ ì  ë¯¸ë¶„ ë°©ì •ì‹
```

## ğŸ¤– VLAì—ì„œì˜ Flow Models

### 1. Ï€â‚€ (Pi-Zero) Architecture

#### Flow Matching for Actions
```python
class PiZeroFlow:
    def __init__(self):
        self.velocity_net = TransformerBackbone()
        self.condition_encoder = VisionLanguageEncoder()
    
    def train(self, noise, expert_action, condition):
        # Optimal transport path
        t = uniform(0, 1)
        x_t = (1-t) * noise + t * expert_action
        
        # Target velocity
        v_target = expert_action - noise
        
        # Predicted velocity
        v_pred = self.velocity_net(x_t, t, condition)
        
        # Loss
        loss = MSE(v_pred, v_target)
```

#### Generation Process
```python
def generate_action(condition):
    # Start from noise
    x = randn(action_dim)
    
    # ODE integration
    for t in linspace(0, 1, steps):
        v = velocity_net(x, t, condition)
        x = x + v * dt
    
    return x
```

### 2. Hierarchical Flow for Complex Actions

#### Multi-Scale Flow
```python
class MultiScaleFlow:
    def __init__(self):
        # Coarse flow: overall trajectory
        self.coarse_flow = FlowModel(output_dim=32)
        
        # Fine flow: detailed actions
        self.fine_flow = FlowModel(
            condition_dim=32,
            output_dim=action_dim
        )
    
    def generate(self, condition):
        # Generate coarse plan
        coarse = self.coarse_flow.sample(condition)
        
        # Refine to detailed actions
        actions = self.fine_flow.sample(coarse)
        
        return actions
```

### 3. Conditional Flow for Task Adaptation

#### Task-Conditioned Generation
```python
class TaskConditionedFlow:
    def __init__(self):
        self.task_encoder = TaskEncoder()
        self.flow = ConditionalFlow()
    
    def adapt_to_task(self, task_description, observation):
        # Encode task
        task_embedding = self.task_encoder(task_description)
        
        # Condition flow on task
        action = self.flow.generate(
            condition=concat(task_embedding, observation)
        )
        
        return action
```

## ğŸ”¬ ê³ ê¸‰ ê¸°ë²•

### 1. Efficient Jacobian Computation

#### Hutchinson's Trace Estimator
```python
def hutchinson_trace(f, x):
    """Stochastic trace estimation"""
    eps = randn_like(x)
    with autograd:
        y = f(x)
        vjp = grad(y, x, grad_outputs=eps)
    trace = (vjp * eps).sum()
    return trace
```

#### Russian Roulette Estimator
```python
def russian_roulette_estimator(f, x, p=0.5):
    """Unbiased infinite series estimator"""
    if random() < p:
        return 0
    else:
        return trace(f, x) / (1 - p)
```

### 2. Training Techniques

#### Maximum Likelihood Training
```python
def ml_loss(flow, x):
    z, log_det = flow.inverse(x)
    log_pz = normal_log_prob(z)
    log_px = log_pz + log_det
    return -log_px.mean()
```

#### Variational Training
```python
def variational_loss(flow, x, beta=1.0):
    z, log_det = flow.inverse(x)
    
    # Reconstruction
    x_recon, _ = flow.forward(z)
    recon_loss = MSE(x, x_recon)
    
    # KL divergence
    kl_loss = -0.5 * (1 + log_det - z.pow(2)).mean()
    
    return recon_loss + beta * kl_loss
```

#### Adversarial Training
```python
def adversarial_loss(flow, discriminator, real_x):
    # Generate fake samples
    z = randn(batch_size, dim)
    fake_x, _ = flow.forward(z)
    
    # Discriminator loss
    d_real = discriminator(real_x)
    d_fake = discriminator(fake_x)
    d_loss = -log(d_real) - log(1 - d_fake)
    
    # Generator (flow) loss
    g_loss = -log(d_fake)
    
    return d_loss, g_loss
```

### 3. Architectural Improvements

#### Glow Architecture
```python
class GlowBlock:
    def __init__(self):
        self.actnorm = ActNorm()
        self.invconv = Invertible1x1Conv()
        self.coupling = AffineCoupling()
    
    def forward(self, x):
        x, log_det1 = self.actnorm(x)
        x, log_det2 = self.invconv(x)
        x, log_det3 = self.coupling(x)
        return x, log_det1 + log_det2 + log_det3
```

#### FFJORD (Free-form Jacobian of Reversible Dynamics)
```python
class FFJORD:
    def __init__(self):
        self.ode_func = CNF()
    
    def forward(self, x, t0=0, t1=1):
        # Augment with log-det
        augmented = concat([x, zeros(batch, 1)])
        
        # Solve ODE
        solution = odeint(self.ode_func, augmented, [t0, t1])
        
        # Extract result and log-det
        y = solution[..., :-1]
        log_det = solution[..., -1]
        
        return y, log_det
```

## ğŸ’¡ ì‹¤ì „ ìµœì í™” ê°€ì´ë“œ

### 1. ì•„í‚¤í…ì²˜ ì„ íƒ

| ìš”êµ¬ì‚¬í•­ | ì¶”ì²œ ì•„í‚¤í…ì²˜ | ì´ìœ  |
|---------|--------------|------|
| ë¹ ë¥¸ ìƒ˜í”Œë§ | IAF, Flow Matching | ë³‘ë ¬ ìƒì„± |
| ì •í™•í•œ ë°€ë„ | MAF, RealNVP | ì •í™•í•œ likelihood |
| ìœ ì—°ì„± | Neural ODE | ì—°ì† ì‹œê°„ |
| íš¨ìœ¨ì„± | Coupling Layers | ê³„ì‚° íš¨ìœ¨ |

### 2. í•™ìŠµ ì•ˆì •í™”

#### Gradient Clipping
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

#### Learning Rate Scheduling
```python
scheduler = CosineAnnealingLR(optimizer, T_max=1000)
# ë˜ëŠ”
scheduler = OneCycleLR(optimizer, max_lr=1e-3, total_steps=10000)
```

#### Spectral Normalization
```python
def spectral_norm(module):
    return torch.nn.utils.spectral_norm(module)
```

### 3. ì„±ëŠ¥ ìµœì í™”

#### Checkpointing
```python
def checkpoint_forward(model, x):
    """Memory-efficient forward pass"""
    return torch.utils.checkpoint.checkpoint(model, x)
```

#### Mixed Precision
```python
with torch.cuda.amp.autocast():
    output = model(input)
scaler.scale(loss).backward()
```

## ğŸš€ ìµœì‹  ì—°êµ¬ ë™í–¥

### 1. Diffusion vs Flow
- **Diffusion**: ë…¸ì´ì¦ˆ ì œê±° ê³¼ì •
- **Flow**: ì§ì ‘ ë³€í™˜
- **Bridge**: ë‘ ë°©ë²• í†µí•©

### 2. Score-Based Models
- Score matchingê³¼ flow ê²°í•©
- SDE ê¸°ë°˜ ìƒì„±
- Consistency models

### 3. Applications
- **Rectified Flow**: ì§ì„  ê²½ë¡œ í•™ìŠµ
- **Stochastic Interpolants**: í™•ë¥ ì  ë³´ê°„
- **Optimal Transport Flow**: ìµœì  ìš´ì†¡

## âš ï¸ ì£¼ì˜ì‚¬í•­ ë° í•œê³„

### ì£¼ìš” ë¬¸ì œì 
1. **Computational Cost**: ë†’ì€ ê³„ì‚° ë¹„ìš©
2. **Architecture Constraints**: ê°€ì—­ì„± ì œì•½
3. **Memory Requirements**: í° ë©”ëª¨ë¦¬ ì‚¬ìš©
4. **Training Instability**: í•™ìŠµ ë¶ˆì•ˆì •ì„±

### í•´ê²° ë°©ì•ˆ
1. **Efficient Architectures**: íš¨ìœ¨ì  êµ¬ì¡° ì„¤ê³„
2. **Approximation Methods**: ê·¼ì‚¬ ë°©ë²• ì‚¬ìš©
3. **Regularization**: ì •ê·œí™” ê¸°ë²•
4. **Hybrid Approaches**: ë‹¤ë¥¸ ë°©ë²•ê³¼ ê²°í•©

## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ

### í•µì‹¬ ë…¼ë¬¸
- "Normalizing Flows for Probabilistic Modeling and Inference"
- "Neural Ordinary Differential Equations"
- "Flow Matching for Generative Modeling"
- "Ï€â‚€: A Vision-Language-Action Flow Model"

### êµ¬í˜„ ë¼ì´ë¸ŒëŸ¬ë¦¬
- normflows: PyTorch normalizing flows
- torchdiffeq: Neural ODE
- zuko: Flow implementations

### ì‘ìš© ë¶„ì•¼
- ë¡œë´‡ ì œì–´
- ë¶„ì ìƒì„±
- ì´ë¯¸ì§€ ìƒì„±
- ìŒì„± í•©ì„±

## ğŸ¯ í•µì‹¬ ìš”ì•½

Flow Modelì€ ê°€ì—­ ë³€í™˜ì„ í†µí•´ ë‹¨ìˆœí•œ ë¶„í¬ë¥¼ ë³µì¡í•œ ë¶„í¬ë¡œ ë§¤í•‘í•˜ëŠ” ê°•ë ¥í•œ ìƒì„± ëª¨ë¸ì…ë‹ˆë‹¤. ì •í™•í•œ likelihood ê³„ì‚°, ì–‘ë°©í–¥ ë³€í™˜, ì—°ì†ì  ìƒì„±ì´ ê°€ëŠ¥í•˜ì—¬ VLAì—ì„œ ë¶€ë“œëŸ½ê³  ì •ë°€í•œ ë¡œë´‡ ë™ì‘ ìƒì„±ì— ì´ìƒì ì…ë‹ˆë‹¤. íŠ¹íˆ Flow Matchingê³¼ ê°™ì€ ìµœì‹  ê¸°ë²•ì€ í•™ìŠµ ì•ˆì •ì„±ê³¼ ìƒì„± í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œì¼œ ì‹¤ì œ ë¡œë´‡ ì‘ìš©ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.