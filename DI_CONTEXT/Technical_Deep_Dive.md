# ğŸ”¬ ê¸°ìˆ ì  ì‹¬í™” ë‚´ìš© - êµìˆ˜ë‹˜ ì§ˆë¬¸ ëŒ€ë¹„
## Selective RAG êµ¬í˜„ì˜ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­

---

## ğŸ“ **Confidence Estimation êµ¬í˜„ ë°©ë²•**

### **ë°©ë²• 1: Ensemble ê¸°ë°˜**
```python
class EnsembleConfidence:
    def __init__(self, n_models=3):
        self.models = [VLAModel() for _ in range(n_models)]
    
    def estimate_confidence(self, observation):
        predictions = [m(observation) for m in self.models]
        
        # ë¶„ì‚°ì´ í¬ë©´ ë¶ˆí™•ì‹¤
        variance = np.var(predictions, axis=0)
        confidence = 1.0 - normalize(variance)
        
        return confidence
```

### **ë°©ë²• 2: MC Dropout**
```python
class MCDropoutConfidence:
    def estimate_confidence(self, observation, n_samples=10):
        # Training modeë¡œ ì „í™˜ (dropout í™œì„±í™”)
        self.model.train()
        
        predictions = []
        for _ in range(n_samples):
            pred = self.model(observation)
            predictions.append(pred)
        
        # ì˜ˆì¸¡ì˜ ì¼ê´€ì„± = í™•ì‹ ë„
        uncertainty = np.std(predictions)
        confidence = 1.0 - uncertainty
        
        return confidence
```

### **ë°©ë²• 3: Output Distribution ë¶„ì„**
```python
class DistributionConfidence:
    def estimate_confidence(self, observation):
        logits = self.model.get_logits(observation)
        probs = F.softmax(logits, dim=-1)
        
        # ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ìœ¼ë©´ í™•ì‹¤
        entropy = -torch.sum(probs * torch.log(probs))
        confidence = 1.0 - (entropy / max_entropy)
        
        return confidence
```

---

## ğŸ” **RAG ê²€ìƒ‰ ìµœì í™” ì „ëµ**

### **1. ë³‘ë ¬ ì²˜ë¦¬ êµ¬ì¡°**
```python
async def parallel_inference(observation):
    # ë™ì‹œì— ì‹œì‘
    action_future = asyncio.create_task(
        generate_action(observation)
    )
    memory_future = asyncio.create_task(
        search_memory_if_needed(observation)
    )
    
    # ë¨¼ì € ì•¡ì…˜ ë°›ê¸°
    action = await action_future
    confidence = estimate_confidence(action)
    
    if confidence < 0.7:
        # ë©”ëª¨ë¦¬ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë‹¤ë¦¬ê¸°
        memory = await memory_future
        action = refine_with_memory(action, memory)
    
    return action  # ëŒ€ë¶€ë¶„ ê²½ìš° ë¹ ë¥´ê²Œ ë¦¬í„´
```

### **2. ê³„ì¸µì  ë©”ëª¨ë¦¬ êµ¬ì¡°**
```python
class HierarchicalMemory:
    def __init__(self):
        self.cache = {}  # L1: ìì£¼ ì“°ëŠ” ê²ƒ
        self.recent = deque(maxlen=100)  # L2: ìµœê·¼ ê²ƒ
        self.database = VectorDB()  # L3: ì „ì²´
    
    def search(self, query, confidence):
        # Confidence ë‚®ì„ìˆ˜ë¡ ê¹Šê²Œ ê²€ìƒ‰
        if confidence > 0.5:
            return self.cache.get(query)
        elif confidence > 0.3:
            return self.search_recent(query)
        else:
            return self.search_all(query)
```

---

## ğŸ“Š **ì„±ëŠ¥ ìµœì í™” ê¸°ë²•**

### **1. Dynamic Batching**
```python
class DynamicBatcher:
    def process(self, requests):
        # Confidenceë³„ë¡œ ê·¸ë£¹í™”
        high_conf = [r for r in requests if r.conf > 0.7]
        low_conf = [r for r in requests if r.conf <= 0.7]
        
        # ê³ ì‹ ë¢°ë„ëŠ” ë°”ë¡œ ì²˜ë¦¬
        fast_results = batch_process(high_conf)
        
        # ì €ì‹ ë¢°ë„ëŠ” RAG í¬í•¨ ì²˜ë¦¬
        slow_results = batch_process_with_rag(low_conf)
        
        return merge(fast_results, slow_results)
```

### **2. Adaptive Threshold**
```python
class AdaptiveThreshold:
    def __init__(self):
        self.threshold = 0.7
        self.performance_history = []
    
    def update(self, was_correct, confidence):
        # í‹€ë ¸ëŠ”ë° confidence ë†’ì•˜ìœ¼ë©´ threshold ì˜¬ë¦¬ê¸°
        if not was_correct and confidence > self.threshold:
            self.threshold += 0.05
        
        # ë§ì•˜ëŠ”ë° RAG ì¼ìœ¼ë©´ threshold ë‚®ì¶”ê¸°
        elif was_correct and confidence < self.threshold:
            self.threshold -= 0.05
        
        self.threshold = np.clip(self.threshold, 0.3, 0.9)
```

---

## ğŸ§ª **ì‹¤í—˜ ì„¤ê³„**

### **1. Baseline ë¹„êµ**
```python
baselines = {
    "No_RAG": "Ï€â‚€ ìŠ¤íƒ€ì¼ (ë¹ ë¥´ì§€ë§Œ í•™ìŠµ ì—†ìŒ)",
    "Always_RAG": "ELLMER ìŠ¤íƒ€ì¼ (ëŠë¦¬ì§€ë§Œ ì •í™•)",
    "Random_RAG": "ëœë¤í•˜ê²Œ 50% ê²€ìƒ‰",
    "Ours": "Confidence ê¸°ë°˜ ì„ íƒì "
}

metrics = {
    "speed": "Hz (ì¶”ë¡  ì†ë„)",
    "accuracy": "ì„±ê³µë¥  %",
    "failure_repeat": "ê°™ì€ ì‹¤íŒ¨ ë°˜ë³µ íšŸìˆ˜",
    "memory_usage": "MB"
}
```

### **2. Ablation Study**
```python
ablation = {
    "confidence_ë°©ë²•ë³„": [
        "Ensemble",
        "MC Dropout",
        "Distribution"
    ],
    "thresholdë³„": [0.5, 0.6, 0.7, 0.8, 0.9],
    "memory_sizeë³„": ["10MB", "100MB", "1GB"]
}
```

---

## ğŸ’¾ **ë©”ëª¨ë¦¬ ê´€ë¦¬ ì „ëµ**

### **Failure-Centric Storage**
```python
class FailureMemory:
    def should_store(self, episode):
        # ì‹¤íŒ¨ë§Œ ì €ì¥
        if episode.success:
            return False
        
        # ìƒˆë¡œìš´ ì‹¤íŒ¨ íŒ¨í„´ì¸ì§€ í™•ì¸
        if self.is_novel_failure(episode):
            return True
        
        # ë°˜ë³µë˜ëŠ” ì‹¤íŒ¨ë©´ ì¹´ìš´íŠ¸ë§Œ ì¦ê°€
        self.increment_failure_count(episode)
        return False
    
    def compress(self, episode):
        # í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ
        return {
            "state": episode.critical_state,
            "action": episode.failed_action,
            "lesson": extract_lesson(episode)
        }
```

---

## ğŸ“ **ì´ë¡ ì  ë°°ê²½**

### **Information Theory ê´€ì **
```
H(action|observation) = Uncertainty

If H > threshold:
    I(action; memory) = Information gain from memory
    Use RAG when I > cost(retrieval)
```

### **Decision Theory ê´€ì **
```
Expected Utility = P(success|no_RAG) Ã— U(fast) 
                  + P(success|RAG) Ã— U(accurate)

Optimize threshold to maximize EU
```

---

## ğŸ”— **ê´€ë ¨ ì—°êµ¬ ì—°ê²°**

### **Uncertainty in Deep Learning**
- Gal & Ghahramani (2016): Dropout as Bayesian Approximation
- Lakshminarayanan (2017): Simple and Scalable Uncertainty

### **Selective Computation**
- Graves (2016): Adaptive Computation Time
- Shazeer (2017): Mixture of Experts

### **Memory in RL**
- Pritzel (2017): Neural Episodic Control
- Fortunato (2019): Generalization in RL with Memory

---

## ğŸš¨ **ì˜ˆìƒë˜ëŠ” ê¸°ìˆ ì  ì±Œë¦°ì§€**

### **1. Confidence Calibration**
```python
ë¬¸ì œ = "ëª¨ë¸ì´ ê³¼ì‹ í•  ìˆ˜ ìˆìŒ"
í•´ê²° = "Temperature scaling, Platt scaling"
```

### **2. Distribution Shift**
```python
ë¬¸ì œ = "í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í¬ ì°¨ì´"
í•´ê²° = "Online adaptation, Continual learning"
```

### **3. Memory Explosion**
```python
ë¬¸ì œ = "ë©”ëª¨ë¦¬ ê³„ì† ì¦ê°€"
í•´ê²° = "Forgetting mechanism, Importance sampling"
```

---

## ğŸ’¬ **ê¹Šì€ ê¸°ìˆ  ì§ˆë¬¸ ëŒ€ë¹„**

### **Q: "Gradientê°€ ì–´ë–»ê²Œ íë¥´ë‚˜ìš”?"**
```
Confidence estimatorëŠ” ë³„ë„ ëª¨ë“ˆë¡œ í›ˆë ¨
Main policyëŠ” RL/ILë¡œ í›ˆë ¨
RAGëŠ” non-differentiable (REINFORCE ê°€ëŠ¥)
```

### **Q: "Real-time constraintëŠ”?"**
```
Worst case: 50ms (20Hz)
Average case: 25ms (40Hz)
ë³‘ë ¬ ì²˜ë¦¬ë¡œ latency hiding
```

### **Q: "Sim2Real gapì€?"**
```
Confidenceê°€ Sim2Real indicator ì—­í• 
Realì—ì„œ confidence ë‚®ìœ¼ë©´ ë” ì¡°ì‹¬
Domain randomizationìœ¼ë¡œ robustí•˜ê²Œ
```

---

*ì´ ë¬¸ì„œë¡œ ê¸°ìˆ ì  ê¹Šì´ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”! ğŸ’ª*