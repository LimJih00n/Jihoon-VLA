# RT-1: Robotics Transformer 완전 분석 (2025년 1월 업데이트)

## 📌 논문 개요

### 기본 정보
- **제목**: RT-1: Robotics Transformer for Real-World Control at Scale
- **저자**: Google Robotics & Everyday Robots 팀 (Anthony Brohan et al., 총 50명)
- **발표**: 2022년 12월 arXiv (2212.06817v2), 2023년 8월 수정판
- **핵심 기여**: 대규모 실제 로봇 데이터로 학습한 최초의 범용 로봇 제어 Transformer 모델

### 핵심 성과
- ✅ **744개의 고유 태스크** 수행 가능 (97% 성공률)
- ✅ **130,000+ 에피소드** 데이터셋 구축 (17개월, 13대 로봇)
- ✅ **Zero-shot 일반화**: 새로운 태스크 76% 성공률
- ✅ **실시간 제어**: 3Hz 속도로 실제 로봇 제어
- ✅ **이종 데이터 통합**: 시뮬레이션 및 다른 로봇 데이터 흡수 가능

---

## 🏗️ 모델 아키텍처

### RT-1 구조 상세

```
[입력]
├── 이미지: 6개 프레임 (300×300)
└── 텍스트: 자연어 지시사항
    ↓
[FiLM-conditioned EfficientNet-B3] (16M 파라미터)
├── ImageNet 사전학습 활용
├── Universal Sentence Encoder로 언어 임베딩
└── FiLM 레이어로 초기 이미지-언어 융합
    ↓
[TokenLearner]
├── 81개 토큰 → 8개 토큰으로 압축
└── Attention 기반 중요 정보 선택
    ↓
[Transformer] (19M 파라미터)
├── 8개 self-attention 레이어
├── Decoder-only 구조
└── 총 48개 토큰 처리 (6 이미지 × 8 토큰)
    ↓
[출력: 이산화된 행동]
├── 7 DoF 팔 제어
├── 3 DoF 베이스 이동
└── 모드 선택 (팔/베이스/종료)
```

### 주요 설계 결정

#### 1. 행동 공간 이산화
```python
action_space = {
    "arm": {
        "x, y, z": 256_bins_each,
        "roll, pitch, yaw": 256_bins_each,
        "gripper": 256_bins
    },
    "base": {
        "x, y": 256_bins_each,
        "yaw": 256_bins
    },
    "mode": ["arm", "base", "terminate"]
}
```
- **장점**: 복잡한 다중모드 분포 표현 가능
- **성능 향상**: 연속 행동 대비 +29% 개선

#### 2. FiLM 레이어 초기화
```python
# Identity 초기화로 사전학습 가중치 보존
def initialize_film():
    fc.weight.data.zero_()  # γ = 0
    hc.weight.data.zero_()  # β = 0
    # 초기에는 identity로 동작, 점진적으로 언어 조건화 학습
```

#### 3. TokenLearner 적용
- 81개 시각 토큰을 8개로 압축
- 추론 속도 2.4배 향상
- 중요한 정보만 선택적 추출

---

## 📊 데이터셋 규모와 구성

### 데이터 수집 통계
- **기간**: 17개월
- **로봇 수**: 13대 (Everyday Robots)
- **총 에피소드**: 130,000+
- **고유 지시사항**: 744개
- **환경**: 사무실 주방 3곳 (학습용 1곳, 평가용 2곳)

### 스킬 분포

| 스킬 카테고리 | 지시사항 수 | 예시 |
|-------------|-----------|------|
| Pick Object | 130 | "pick iced tea can" |
| Move Near | 337 | "move pepsi can near rxbar" |
| Place Upright | 8 | "place water bottle upright" |
| Knock Over | 8 | "knock redbull can over" |
| Open/Close Drawer | 6 | "open the top drawer" |
| Place into Receptacle | 84 | "place chip bag into bowl" |
| Pick from Receptacle | 162 | "pick apple from bowl" |
| Complex Tasks | 9 | "pull napkin from dispenser" |

---

## 🔬 실험 결과

### 1. 전체 성능 비교

| 평가 항목 | RT-1 | Gato | BC-Z | BC-Z XL |
|----------|------|------|------|---------|
| **학습된 태스크** | **97%** | 65% | 72% | 56% |
| **새로운 태스크** | **76%** | 52% | 19% | 43% |
| **방해물 강건성** | **83%** | 43% | 47% | 23% |
| **배경 강건성** | **59%** | 35% | 41% | 35% |

### 2. Zero-shot 일반화 능력
- 21개 새로운 지시사항에 대해 **76% 성공률**
- 기존 최고 모델(Gato) 대비 **24% 향상**
- 새로운 스킬-객체 조합을 이해하고 수행

### 3. 환경 강건성

#### 방해물 강건성 (Distractor Robustness)
```
쉬움 (0-5개 방해물): 100%
중간 (9개 방해물): 100%
어려움 (9개 + 가림): 64%
```

#### 배경 강건성 (Background Robustness)
```
원래 환경: 높은 성공률
패턴 테이블보: 중간 성공률
새로운 주방: 59% 성공률
```

### 4. 장기 태스크 수행 (SayCan 통합)

| 환경 | 계획 성공률 | 실행 성공률 |
|------|-----------|-----------|
| Kitchen1 | 87% | **67%** |
| Kitchen2 (새 환경) | 87% | **67%** |

- 최대 **50단계** 복잡한 태스크 수행 가능
- BC-Z (53%), Gato (33%) 대비 우수한 성능

---

## 💡 이종 데이터 흡수 능력

### 1. 시뮬레이션 데이터 통합

```python
results = {
    "실제 물체 성능": "90% (-2% 하락)",
    "시뮬레이션 물체 (실제 평가)": "87% (+64% 향상)",
    "보지 못한 스킬": "33% (+26% 향상)"
}
```

**핵심 발견**: 시뮬레이션 데이터 추가가 실제 성능을 거의 해치지 않으면서 새로운 물체에 대한 일반화 크게 향상

### 2. 다른 로봇 데이터 통합 (Kuka IIWA)

```python
cross_robot_results = {
    "원래 태스크": "90% (-2% 하락)",
    "Bin-picking": "39% (+17% 향상, 거의 2배)"
}
```

**의미**: 다른 로봇의 경험에서 학습하여 새로운 스킬 획득 가능

---

## 📈 스케일링 특성

### 데이터 다양성 vs 데이터 양

| 구성 | 데이터 비율 | 태스크 비율 | Seen | Unseen | 방해물 | 배경 |
|-----|-----------|-----------|------|--------|-------|-----|
| 전체 | 100% | 100% | 97% | 76% | 83% | 59% |
| 데이터 51% | 51% | 100% | 71% | 52% | 39% | 59% |
| 태스크 75% | 97% | 75% | 86% | 67% | 42% | 53% |

**핵심 통찰**: 
- 데이터 다양성 > 데이터 양
- 25% 태스크 제거 = 49% 데이터 제거와 동일한 영향

---

## 🔍 주요 설계 결정의 영향

| 설계 요소 | 성능 영향 | 이유 |
|----------|---------|------|
| **이산화된 행동** | +29% | 복잡한 다중모드 분포 표현 |
| **ImageNet 사전학습** | +33% (일반화) | 시각적 지식 전이 |
| **TokenLearner** | 2.4x 속도 향상 | 효율적 토큰 압축 |
| **FiLM 조건화** | 방해물 강건성 향상 | 초기 언어-비전 융합 |
| **히스토리 사용** | +33% (방해물) | 시간적 컨텍스트 활용 |
| **Transformer** | +13% | 복잡한 관계 모델링 |

---

## 🚀 한계와 향후 연구 방향

### 현재 한계
1. **모방 학습의 한계**: 시연자 성능을 넘을 수 없음
2. **새로운 동작 생성 불가**: 학습 데이터에 없는 완전히 새로운 모션 수행 불가
3. **민첩성 제한**: 복잡한 조작 태스크에는 여전히 제한적

### 개선 가능 영역
1. **메모리 시스템 부재** → Selective RAG 추가 가능
2. **과거 실패 학습 불가** → 경험 기반 학습 메커니즘 필요
3. **고정된 추론 속도** → Adaptive 처리 필요

### 우리 연구(Flow-RAG)와의 연결점
```python
improvement_opportunities = {
    "RT-1 강점": "효율적 실시간 제어 + 강력한 일반화",
    "RT-1 약점": "메모리 없음 + 적응 능력 부족",
    "Flow-RAG 해결책": {
        "Confidence-based Retrieval": "필요시에만 메모리 검색",
        "Selective Memory": "실패 경험 중심 학습",
        "예상 성능": "20-30Hz 속도 + 지속 학습 능력"
    }
}
```

---

## 🎯 핵심 요약

### RT-1이 증명한 것:
✅ **대규모 데이터로 강력한 로봇 정책 학습 가능**
✅ **Transformer 기반 실시간 로봇 제어 가능**
✅ **이종 데이터 소스 효과적 통합 가능**
✅ **Zero-shot 일반화 능력 실현**

### RT-1의 혁신:
🔸 **조기 언어-비전 융합** (FiLM 레이어)
🔸 **효율적 토큰 압축** (TokenLearner)
🔸 **이산화된 행동 표현** (다중모드 분포 처리)
🔸 **대규모 실제 데이터** 수집 및 활용

### 미래 영향:
> "RT-1은 로봇 학습의 **스케일 문제를 해결**했지만,
> **지속적 학습과 적응 능력**이 부족합니다.
> 우리의 **Confidence-based Selective RAG**가 이를 보완할 것입니다."

---

## 📚 참고 자료
- 논문: https://arxiv.org/abs/2212.06817
- 프로젝트: https://robotics-transformer1.github.io
- 코드: https://github.com/google-research/robotics_transformer

---

*이 분석은 2025년 1월 기준으로 작성되었으며, 유환조 교수님 랩 컨택을 위해 준비되었습니다.*