# RT-1: Robotics Transformer 상세 분석
## Google의 대규모 로봇 학습 시스템

---

## 📚 논문 개요

### 기본 정보
- **제목**: RT-1: Robotics Transformer for Real-World Control at Scale
- **저자**: Google Robotics & Everyday Robots 팀 (Anthony Brohan et al., 총 50명)
- **발표**: 2022년 12월 arXiv, 2023년 8월 수정판
- **논문 링크**: [https://arxiv.org/abs/2212.06817](https://arxiv.org/abs/2212.06817)
- **프로젝트 페이지**: [https://robotics-transformer1.github.io](https://robotics-transformer1.github.io)

### 핵심 기여
1. **대규모 로봇 데이터 학습**: 130k+ 에피소드, 700+ 태스크
2. **실시간 제어**: 3Hz 속도로 실제 로봇 제어 가능
3. **강력한 일반화**: 새로운 태스크, 환경, 객체에 대한 뛰어난 일반화 능력
4. **이종 데이터 통합**: 시뮬레이션 및 다른 로봇 데이터 흡수 가능

---

## 🏗️ 시스템 아키텍처

### RT-1 모델 구조

```
이미지 + 텍스트 지시사항
        ↓
[FiLM-conditioned EfficientNet-B3]
- ImageNet 사전학습
- Universal Sentence Encoder로 언어 임베딩
- FiLM 레이어로 이미지-언어 융합
        ↓
[TokenLearner]
- 81개 토큰 → 8개 토큰으로 압축
- 중요한 정보만 선택적 추출
        ↓
[Transformer]
- 8개 self-attention 레이어
- 19M 파라미터
- Decoder-only 구조
        ↓
[이산화된 행동 출력]
- 각 차원을 256개 빈으로 이산화
- 7 DoF 팔 + 3 DoF 베이스 + 모드 선택
```

### 주요 구성 요소

#### 1. **이미지-언어 토크나이저**
```python
특징 = {
    "입력": "6개 이미지 (300x300) + 자연어 지시사항",
    "처리": {
        "이미지": "EfficientNet-B3 (ImageNet 사전학습)",
        "언어": "Universal Sentence Encoder",
        "융합": "FiLM 레이어 (초기 융합)"
    },
    "출력": "81개 시각-언어 토큰"
}
```

#### 2. **TokenLearner**
- 81개 토큰을 8개로 압축
- Attention 기반 선택적 토큰 추출
- 추론 속도 2.4배 향상

#### 3. **Transformer 백본**
- 8개 self-attention 레이어
- 19M 파라미터
- 실시간 추론 최적화

#### 4. **행동 이산화**
```python
action_space = {
    "arm": {
        "x, y, z": 256_bins,
        "roll, pitch, yaw": 256_bins,
        "gripper": 256_bins
    },
    "base": {
        "x, y": 256_bins,
        "yaw": 256_bins
    },
    "mode": ["arm", "base", "terminate"]
}
```

---

## 📊 데이터셋 상세

### 데이터 수집 규모
- **기간**: 17개월
- **로봇 수**: 13대
- **총 에피소드**: 130,000+
- **총 태스크 수**: 744개 고유 지시사항

### 스킬 분류

| 스킬 | 개수 | 설명 | 예시 |
|------|------|------|------|
| Pick Object | 130 | 물체 집기 | "pick iced tea can" |
| Move Near | 337 | 물체 근처로 이동 | "move pepsi can near rxbar" |
| Place Upright | 8 | 물체 세우기 | "place water bottle upright" |
| Knock Over | 8 | 물체 넘어뜨리기 | "knock redbull can over" |
| Open/Close Drawer | 6 | 서랍 열기/닫기 | "open the top drawer" |
| Place into Receptacle | 84 | 용기에 넣기 | "place chip bag into bowl" |
| Pick from Receptacle | 162 | 용기에서 꺼내기 | "pick apple from bowl" |
| 기타 태스크 | 9 | 복잡한 지시사항 | "pull napkin from dispenser" |

### 데이터 수집 방법
1. **VR 컨트롤러**: 직접 시연을 통한 데이터 수집
2. **자동 태스크 할당**: 균형잡힌 데이터셋 구성
3. **장면 무작위화**: 물체 위치, 조명 등 변화

---

## 🔬 실험 결과

### 주요 성능 지표

| 평가 항목 | RT-1 | Gato | BC-Z | BC-Z XL |
|-----------|------|------|------|---------|
| **학습된 태스크** | 97% | 65% | 72% | 56% |
| **새로운 태스크** | 76% | 52% | 19% | 43% |
| **방해물 강건성** | 83% | 43% | 47% | 23% |
| **배경 강건성** | 59% | 35% | 41% | 35% |

### 일반화 능력 평가

#### 1. **Zero-shot 일반화**
- 21개의 새로운 지시사항에 대해 76% 성공률
- 기존 최고 모델 대비 24% 향상

#### 2. **환경 강건성**
```python
robustness_results = {
    "방해물": {
        "쉬움 (0-5개)": "100%",
        "중간 (9개)": "100%",
        "어려움 (9개+가림)": "64%"
    },
    "배경": {
        "원래 환경": "높은 성공률",
        "패턴 테이블보": "중간 성공률",
        "새로운 주방": "59% 성공률"
    }
}
```

#### 3. **장기 태스크 수행 (SayCan 통합)**
- Kitchen1: 67% 실행 성공률
- Kitchen2 (새로운 환경): 67% 유지
- 최대 50단계 태스크 수행 가능

### 이종 데이터 흡수 능력

#### 시뮬레이션 데이터 통합
```python
sim_integration_results = {
    "실제 물체 성능": "90% (2% 하락)",
    "시뮬레이션 물체 (실제 평가)": "87% (64% 향상)",
    "보지 못한 스킬": "33% (26% 향상)"
}
```

#### 다른 로봇 데이터 통합 (Kuka IIWA)
```python
robot_transfer_results = {
    "원래 태스크": "90% (2% 하락)",
    "Bin-picking": "39% (17% 향상, 거의 2배)"
}
```

---

## 💡 핵심 설계 결정과 영향

### 모델 구조 분석

| 설계 요소 | 성능 영향 | 설명 |
|-----------|----------|------|
| **이산화된 행동** | +29% | 복잡한 다중모드 분포 표현 가능 |
| **ImageNet 사전학습** | +33% (일반화) | 시각적 지식 전이 |
| **TokenLearner** | 2.4x 속도 향상 | 효율적 토큰 압축 |
| **FiLM 조건화** | 방해물 강건성 향상 | 초기 언어-비전 융합 |
| **히스토리 사용** | +33% (방해물) | 시간적 컨텍스트 활용 |
| **Transformer** | +13% | 복잡한 관계 모델링 |

### 추론 속도 최적화
```python
inference_optimization = {
    "목표": "3Hz (333ms/action)",
    "네트워크 추론": "<100ms",
    "기법": [
        "TokenLearner (토큰 압축)",
        "토큰 재사용 (겹치는 윈도우)",
        "Non-autoregressive 생성"
    ]
}
```

---

## 📈 스케일링 특성

### 데이터 다양성 vs 데이터 양

| 데이터 구성 | 데이터 비율 | 태스크 비율 | Seen | Unseen | Distractor | Background |
|-------------|-------------|-------------|------|--------|------------|------------|
| **전체 데이터** | 100% | 100% | 97% | 76% | 83% | 59% |
| **데이터 51%** | 51% | 100% | 71% | 52% | 39% | 59% |
| **데이터 37%** | 37% | 100% | 55% | 57% | 35% | 47% |
| **태스크 75%** | 97% | 75% | 86% | 67% | 42% | 53% |

**핵심 발견**: 데이터 다양성 > 데이터 양
- 25% 태스크 제거 = 49% 데이터 제거와 동일한 영향

---

## 🔍 주목할 기술적 세부사항

### 1. FiLM 레이어 초기화
```python
# Identity 초기화로 사전학습 가중치 보존
def initialize_film():
    fc.weight.data.zero_()  # 0으로 초기화
    hc.weight.data.zero_()  # 처음엔 identity로 동작
    # 점진적으로 언어 조건화 학습
```

### 2. Attention 패턴 분석
- Layer 2, Head 6: 그리퍼와 대상 물체 상호작용 포커스
- Layer 4, Head 2: 서랍 같은 환경 요소 포커스
- 컴팩트한 표현으로 방해물 무시

### 3. 실패 모드와 한계
```python
limitations = {
    "모방 학습": "시연자 성능 한계",
    "새로운 동작": "완전히 새로운 모션 불가",
    "민첩성": "복잡한 조작 제한적"
}
```

---

## 🚀 우리 연구와의 연결점

### RT-1이 증명한 것
✅ **대규모 데이터로 강력한 일반화 가능**
✅ **실시간 Transformer 제어 가능**
✅ **이종 데이터 흡수 가능**

### RT-1의 한계 (우리가 개선할 점)
❌ **메모리 시스템 부재** → Confidence-based RAG 추가
❌ **과거 실패 학습 불가** → 선택적 메모리 검색
❌ **항상 같은 속도로 동작** → Adaptive 처리

### 제안하는 개선 방향
```python
our_improvements = {
    "Confidence-based Retrieval": {
        "높은 확신도": "RT-1 속도 유지 (3Hz)",
        "낮은 확신도": "RAG 검색으로 정확도 향상",
        "예상 성능": "20-30Hz + 학습 능력"
    },
    "시너지": "RT-1의 효율성 + ELLMER의 메모리"
}
```

---

## 📝 교수님께 어필할 포인트

1. **"RT-1은 빠르지만 기억이 없습니다"**
   - 50Hz π₀보다는 느리지만 더 안정적
   - 하지만 여전히 과거 경험 활용 불가

2. **"우리의 Selective RAG가 해결책입니다"**
   - RT-1의 실시간성 유지
   - 필요할 때만 메모리 검색
   - 실패 경험 중심 학습

3. **"RT-1 + RAG = 완벽한 조합"**
   - RT-1: 효율적 베이스 모델
   - RAG: 지능적 메모리 시스템
   - 결과: 빠르면서도 똑똑한 로봇

---

## 🎯 핵심 요약

> **RT-1은 로봇 학습의 스케일 문제를 해결했지만,**
> **지속적 학습과 적응 능력이 부족합니다.**
> **우리의 Confidence-based Selective RAG가 이를 보완할 것입니다.**

---

*마지막 업데이트: 2025년 1월*
*이 분석은 유환조 교수님 랩 컨택을 위해 준비되었습니다*